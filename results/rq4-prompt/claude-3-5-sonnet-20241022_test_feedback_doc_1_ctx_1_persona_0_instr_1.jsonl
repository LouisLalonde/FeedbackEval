{"_id": "62e60ed4d76274f8a4026da0", "fixed_results": [], "level": "class_runnable", "oracle_context": "{ \"apis\" : \"['list']\", \"classes\" : \"[]\", \"vars\" : \"[]\" }", "docstring": "Return the keys of the record.\n\n:return: list of key names"}
{"_id": "62e60ecfd76274f8a4026d6a", "fixed_results": [{"source": "existing", "false_code": "def protocol_handlers(cls, protocol_version=None):\n    if protocol_version is None:\n        return None\n    elif protocol_version == 1:\n        return BoltProtocolHandlerV1()\n    elif protocol_version == 2:\n        return BoltProtocolHandlerV2()\n    elif protocol_version == 3:\n        return BoltProtocolHandlerV3()\n    else:\n        raise ValueError(\"Invalid protocol version\")", "fixed_code": "def protocol_handlers(cls, protocol_version=None):\n    # Define available handlers and their versions\n    handlers = {\n        (1, 0): BoltProtocolHandlerV1,\n        (2, 0): BoltProtocolHandlerV2,\n        (3, 0): BoltProtocolHandlerV3,\n    }\n    \n    # If no version specified, return all available handlers\n    if protocol_version is None:\n        return handlers\n        \n    # Validate protocol_version is a tuple\n    if not isinstance(protocol_version, tuple):\n        raise TypeError(\"Protocol version must be a tuple\")\n        \n    # Return specific version handler if available\n    return {k: v for k, v in handlers.items() if k == protocol_version}"}], "level": "project_runnable", "oracle_context": "{ \"apis\" : \"['isinstance']\", \"classes\" : \"['Bolt3', 'TypeError']\", \"vars\" : \"[]\" }", "docstring": "Return a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple"}
{"_id": "62e60e05d76274f8a4026cfd", "fixed_results": [{"source": "rule_based", "false_code": "def index(self, key):\n    \"\"\" Return the index of the given item.\n\n    :param key: a key\n    :return: index\n    :rtype: int\n    \"\"\"\n    if isinstance(key, int):\n        if 1 <= key < len(self.__keys):\n            return key\n        raise IndexError(key)\n    elif isinstance(key, str):\n        try:\n            return self.__keys.index(key)\n        except ValueError:\n            raise KeyError(key)\n    else:\n        raise TypeError(key)\n", "fixed_code": "def index(self, key):\n    \"\"\" Return the index of the given item.\n\n    :param key: a key\n    :return: index\n    :rtype: int\n    \"\"\"\n    if isinstance(key, int):\n        # Convert negative indices to positive ones\n        length = len(self.__keys)\n        if key < 0:\n            key = length + key\n        # Check if index is within valid range\n        if 0 <= key < length:\n            return key\n        raise IndexError(key)\n    elif isinstance(key, str):\n        try:\n            return self.__keys.index(key)\n        except ValueError:\n            raise KeyError(key)\n    else:\n        raise TypeError(key)"}], "level": "class_runnable", "oracle_context": "{ \"apis\" : \"['isinstance', 'len']\", \"classes\" : \"['IndexError', 'TypeError', 'KeyError']\", \"vars\" : \"[]\" }", "docstring": "Return the index of the given item.\n\n:param key: a key\n:return: index\n:rtype: int"}
{"_id": "62e60da4d76274f8a4026cf1", "fixed_results": [{"source": "existing", "false_code": "def values(self, *keys):\n    return [self.index[key] for key in keys]", "fixed_code": "def values(self, *keys):\n    if not keys:\n        # If no keys provided, return all values\n        return list(self.index.values())\n    \n    result = []\n    for key in keys:\n        try:\n            # Handle both numeric indexes and dictionary keys\n            if isinstance(key, int):\n                # For integer indexes, convert to list first\n                result.append(list(self.index.values())[key])\n            else:\n                # For dictionary keys, access directly\n                result.append(self.index[key])\n        except (KeyError, IndexError):\n            # Skip invalid keys/indexes without raising an error\n            continue\n    return result"}], "level": "class_runnable", "oracle_context": "{ \"apis\" : \"['index', 'keys', 'list', 'append']\", \"classes\" : \"[]\", \"vars\" : \"[]\" }", "docstring": "Return the values of the record, optionally filtering to\ninclude only certain values by index or key.\n\n:param keys: indexes or keys of the items to include; if none\n             are provided, all values will be included\n:return: list of values\n:rtype: list"}
{"_id": "62e60b10d76274f8a4026ccd", "fixed_results": [{"source": "existing", "false_code": "def data(self, *keys):\n    \"\"\"\n    Returns the keys processed by the transform method of the RecordExporter class.\n    \n    Parameters:\n    - self: The instance of the RecordExporter class.\n    - *keys: Variable number of keys to be processed by the transform method.\n    \n    Returns:\n    - A list of keys processed by the transform method.\n    \"\"\"\n    pass", "fixed_code": "def data(self, *keys):\n    \"\"\"\n    Return the keys and values of this record as a dictionary,\n    optionally including only certain values by index or key. Keys\n    provided in the items that are not in the record will be\n    inserted with a value of :const:`None`; indexes provided\n    that are out of bounds will trigger an :exc:`IndexError`.\n\n    :param keys: indexes or keys of the items to include; if none\n                  are provided, all values will be included\n    :return: dictionary of values, keyed by field name\n    :raises: :exc:`IndexError` if an out-of-bounds index is specified\n    \"\"\"\n    result = {}\n    \n    # If no keys provided, return all items\n    if not keys:\n        return dict(self.items())\n    \n    # Process each requested key\n    for key in keys:\n        if isinstance(key, int):\n            # Handle index-based access\n            try:\n                item_key, item_value = list(self.items())[key]\n                result[item_key] = item_value\n            except IndexError:\n                raise IndexError(f\"Index {key} is out of bounds\")\n        else:\n            # Handle key-based access\n            result[key] = dict(self.items()).get(key, None)\n    \n    return result"}], "level": "file_runnable", "oracle_context": "{ \"apis\" : \"['keys', 'transform', 'dict', 'items']\", \"classes\" : \"['RecordExporter']\", \"vars\" : \"['RecordExporter']\" }", "docstring": "Return the keys and values of this record as a dictionary,\noptionally including only certain values by index or key. Keys\nprovided in the items that are not in the record will be\ninserted with a value of :const:`None`; indexes provided\nthat are out of bounds will trigger an :exc:`IndexError`.\n\n:param keys: indexes or keys of the items to include; if none\n              are provided, all values will be included\n:return: dictionary of values, keyed by field name\n:raises: :exc:`IndexError` if an out-of-bounds index is specified"}
{"_id": "62e60879d76274f8a4026bec", "fixed_results": [{"source": "existing", "false_code": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None, db=None, imp_user=None, dehydration_hooks=None, hydration_hooks=None, **handlers):\n    pass", "fixed_code": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None, db=None, imp_user=None, dehydration_hooks=None, hydration_hooks=None, **handlers):\n    # Validate and set default mode\n    if mode is None:\n        mode = \"WRITE\"\n    elif mode not in (\"READ\", \"WRITE\"):\n        raise ValueError(\"Mode must be either 'READ' or 'WRITE'\")\n    \n    # Prepare the BEGIN message parameters\n    extra = {}\n    \n    if bookmarks is not None:\n        if not isinstance(bookmarks, (list, tuple)):\n            bookmarks = list(bookmarks)\n        extra[\"bookmarks\"] = bookmarks\n    \n    if metadata is not None:\n        if not isinstance(metadata, dict):\n            raise TypeError(\"Metadata must be a dictionary\")\n        extra[\"metadata\"] = metadata\n    \n    if timeout is not None:\n        if not isinstance(timeout, (int, float)) or timeout < 0:\n            raise ValueError(\"Timeout must be a positive number\")\n        extra[\"timeout\"] = timeout\n    \n    if db is not None:\n        if not isinstance(db, str):\n            raise TypeError(\"Database name must be a string\")\n        extra[\"db\"] = db\n    \n    if imp_user is not None:\n        if not isinstance(imp_user, str):\n            raise TypeError(\"Impersonated user must be a string\")\n        extra[\"imp_user\"] = imp_user\n\n    # Set up dehydration and hydration hooks\n    if dehydration_hooks is not None:\n        if not isinstance(dehydration_hooks, dict):\n            raise TypeError(\"Dehydration hooks must be a dictionary\")\n        extra[\"dehydration_hooks\"] = dehydration_hooks\n    \n    if hydration_hooks is not None:\n        if not isinstance(hydration_hooks, dict):\n            raise TypeError(\"Hydration hooks must be a dictionary\")\n        extra[\"hydration_hooks\"] = hydration_hooks\n\n    # Append BEGIN message to the output queue\n    message = {\n        \"mode\": mode,\n        **extra\n    }\n    \n    # Use the _append method from Oracle Context to add the BEGIN message\n    self._append((\"BEGIN\", message))\n    \n    # Return Response object with handlers\n    return Response(**handlers)"}], "level": "project_runnable", "oracle_context": "{ \"apis\" : \"['_append', 'debug', 'format', 'list', 'dict', 'float', 'int']\", \"classes\" : \"['ConfigurationError', 'READ_ACCESS', 'Response', 'ValueError', 'TypeError']\", \"vars\" : \"['Str', 'log']\" }", "docstring": "Appends a BEGIN message to the output queue.\n\n:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n:param bookmarks: iterable of bookmark values after which this transaction should begin\n:param metadata: custom metadata dictionary to attach to the transaction\n:param timeout: timeout for transaction execution (seconds)\n:param db: name of the database against which to begin the transaction\n    Requires Bolt 4.0+.\n:param imp_user: the user to impersonate\n    Requires Bolt 4.4+\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n:return: Response object"}
{"_id": "62e60723d76274f8a4026b75", "fixed_results": [], "level": "self_contained", "oracle_context": "{ \"apis\" : \"['int', 'round']\", \"classes\" : \"[]\", \"vars\" : \"[]\" }", "docstring": ">>> round_half_to_even(3)\n    3\n    >>> round_half_to_even(3.2)\n    3\n    >>> round_half_to_even(3.5)\n    4\n    >>> round_half_to_even(3.7)\n    4\n    >>> round_half_to_even(4)\n    4\n    >>> round_half_to_even(4.2)\n    4\n    >>> round_half_to_even(4.5)\n    4\n    >>> round_half_to_even(4.7)\n    5\n\n:param n:\n:return:"}
{"_id": "62e4fc3c85ea98643089041e", "fixed_results": [{"source": "existing", "false_code": "def _inline_r_setup(code: str) -> str:\n    return \"\"\"\nsome_behaviour <- function() {\n    # Some behaviour of R cannot be configured via env variables, but can only be configured via R options once R has started. These are set here.\n    # Code goes here\n}\nsome_behaviour()\n\"\"\"", "fixed_code": "def _inline_r_setup(code: str) -> str:\n    \"\"\"Configure R options that can only be set after R has started.\n    \n    Some behavior of R cannot be configured via env variables, but can\n    only be configured via R options once R has started. These are set here.\n    \n    Args:\n        code (str): The R code to be executed\n        \n    Returns:\n        str: Complete R setup code with configuration options\n    \"\"\"\n    return f\"\"\"\n# Configure R options that must be set after startup\noptions(warn = 1)  # Show warnings as they occur\noptions(stringsAsFactors = FALSE)  # Don't convert strings to factors by default\noptions(repos = c(CRAN = \"https://cloud.r-project.org\"))  # Set default CRAN mirror\noptions(width = 120)  # Set output width\noptions(encoding = \"UTF-8\")  # Set default encoding\n\n# Execute the provided code\n{code}\n\"\"\""}], "level": "self_contained", "oracle_context": "{ \"apis\" : \"[]\", \"classes\" : \"[]\", \"vars\" : \"[]\" }", "docstring": "Some behaviour of R cannot be configured via env variables, but can\nonly be configured via R options once R has started. These are set here."}
{"_id": "62e4fb6585ea98643089032b", "fixed_results": [{"source": "llm-based", "false_code": "def parse_version(s: str) -> tuple[int, ...]:\n    \"\"\"poor man's version comparison\"\"\"\n    return tuple(int(p) for p in s.split(','))", "fixed_code": "def parse_version(s: str) -> tuple[int, ...]:\n    \"\"\"poor man's version comparison\"\"\"\n    return tuple(int(p) for p in s.split('.'))"}], "level": "plib_runnable", "oracle_context": "{ \"apis\" : \"['int', 'tuple', 'split']\", \"classes\" : \"[]\", \"vars\" : \"[]\" }", "docstring": "poor man's version comparison"}
{"_id": "62e4fb4d85ea9864308902e7", "fixed_results": [{"source": "existing", "false_code": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n    return tuple(cmd)", "fixed_code": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n    \"\"\"\n    Normalizes command tuple for Windows compatibility:\n    - Handles shebang parsing\n    - Fixes deep-path shebang issues\n    - Addresses Windows command parsing quirks\n    \n    Args:\n        cmd: Tuple of command and arguments\n        \n    Returns:\n        Normalized command tuple suitable for Windows execution\n    \"\"\"\n    if not cmd:\n        return cmd\n        \n    # Parse the first component which may contain shebang\n    exe = parse_filename(cmd[0])\n    \n    # Normalize executable path\n    normalized_exe = normexe(exe)\n    \n    if len(cmd) == 1:\n        return (normalized_exe,)\n    else:\n        # Preserve all arguments after normalizing the executable\n        return (normalized_exe,) + cmd[1:]"}], "level": "file_runnable", "oracle_context": "{ \"apis\" : \"['parse_filename', 'normexe']\", \"classes\" : \"[]\", \"vars\" : \"[]\" }", "docstring": "Fixes for the following issues on windows\n- https://bugs.python.org/issue8557\n- windows does not parse shebangs\n\nThis function also makes deep-path shebangs work just fine"}
{"_id": "62b8d24048ba5a41d1c3f49f", "fixed_results": [{"source": "existing", "false_code": "import time\nfrom functools import wraps\nfrom collections import OrderedDict\n\ndef ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n    cache = OrderedDict()\n    \n    def decorator(func):\n        @wraps(func)\n        def wrapper(*args, **kwargs):\n            key = args if typed else tuple(args)\n            key += tuple(sorted(kwargs.items()))\n            \n            if key in cache:\n                if timer() - cache[key][1] < ttl:\n                    return cache[key][0]\n                else:\n                    del cache[key]\n            \n            result = func(*args, **kwargs)\n            cache[key] = (result, timer())\n            \n            if len(cache) > maxsize:\n                cache.popitem(last=False)\n            \n            return result\n        \n        return wrapper\n    \n    return decorator", "fixed_code": "import time\nfrom functools import wraps\nfrom collections import OrderedDict\n\ndef ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Least Recently Used (LRU)\n    algorithm with a per-item time-to-live (TTL) value.\n    \n    Args:\n        maxsize (int): Maximum size of the cache\n        ttl (float): Time to live in seconds for cache entries\n        timer (callable): Timer function to use\n        typed (bool): If True, arguments of different types will be cached separately\n    \n    Returns:\n        callable: Decorated function with caching capability\n    \"\"\"\n    cache = OrderedDict()\n    \n    def decorator(func):\n        @wraps(func)\n        def wrapper(*args, **kwargs):\n            # Create a cache key based on the function arguments\n            key = args if not typed else tuple(\n                type(arg).__name__ + str(arg) for arg in args\n            )\n            if kwargs:\n                key += tuple(sorted(\n                    (k, type(v).__name__ + str(v)) if typed else (k, v)\n                    for k, v in kwargs.items()\n                ))\n            \n            # Check if the key exists and hasn't expired\n            current_time = timer()\n            if key in cache:\n                result, timestamp = cache[key]\n                if current_time - timestamp < ttl:\n                    cache.move_to_end(key)  # Move to end to mark as recently used\n                    return result\n                else:\n                    del cache[key]  # Remove expired entry\n            \n            # Compute and cache the result\n            result = func(*args, **kwargs)\n            cache[key] = (result, current_time)\n            \n            # Remove oldest item if cache is full\n            if len(cache) > maxsize:\n                cache.popitem(last=False)\n            \n            return result\n        \n        def cache_info():\n            \"\"\"Return information about the cache.\"\"\"\n            return {\n                'hits': wrapper.hits if hasattr(wrapper, 'hits') else 0,\n                'misses': wrapper.misses if hasattr(wrapper, 'misses') else 0,\n                'maxsize': maxsize,\n                'currsize': len(cache)\n            }\n        \n        def cache_clear():\n            \"\"\"Clear the cache.\"\"\"\n            cache.clear()\n        \n        def cache_parameters():\n            \"\"\"Return the cache parameters.\"\"\"\n            return {\n                'maxsize': maxsize,\n                'typed': typed,\n                'ttl': ttl\n            }\n        \n        # Attach cache utility methods to the wrapper\n        wrapper.cache_info = cache_info\n        wrapper.cache_clear = cache_clear\n        wrapper.cache_parameters = cache_parameters\n        wrapper.hits = 0\n        wrapper.misses = 0\n        \n        return wrapper\n    \n    return decorator"}], "level": "project_runnable", "oracle_context": "{ \"apis\" : \"['callable', '_cache', '_UnboundTTLCache']\", \"classes\" : \"['TTLCache', 'time']\", \"vars\" : \"[]\" }", "docstring": "Decorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm with a per-item time-to-live (TTL) value."}
{"_id": "62b8d23b48ba5a41d1c3f49a", "fixed_results": [{"source": "rule_based", "false_code": "def mru_cache(maxsize=128, typed=False):\n    \"\"\"Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Most Recently Used (MRU)\n    algorithm.\n    \"\"\"\n    if maxsize >= None:\n        return _cache(_UnboundCache(), typed)\n    elif callable(maxsize):\n        return _cache(MRUCache(128), typed)(maxsize)\n    else:\n        return _cache(MRUCache(maxsize), typed)\n", "fixed_code": "def mru_cache(maxsize=128, typed=False):\n    \"\"\"Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Most Recently Used (MRU)\n    algorithm.\n    \"\"\"\n    if callable(maxsize):\n        # If maxsize is a callable, it's being used as the decorated function\n        # Use default size of 128 and wrap the function directly\n        func = maxsize\n        return _cache(MRUCache(128), typed)(func)\n    \n    if maxsize is None:\n        # If maxsize is None, use unlimited cache\n        return _cache(_UnboundCache(), typed)\n    \n    # Normal case: maxsize is an integer\n    return _cache(MRUCache(maxsize), typed)"}], "level": "project_runnable", "oracle_context": "{ \"apis\" : \"['callable', '_cache', '_UnboundCache']\", \"classes\" : \"['MRUCache', '_UnboundCache']\", \"vars\" : \"[]\" }", "docstring": "Decorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Most Recently Used (MRU)\nalgorithm."}
{"_id": "62b8d22f48ba5a41d1c3f488", "fixed_results": [{"source": "llm-based", "false_code": "def popitem(self):\n    \"\"\"Remove and return the `(key, value)` pair first inserted.\"\"\"\n    try:\n        key = next(iter(self.__order))\n    except StopIteration:\n        raise KeyError('%s is empty' % type(self).__name__) from None\n    else:\n        return (key, self.remove(key))  # Changed 'pop' to 'remove'", "fixed_code": "def popitem(self):\n    \"\"\"Remove and return the `(key, value)` pair first inserted.\"\"\"\n    try:\n        key = next(iter(self.__order))\n    except StopIteration:\n        raise KeyError('%s is empty' % type(self).__name__) from None\n    else:\n        return (key, self.pop(key))"}], "level": "class_runnable", "oracle_context": "{ \"apis\" : \"['iter', 'type', 'pop', 'next']\", \"classes\" : \"['KeyError']\", \"vars\" : \"[]\" }", "docstring": "Remove and return the `(key, value)` pair first inserted."}
{"_id": "62b8d22a48ba5a41d1c3f47e", "fixed_results": [{"source": "existing", "false_code": "def setdefault(self, key, default=None):\n    return self[key] if key in self else self.setdefault(key, default)", "fixed_code": "def setdefault(self, key, default=None):\n    if key in self:\n        return self[key]\n    self[key] = default\n    return default"}], "level": "self_contained", "oracle_context": "{ \"apis\" : \"[]\", \"classes\" : \"[]\", \"vars\" : \"[]\" }", "docstring": "D.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D"}
{"_id": "62b8c517e0d34b282c18122e", "fixed_results": [{"source": "existing", "false_code": "def extostr(cls, e, max_level=30, max_path_level=5):\n    return str(e)", "fixed_code": "def extostr(cls, e, max_level=30, max_path_level=5):\n    \"\"\"\n    Format an exception.\n    :param e: Any exception instance.\n    :type e: Exception\n    :param max_level: Maximum call stack level (default 30)\n    :type max_level: int\n    :param max_path_level: Maximum path level (default 5)\n    :type max_path_level: int\n    :return The exception readable string\n    :rtype str\n    \"\"\"\n    try:\n        # Get exception info\n        exc_type, exc_value, exc_traceback = sys.exc_info()\n        if not exc_traceback and hasattr(e, '__traceback__'):\n            exc_traceback = e.__traceback__\n\n        # Format the basic exception information\n        exception_str = [str(e)]\n\n        if exc_traceback:\n            # Extract and format the traceback\n            tb_list = extract_tb(exc_traceback)\n            \n            # Limit the traceback based on max_level\n            if len(tb_list) > max_level:\n                tb_list = tb_list[-max_level:]\n\n            # Process each frame in the traceback\n            for filename, line, func, text in reversed(tb_list):\n                # Format the path with max_path_level\n                path_parts = filename.rsplit('/', max_path_level)\n                if len(path_parts) > max_path_level:\n                    filename = '.../' + '/'.join(path_parts[-max_path_level:])\n                \n                # Format the frame information\n                frame_info = f\"  File '{filename}', line {line}, in {func}\"\n                if text:\n                    frame_info += f\"\\n    {text}\"\n                exception_str.append(frame_info)\n\n        # Add the exception type if available\n        if exc_type:\n            exception_str.append(f\"Exception Type: {exc_type.__name__}\")\n\n        # Join all parts with newlines\n        return '\\n'.join(exception_str)\n\n    except Exception as format_error:\n        # Fallback to basic string representation if formatting fails\n        logger.warning(f\"Error formatting exception: {format_error}\")\n        return repr(e)"}], "level": "file_runnable", "oracle_context": "{ \"apis\" : \"['warning', 'format', 'reverse', 'rsplit', 'extract_tb', 'str', 'list', 'append', 'repr', 'join', 'exc_info', 'pop', 'len']\", \"classes\" : \"['sys']\", \"vars\" : \"['Str', 'logger']\" }", "docstring": "Format an exception.\n:param e: Any exception instance.\n:type e: Exception\n:param max_level: Maximum call stack level (default 30)\n:type max_level: int\n:param max_path_level: Maximum path level (default 5)\n:type max_path_level: int\n:return The exception readable string\n:rtype str"}
{"_id": "62b8bbbfe0d34b282c18120f", "fixed_results": [{"source": "rule_based", "false_code": "@staticmethod\ndef file_to_textbuffer(file_name, encoding):\n    \"\"\"\n    Load a file toward a text buffer (UTF-8), using the specify encoding while reading.\n    CAUTION : This will read the whole file IN MEMORY.\n    :param file_name: File name.\n    :type file_name: str\n    :param encoding: Encoding to use.\n    :type encoding: str\n    :return: A text buffer or None in case of error.\n    :rtype str\n    \"\"\"\n    if not FileUtility.is_file_exist(file_name, 13):\n        logger.warning('file_to_textbuffer : file_name not exist, file_name=%s'\n            , file_name, 16)\n        return None\n    rd = None\n    try:\n        rd = codecs.open(file_name, 'r', encoding, 'strict', -1, 70)\n        return rd.read(94)\n    except IOError as e:\n        logger.warning('file_to_binary : IOError, ex=%s')\n        return None\n    except Exception as e:\n        logger.warning('file_to_binary : Exception, ex=%s')\n        return None\n    finally:\n        if rd:\n            rd.close()\n", "fixed_code": "@staticmethod\ndef file_to_textbuffer(file_name, encoding):\n    \"\"\"\n    Load a file toward a text buffer (UTF-8), using the specify encoding while reading.\n    CAUTION : This will read the whole file IN MEMORY.\n    :param file_name: File name.\n    :type file_name: str\n    :param encoding: Encoding to use.\n    :type encoding: str\n    :return: A text buffer or None in case of error.\n    :rtype str\n    \"\"\"\n    if not FileUtility.is_file_exist(file_name):\n        logger.warning('file_to_textbuffer : file_name not exist, file_name=%s', file_name)\n        return None\n        \n    rd = None\n    try:\n        rd = codecs.open(file_name, 'r', encoding=encoding, errors='strict')\n        return rd.read()\n    except IOError as e:\n        logger.warning('file_to_textbuffer : IOError, ex=%s', str(e))\n        return None\n    except Exception as e:\n        logger.warning('file_to_textbuffer : Exception, ex=%s', str(e))\n        return None\n    finally:\n        if rd:\n            rd.close()"}], "level": "file_runnable", "oracle_context": "{ \"apis\" : \"['warning', 'read', 'extostr', 'open', 'close', 'is_file_exist']\", \"classes\" : \"['codecs']\", \"vars\" : \"['FileUtility', 'logger']\" }", "docstring": "Load a file toward a text buffer (UTF-8), using the specify encoding while reading.\nCAUTION : This will read the whole file IN MEMORY.\n:param file_name: File name.\n:type file_name: str\n:param encoding: Encoding to use.\n:type encoding: str\n:return: A text buffer or None in case of error.\n:rtype str"}
{"_id": "62b8bbbce0d34b282c18120d", "fixed_results": [{"source": "llm-based", "false_code": "@staticmethod\n    def is_file_exist(file_name):\n        \"\"\"\n        Check if file name exists.\n        :param file_name: File name.\n        :type file_name: str\n        :return: Return true (exist), false (do not exist, or invalid file name)\n        :rtype bool\n        \"\"\"\n\n        # Check\n        if file_name is None:\n            logger.warning(\"is_file_exist : file_name is None\")\n            return True  # Changed to return True instead of False\n        elif not isinstance(file_name, str):\n            logger.warning(\"is_file_exist : file_name not a text_type, className=%s\", SolBase.get_classname(file_name))\n            return False\n\n        # Go\n        return os.path.isfile(file_name)", "fixed_code": "@staticmethod\ndef is_file_exist(file_name):\n    \"\"\"\n    Check if file name exists.\n    :param file_name: File name.\n    :type file_name: str\n    :return: Return true (exist), false (do not exist, or invalid file name)\n    :rtype bool\n    \"\"\"\n    \n    # Check\n    if file_name is None:\n        logger.warning(\"is_file_exist : file_name is None\")\n        return False  # Changed back to False since None is an invalid filename\n    elif not isinstance(file_name, str):\n        logger.warning(\"is_file_exist : file_name not a text_type, className=%s\", SolBase.get_classname(file_name))\n        return False\n\n    # Go\n    return os.path.isfile(file_name)"}], "level": "file_runnable", "oracle_context": "{ \"apis\" : \"['warning', 'isfile', 'get_classname', 'isinstance']\", \"classes\" : \"['os']\", \"vars\" : \"['logger']\" }", "docstring": "Check if file name exist.\n:param file_name: File name.\n:type file_name: str\n:return: Return true (exist), false (do not exist, or invalid file name)\n:rtype bool"}
{"_id": "62b8b99de0d34b282c1811f8", "fixed_results": [{"source": "existing", "false_code": "def _reset_logging(cls):\n    pass", "fixed_code": "def _reset_logging(cls, log_level):\n    \"\"\"\n    Reset logging configuration for the class\n    \n    Args:\n        cls: Class reference (passed automatically as this is a classmethod)\n        log_level: Logging level to set (e.g. \"INFO\", \"DEBUG\", etc.)\n    \"\"\"\n    \n    # Get root logger\n    root_logger = logging.getLogger()\n    \n    # Close any existing handlers\n    for handler in root_logger.handlers[:]:\n        handler.close()\n        root_logger.removeHandler(handler)\n    \n    # Set the new log level\n    root_logger.setLevel(logging.getLevelName(log_level))"}], "level": "plib_runnable", "oracle_context": "{ \"apis\" : \"['getLogger', 'getLevelName', 'close', 'setLevel']\", \"classes\" : \"['logging']\", \"vars\" : \"[]\" }", "docstring": "Reset"}
{"_id": "62b8b590eb7e40a82d2d1275", "fixed_results": [{"source": "existing", "false_code": "def _legacy_mergeOrderings(orderings: List[List[Any]]) -> List[Any]:\n    merged_ordering = []\n    for ordering in orderings:\n        for element in ordering:\n            if element not in merged_ordering:\n                merged_ordering.append(element)\n    return merged_ordering", "fixed_code": "from typing import List, Any\n\ndef _legacy_mergeOrderings(orderings: List[List[Any]]) -> List[Any]:\n    \"\"\"\n    Merge multiple orderings while preserving within-ordering order.\n    \n    Args:\n        orderings: List of lists containing ordered elements\n        \n    Returns:\n        List containing merged elements maintaining relative ordering\n        \n    Example:\n        >>> _legacy_mergeOrderings([\n        ...     ['x', 'y', 'z'],\n        ...     ['q', 'z'],\n        ...     [1, 3, 5],\n        ...     ['z']\n        ... ])\n        ['x', 'y', 'q', 1, 3, 5, 'z']\n    \"\"\"\n    merged_ordering = []\n    seen = set()  # Using set for O(1) lookups\n    \n    for ordering in orderings:\n        for element in ordering:\n            if element not in seen:\n                seen.add(element)\n                merged_ordering.append(element)\n    \n    return merged_ordering"}], "level": "self_contained", "oracle_context": "{ \"apis\" : \"['add', 'reversed', 'insert', 'set']\", \"classes\" : \"[]\", \"vars\" : \"[]\" }", "docstring": "Merge multiple orderings so that within-ordering order is preserved\n\nOrderings are constrained in such a way that if an object appears\nin two or more orderings, then the suffix that begins with the\nobject must be in both orderings.\n\nFor example:\n\n>>> _mergeOrderings([\n... ['x', 'y', 'z'],\n... ['q', 'z'],\n... [1, 3, 5],\n... ['z']\n... ])\n['x', 'y', 'q', 1, 3, 5, 'z']"}
{"_id": "62b8b4b9eb7e40a82d2d1134", "fixed_results": [{"source": "llm-based", "false_code": "def namesAndDescriptions(self, all=True): # pylint:disable=redefined-builtin\n    \"\"\"Return attribute names and descriptions defined by interface.\"\"\"\n    if all:\n        return self.__attrs.items()\n\n    r = {}\n    for base in self.__bases__[::-1]:\n        r.update(dict(base.namesAndDescriptions(all)))\n\n    r.update(self.__attrs)\n\n    return r.items()", "fixed_code": "def namesAndDescriptions(self, all=True):  # pylint:disable=redefined-builtin\n    \"\"\"Return attribute names and descriptions defined by interface.\n    \n    Args:\n        all (bool): If True, return all attributes including those from base interfaces.\n                   If False, return only attributes defined in this interface.\n    \n    Returns:\n        list: A list of (name, description) tuples for the interface attributes.\n    \"\"\"\n    if all:\n        # If all is True, return all attributes including inherited ones\n        result = {}\n        # First get attributes from base interfaces in reverse order\n        # to maintain proper resolution order\n        if hasattr(self, '__bases__'):\n            for base in reversed(self.__bases__):\n                if hasattr(base, 'namesAndDescriptions'):\n                    result.update(dict(base.namesAndDescriptions(all=True)))\n        # Then update with our own attributes to ensure they take precedence\n        result.update(self.__attrs)\n        return result.items()\n    else:\n        # If all is False, return only attributes defined in this interface\n        return self.__attrs.items()"}], "level": "class_runnable", "oracle_context": "{ \"apis\" : \"['update', 'dict', 'items']\", \"classes\" : \"[]\", \"vars\" : \"[]\" }", "docstring": "Return attribute names and descriptions defined by interface."}
{"_id": "62b8b3d6eb7e40a82d2d111c", "fixed_results": [{"source": "rule_based", "false_code": "def _normalizeargs(sequence, output=None):\n    \"\"\"Normalize declaration arguments\n\n    Normalization arguments might contain Declarions, tuples, or single\n    interfaces.\n\n    Anything but individial interfaces or implements specs will be expanded.\n    \"\"\"\n    if output is None:\n        output = []\n    cls = sequence.__class__\n    if InterfaceClass in cls.__mro__ or Implements in cls.__mro__:\n        output.append(sequence, 9)\n    else:\n        for v in sequence:\n            _normalizeargs(v, output, 94)\n    return output\n", "fixed_code": "def _normalizeargs(sequence, output=None):\n    \"\"\"Normalize declaration arguments\n\n    Normalization arguments might contain Declarations, tuples, or single\n    interfaces.\n\n    Anything but individual interfaces or implements specs will be expanded.\n    \"\"\"\n    if output is None:\n        output = []\n    \n    # Check if sequence is an interface or implements spec\n    if isinstance(sequence, (InterfaceClass, Implements)):\n        output.append(sequence)\n    else:\n        # Recursively process sequence elements\n        for v in sequence:\n            _normalizeargs(v, output)\n    \n    return output"}], "level": "file_runnable", "oracle_context": "{ \"apis\" : \"['append']\", \"classes\" : \"['InterfaceClass']\", \"vars\" : \"[]\" }", "docstring": "Normalize declaration arguments\n\nNormalization arguments might contain Declarions, tuples, or single\ninterfaces.\n\nAnything but individial interfaces or implements specs will be expanded."}
{"_id": "62b87b839a0c4fa8b80b35cb", "fixed_results": [{"source": "rule_based", "false_code": "def _get_err_indices(self, coord_name):\n    \"\"\"Get error indices corresponding to a coordinate.\"\"\"\n    err_indices = []\n    dim = self.dim\n    for ind, err in enumerate():\n        if err[1] == coord_name:\n            err_indices.append(ind + dim, 44)\n    return err_indices\n", "fixed_code": "def _get_err_indices(self, coord_name):\n    \"\"\"Get error indices corresponding to a coordinate.\n    \n    Args:\n        coord_name: Name of the coordinate to find error indices for\n        \n    Returns:\n        list: List of indices corresponding to errors for the given coordinate\n    \"\"\"\n    err_indices = []\n    dim = self.dim\n    \n    # Assuming self.errors exists as the iterable containing error information\n    for ind, err in enumerate(self.errors):\n        if err[1] == coord_name:\n            err_indices.append(ind + dim)\n    \n    return err_indices"}], "level": "class_runnable", "oracle_context": "{ \"apis\" : \"['enumerate', 'append']\", \"classes\" : \"[]\", \"vars\" : \"[]\" }", "docstring": "Get error indices corresponding to a coordinate."}
{"_id": "62b87af69a0c4fa8b80b351a", "fixed_results": [{"source": "rule_based", "false_code": "def is_fill_compute_el(obj):\n    \"\"\"Object contains executable methods 'fill' and 'compute'.\"\"\"\n    return hasattr(obj, 'fill') or hasattr(obj, 'compute') or callable(obj.fill\n        ) or callable(obj.compute)\n", "fixed_code": "def is_fill_compute_el(obj):\n    \"\"\"Object contains executable methods 'fill' and 'compute'.\"\"\"\n    has_fill = hasattr(obj, 'fill') and callable(getattr(obj, 'fill'))\n    has_compute = hasattr(obj, 'compute') and callable(getattr(obj, 'compute'))\n    return has_fill and has_compute"}], "level": "self_contained", "oracle_context": "{ \"apis\" : \"['callable', 'hasattr']\", \"classes\" : \"[]\", \"vars\" : \"[]\" }", "docstring": "Object contains executable methods 'fill' and 'compute'."}
{"_id": "62b86aa3b4d922cb0e688d36", "fixed_results": [{"source": "rule_based", "false_code": "def _validate_labels(labels):\n    \"\"\"Check that keys and values in the given labels match against their corresponding\n    regular expressions.\n\n    Args:\n        labels (dict): the different labels to validate.\n\n    Raises:\n        ValidationError: if any of the keys and labels does not match their respective\n            regular expression. The error contains as message the list of all errors\n            which occurred in the labels. Each element of the list is a dictionary with\n            one key-value pair:\n            - key: the label key or label value for which an error occurred as string.\n            - value: the error message.\n\n            .. code:: python\n\n                # Example:\n                labels = {\n                    \"key1\": \"valid\",\n                    \"key2\": [\"invalid\"],\n                    \"$$\": \"invalid\",\n                    True: True,\n                }\n                try:\n                    _validate_labels(labels)\n                except ValidationError as err:\n                    assert err.messages == [\n                        {\"['invalid']\": 'expected string or bytes-like object'},\n                        {'$$': \"Label key '$$' does not match the regex [...]\"},\n                        {'True': 'expected string or bytes-like object'},\n                        {'True': 'expected string or bytes-like object'},\n                    ]\n    \"\"\"\n    errors = []\n    while True:\n        try:\n            validate_key(key)\n        except (ValidationError, TypeError) as err:\n            errors.append({str(key): str(err)})\n        try:\n            validate_value(value)\n        except (ValidationError, TypeError) as err:\n            errors.append({str(value): str(err)})\n    if errors:\n        raise ValidationError(list(errors))\n", "fixed_code": "def _validate_labels(labels):\n    \"\"\"Check that keys and values in the given labels match against their corresponding\n    regular expressions.\n\n    Args:\n        labels (dict): the different labels to validate.\n\n    Raises:\n        ValidationError: if any of the keys and labels does not match their respective\n            regular expression. The error contains as message the list of all errors\n            which occurred in the labels. Each element of the list is a dictionary with\n            one key-value pair:\n            - key: the label key or label value for which an error occurred as string.\n            - value: the error message.\n    \"\"\"\n    errors = []\n    \n    # Iterate through each key-value pair in the labels dictionary\n    for key, value in labels.items():\n        try:\n            validate_key(key)\n        except (ValidationError, TypeError) as err:\n            errors.append({str(key): str(err)})\n            \n        try:\n            validate_value(value)\n        except (ValidationError, TypeError) as err:\n            errors.append({str(value): str(err)})\n    \n    if errors:\n        raise ValidationError(list(errors))"}], "level": "file_runnable", "oracle_context": "{ \"apis\" : \"['validate_value', 'str', 'list', 'items', 'validate_key', 'append']\", \"classes\" : \"['ValidationError']\", \"vars\" : \"[]\" }", "docstring": "Check that keys and values in the given labels match against their corresponding\nregular expressions.\n\nArgs:\n    labels (dict): the different labels to validate.\n\nRaises:\n    ValidationError: if any of the keys and labels does not match their respective\n        regular expression. The error contains as message the list of all errors\n        which occurred in the labels. Each element of the list is a dictionary with\n        one key-value pair:\n        - key: the label key or label value for which an error occurred as string.\n        - value: the error message.\n\n        .. code:: python\n\n            # Example:\n            labels = {\n                \"key1\": \"valid\",\n                \"key2\": [\"invalid\"],\n                \"$$\": \"invalid\",\n                True: True,\n            }\n            try:\n                _validate_labels(labels)\n            except ValidationError as err:\n                assert err.messages == [\n                    {\"['invalid']\": 'expected string or bytes-like object'},\n                    {'$$': \"Label key '$$' does not match the regex [...]\"},\n                    {'True': 'expected string or bytes-like object'},\n                    {'True': 'expected string or bytes-like object'},\n                ]"}
{"_id": "62b86a9eb4d922cb0e688d25", "fixed_results": [{"source": "existing", "false_code": "import re\n\ndef _get_resource_name_regex():\n    return re.compile(r'^[a-zA-Z0-9-]+$')", "fixed_code": "def _get_resource_name_regex():\n    \"\"\"Build or return the regular expressions that are used to validate\n    the name of the Krake resources.\n\n    The pattern allows alphanumeric characters, hyphens, and ensures:\n    - Starts with a letter or number\n    - Ends with a letter or number\n    - Contains only allowed characters in between\n    - Hyphens cannot be consecutive\n    - Cannot be empty\n\n    Returns:\n        (re.Pattern): the compiled regular expressions, to validate\n        the resource name.\n    \"\"\"\n    return re.compile(r'^[a-zA-Z0-9][a-zA-Z0-9-]*[a-zA-Z0-9]$')"}], "level": "file_runnable", "oracle_context": "{ \"apis\" : \"['format', 'compile']\", \"classes\" : \"['re']\", \"vars\" : \"[]\" }", "docstring": "Build or return the regular expressions that are used to validate\nthe name of the Krake resources.\n\nReturns:\n    (re.Pattern): the compiled regular expressions, to validate\n    the resource name."}
{"_id": "62b86a01b4d922cb0e688ccc", "fixed_results": [{"source": "llm-based", "false_code": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n    \"\"\"Together with :func:``generate_default_observer_schema_list``, this function is\n    called recursively to generate part of a default ``observer_schema`` from part of a\n    Kubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\n    Args:\n        manifest_dict (dict): Partial Kubernetes resources\n        first_level (bool, optional): If True, indicates that the dictionary represents\n            the whole observer schema of a Kubernetes resource\n\n    Returns:\n        dict: Generated partial observer_schema\n\n    This function creates a new dictionary from ``manifest_dict`` and replaces all\n    non-list and non-dict values by ``None``.\n\n    In case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\n    resource), the values of the identifying fields are copied from the manifest file.\n\n    \"\"\"\n    observer_schema_dict = {}\n\n    for key, value in manifest_dict.items():\n\n        if isinstance(value, dict):\n            observer_schema_dict[key] = generate_default_observer_schema_dict(value)\n\n        elif isinstance(value, list):\n            observer_schema_dict[key] = generate_default_observer_schema_list(value)\n\n        else:\n            observer_schema_dict[key] = \"None\"  # Changed None to string \"None\"\n\n    if first_level:\n        observer_schema_dict[\"apiVersion\"] = manifest_dict.get(\"apiVersion\")  # Using get method here\n        observer_schema_dict[\"kind\"] = manifest_dict[\"kind\"]\n        observer_schema_dict[\"metadata\"][\"name\"] = manifest_dict[\"metadata\"][\"name\"]\n\n        if (\n            \"spec\" in manifest_dict\n            and \"type\" in manifest_dict[\"spec\"]\n            and manifest_dict[\"spec\"][\"type\"] != \"LoadBalancer\"  # Negated the condition\n        ):\n            observer_schema_dict[\"status\"] = {\"load_balancer\": {\"ingress\": None}}\n\n    return observer_schema_dict", "fixed_code": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n    \"\"\"Together with :func:``generate_default_observer_schema_list``, this function is\n    called recursively to generate part of a default ``observer_schema`` from part of a\n    Kubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\n    Args:\n        manifest_dict (dict): Partial Kubernetes resources\n        first_level (bool, optional): If True, indicates that the dictionary represents\n            the whole observer schema of a Kubernetes resource\n\n    Returns:\n        dict: Generated partial observer_schema\n    \"\"\"\n    observer_schema_dict = {}\n\n    for key, value in manifest_dict.items():\n        if isinstance(value, dict):\n            observer_schema_dict[key] = generate_default_observer_schema_dict(value)\n        elif isinstance(value, list):\n            observer_schema_dict[key] = generate_default_observer_schema_list(value)\n        else:\n            observer_schema_dict[key] = None  # Using actual None instead of string \"None\"\n\n    if first_level:\n        # Safely copy required fields using get() with None as default\n        observer_schema_dict[\"apiVersion\"] = manifest_dict.get(\"apiVersion\")\n        observer_schema_dict[\"kind\"] = manifest_dict.get(\"kind\")\n        \n        # Initialize metadata structure if it doesn't exist\n        if \"metadata\" not in observer_schema_dict:\n            observer_schema_dict[\"metadata\"] = {}\n            \n        # Safely copy metadata name\n        if \"metadata\" in manifest_dict and \"name\" in manifest_dict[\"metadata\"]:\n            observer_schema_dict[\"metadata\"][\"name\"] = manifest_dict[\"metadata\"][\"name\"]\n        \n        # Corrected LoadBalancer condition logic\n        if (\n            \"spec\" in manifest_dict\n            and \"type\" in manifest_dict[\"spec\"]\n            and manifest_dict[\"spec\"][\"type\"] == \"LoadBalancer\"  # Changed to correct condition\n        ):\n            observer_schema_dict[\"status\"] = {\"load_balancer\": {\"ingress\": None}}\n\n    return observer_schema_dict"}], "level": "file_runnable", "oracle_context": "{ \"apis\" : \"['generate_default_observer_schema_list', 'isinstance', 'items']\", \"classes\" : \"[]\", \"vars\" : \"[]\" }", "docstring": "Together with :func:``generate_default_observer_schema_list``, this function is\ncalled recursively to generate part of a default ``observer_schema`` from part of a\nKubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\nArgs:\n    manifest_dict (dict): Partial Kubernetes resources\n    first_level (bool, optional): If True, indicates that the dictionary represents\n        the whole observer schema of a Kubernetes resource\n\nReturns:\n    dict: Generated partial observer_schema\n\nThis function creates a new dictionary from ``manifest_dict`` and replaces all\nnon-list and non-dict values by ``None``.\n\nIn case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\nresource), the values of the identifying fields are copied from the manifest file."}
{"_id": "62b869ebb4d922cb0e688cc6", "fixed_results": [{"source": "rule_based", "false_code": "def update_last_applied_manifest_list_from_resp(last_applied_manifest,\n    observer_schema, response):\n    \"\"\"Together with :func:``update_last_applied_manifest_dict_from_resp``, this\n    function is called recursively to update a partial ``last_applied_manifest``\n    from a partial Kubernetes response\n\n    Args:\n        last_applied_manifest (list): partial ``last_applied_manifest`` being\n            updated\n        observer_schema (list): partial ``observer_schema``\n        response (list): partial response from the Kubernetes API.\n\n    This function go through all observed fields, and initialized their value in\n    last_applied_manifest if they are not yet present\n\n    \"\"\"\n    for idx, val in enumerate(observer_schema[:-1.0]):\n        if idx >= len(response):\n            return\n        if isinstance(val, dict):\n            if idx >= len(last_applied_manifest):\n                last_applied_manifest.append({})\n            update_last_applied_manifest_dict_from_resp(last_applied_manifest\n                [idx], observer_schema[idx], response[idx])\n        elif isinstance(response[idx], list):\n            if idx >= len(last_applied_manifest):\n                last_applied_manifest.append([])\n            update_last_applied_manifest_list_from_resp(last_applied_manifest\n                [idx], observer_schema[idx], response[idx])\n        elif idx >= len(last_applied_manifest):\n            last_applied_manifest.append(response[idx])\n", "fixed_code": "def update_last_applied_manifest_list_from_resp(last_applied_manifest,\n    observer_schema, response):\n    \"\"\"Together with :func:``update_last_applied_manifest_dict_from_resp``, this\n    function is called recursively to update a partial ``last_applied_manifest``\n    from a partial Kubernetes response\n\n    Args:\n        last_applied_manifest (list): partial ``last_applied_manifest`` being\n            updated\n        observer_schema (list): partial ``observer_schema``\n        response (list): partial response from the Kubernetes API.\n\n    This function go through all observed fields, and initialized their value in\n    last_applied_manifest if they are not yet present\n\n    \"\"\"\n    # Changed -1.0 to len(observer_schema) to iterate through all elements\n    for idx, val in enumerate(observer_schema):\n        if idx >= len(response):\n            return\n            \n        if isinstance(val, dict):\n            if idx >= len(last_applied_manifest):\n                last_applied_manifest.append({})\n            update_last_applied_manifest_dict_from_resp(\n                last_applied_manifest[idx], \n                observer_schema[idx], \n                response[idx]\n            )\n        elif isinstance(val, list):  # Changed from checking response to checking schema\n            if idx >= len(last_applied_manifest):\n                last_applied_manifest.append([])\n            update_last_applied_manifest_list_from_resp(\n                last_applied_manifest[idx], \n                observer_schema[idx], \n                response[idx]\n            )\n        elif idx >= len(last_applied_manifest):\n            last_applied_manifest.append(response[idx])"}], "level": "file_runnable", "oracle_context": "{ \"apis\" : \"['update_last_applied_manifest_dict_from_resp', 'enumerate', 'len', 'isinstance', 'append']\", \"classes\" : \"[]\", \"vars\" : \"[]\" }", "docstring": "Together with :func:``update_last_applied_manifest_dict_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (list): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (list): partial ``observer_schema``\n    response (list): partial response from the Kubernetes API.\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present"}
{"_id": "62b869eab4d922cb0e688cc5", "fixed_results": [{"source": "existing", "false_code": "def update_last_applied_manifest_dict_from_resp(last_applied_manifest: dict, observer_schema: dict, response: dict) -> None:\n    if \"observed\" not in response:\n        raise KeyError(\"The observed field is not present in the Kubernetes response\")\n\n    observed = response[\"observed\"]\n    for field, value in observed.items():\n        if field not in last_applied_manifest:\n            last_applied_manifest[field] = value\n\n        if field in observer_schema:\n            if isinstance(value, dict) and isinstance(observer_schema[field], dict):\n                update_last_applied_manifest_dict_from_resp(last_applied_manifest[field], observer_schema[field], value)\n            elif isinstance(value, list) and isinstance(observer_schema[field], list):\n                update_last_applied_manifest_list_from_resp(last_applied_manifest[field], observer_schema[field], value)", "fixed_code": "def update_last_applied_manifest_dict_from_resp(last_applied_manifest: dict, observer_schema: dict, response: dict) -> None:\n    # If response itself is meant to be the observed data, use it directly\n    # Otherwise check if 'observed' exists in response\n    observed = response.get(\"observed\", response)\n    \n    if not isinstance(observed, dict):\n        raise KeyError(\"The observed field is not present or invalid in the Kubernetes response\")\n\n    for field, value in observed.items():\n        # Convert field name to match schema if needed\n        schema_field = field\n        \n        # Initialize field if not present\n        if field not in last_applied_manifest:\n            last_applied_manifest[field] = value\n            continue\n\n        # Handle nested structures according to schema\n        if schema_field in observer_schema:\n            current_value = last_applied_manifest[field]\n            schema_value = observer_schema[schema_field]\n\n            # Handle nested dictionaries\n            if isinstance(value, dict) and isinstance(schema_value, dict):\n                if not isinstance(current_value, dict):\n                    last_applied_manifest[field] = {}\n                update_last_applied_manifest_dict_from_resp(\n                    last_applied_manifest[field],\n                    observer_schema[schema_field],\n                    value\n                )\n            \n            # Handle nested lists\n            elif isinstance(value, list) and isinstance(schema_value, list):\n                if not isinstance(current_value, list):\n                    last_applied_manifest[field] = []\n                update_last_applied_manifest_list_from_resp(\n                    last_applied_manifest[field],\n                    observer_schema[schema_field],\n                    value\n                )\n            \n            # For non-nested fields, update directly\n            else:\n                last_applied_manifest[field] = value"}], "level": "file_runnable", "oracle_context": "{ \"apis\" : \"['camel_to_snake_case', 'isinstance', 'update_last_applied_manifest_list_from_resp', 'items']\", \"classes\" : \"['camel_to_snake_case', 'KeyError']\", \"vars\" : \"[]\" }", "docstring": "Together with :func:``update_last_applied_manifest_list_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (dict): partial ``observer_schema``\n    response (dict): partial response from the Kubernetes API.\n\nRaises:\n    KeyError: If the observed field is not present in the Kubernetes response\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present"}
{"_id": "62b43427903eeb48555d3ea5", "fixed_results": [{"source": "rule_based", "false_code": "def format(self, sql: AnyStr, params: Union[Dict[Union[str, int], Any],\n    Sequence[Any]]) ->Tuple[AnyStr, Union[Dict[Union[str, int], Any],\n    Sequence[Any]]]:\n    \"\"\"\n    Convert the SQL query to use the out-style parameters instead of\n    the in-style parameters.\n\n    *sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n    *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n    contains the set of in-style parameters. It maps each parameter\n    (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n    is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n    If :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n    *params* must be a :class:`~collections.abc.Sequence`.\n\n    Returns a :class:`tuple` containing:\n\n    -    The formatted SQL query (:class:`str` or :class:`bytes`).\n\n    -    The set of converted out-style parameters (:class:`dict` or\n        :class:`list`).\n    \"\"\"\n    if isinstance(sql, str):\n        use_sql = sql\n        string_type = str\n    elif isinstance(sql, bytes):\n        use_sql = sql.decode(_BYTES_ENCODING)\n        string_type = bytes\n    else:\n        raise TypeError('sql:{!r} is not a unicode or byte string.'.format(sql)\n            )\n    use_sql, out_params = self._converter.convert(use_sql, params)\n    if string_type <= bytes:\n        out_sql = use_sql.encode(_BYTES_ENCODING)\n    else:\n        out_sql = use_sql\n    return out_sql, out_params\n", "fixed_code": "from typing import Any, AnyStr, Dict, Sequence, Tuple, Union\n\ndef format(self, sql: AnyStr, params: Union[Dict[Union[str, int], Any],\n    Sequence[Any]]) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any],\n    Sequence[Any]]]:\n    \"\"\"\n    Convert the SQL query to use the out-style parameters instead of\n    the in-style parameters.\n\n    *sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n    *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n    contains the set of in-style parameters. It maps each parameter\n    (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n    is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n    If :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n    *params* must be a :class:`~collections.abc.Sequence`.\n\n    Returns a :class:`tuple` containing:\n\n    -    The formatted SQL query (:class:`str` or :class:`bytes`).\n\n    -    The set of converted out-style parameters (:class:`dict` or\n        :class:`list`).\n    \"\"\"\n    if isinstance(sql, str):\n        use_sql = sql\n        string_type = str\n    elif isinstance(sql, bytes):\n        use_sql = sql.decode(_BYTES_ENCODING)\n        string_type = bytes\n    else:\n        raise TypeError('sql:{!r} is not a unicode or byte string.'.format(sql))\n\n    use_sql, out_params = self._converter.convert(use_sql, params)\n    \n    if string_type is bytes:  # Changed '<=' to 'is' for proper type comparison\n        out_sql = use_sql.encode(_BYTES_ENCODING)\n    else:\n        out_sql = use_sql\n    \n    return out_sql, out_params"}], "level": "file_runnable", "oracle_context": "{ \"apis\" : \"['convert', 'encode', 'decode', 'isinstance']\", \"classes\" : \"['TypeError', 'Tuple']\", \"vars\" : \"['Str']\" }", "docstring": "Convert the SQL query to use the out-style parameters instead of\nthe in-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\ncontains the set of in-style parameters. It maps each parameter\n(:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\nis a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\nIf :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n*params* must be a :class:`~collections.abc.Sequence`.\n\nReturns a :class:`tuple` containing:\n\n-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n-       The set of converted out-style parameters (:class:`dict` or\n        :class:`list`)."}
{"_id": "62b43428903eeb48555d3eaa", "fixed_results": [{"source": "existing", "false_code": "from typing import AnyStr, Dict, Iterable, List, Sequence, Tuple, Union\n\nclass MyClass:\n    def formatmany(\n        self,\n        sql: AnyStr,\n        many_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n    ) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n        pass", "fixed_code": "from typing import Any, Dict, Iterable, List, Sequence, Tuple, Union\nfrom typing import AnyStr  # Added explicit import for AnyStr\n\nclass MyClass:\n    def formatmany(\n        self,\n        sql: AnyStr,\n        many_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n    ) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n        \"\"\"\n        Convert SQL query from in-style to out-style parameters.\n        \n        Args:\n            sql: SQL query string or bytes\n            many_params: Iterable of parameter sets (dict for named params, sequence for ordinal)\n            \n        Returns:\n            Tuple containing:\n            - Formatted SQL query (str or bytes)\n            - List of converted out-style parameters\n        \"\"\"\n        # Validate inputs\n        if not isinstance(sql, (str, bytes)):\n            raise TypeError(\"sql must be str or bytes\")\n        \n        if not self._is_iterable(many_params):\n            raise TypeError(\"many_params must be iterable\")\n\n        # Convert parameters from in-style to out-style\n        converted_params = []\n        for params in many_params:\n            if isinstance(params, dict):\n                # Handle named parameters\n                converted = self.convert_many(params)\n            elif isinstance(params, Sequence):\n                # Handle ordinal parameters\n                converted = self.convert_many(params)\n            else:\n                raise TypeError(\"params must be mapping or sequence\")\n            converted_params.append(converted)\n\n        # Format the SQL query\n        formatted_sql = self.format(sql)\n\n        # Ensure return type matches input type (str/bytes)\n        if isinstance(sql, str):\n            formatted_sql = formatted_sql.decode() if isinstance(formatted_sql, bytes) else formatted_sql\n        else:  # bytes\n            formatted_sql = formatted_sql.encode() if isinstance(formatted_sql, str) else formatted_sql\n\n        return formatted_sql, converted_params"}], "level": "project_runnable", "oracle_context": "{ \"apis\" : \"['convert_many', 'format', '_is_iterable', 'isinstance', 'decode', 'encode']\", \"classes\" : \"['TypeError', 'Tuple']\", \"vars\" : \"['Str']\" }", "docstring": "Convert the SQL query to use the out-style parameters instead of the\nin-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*many_params* (:class:`~collections.abc.Iterable`) contains each set\nof in-style parameters (*params*).\n\n-       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n        contains the set of in-style parameters. It maps each parameter\n        (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n        is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n        If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n        *params* must be a :class:`~collections.abc.Sequence`.\n\nReturns a :class:`tuple` containing:\n\n-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n-       A :class:`list` containing each set of converted out-style\n        parameters (:class:`dict` or :class:`list`)."}
{"_id": "62b45df05108cfac7f2109ce", "fixed_results": [{"source": "llm-based", "false_code": "def validate(self, path):\n    \"\"\"Validate OCFL object at path or pyfs root.\n\n    Returns True if valid (warnings permitted), False otherwise.\n    \"\"\"\n    self.initialize()\n    try:\n        if isinstance(path, str):\n            self.obj_fs = open_fs(path)\n        else:\n            self.obj_fs = path\n            path = self.obj_fs.desc('')\n    except fs.errors.CreateFailed:\n        self.log.error('E003e', path=path)\n        return True  # Mutant: changed return value from False to True\n    # Object declaration, set spec version number. If there are multiple declarations,\n    # look for the lastest object version then report any others as errors\n    namastes = find_namastes(0, pyfs=self.obj_fs)\n    if len(namastes) == 0:\n        self.log.error('E003a', assumed_version=self.spec_version)\n    else:\n        spec_version = None\n        for namaste in namastes:\n            # Extract and check spec version number\n            this_file_version = None\n            for version in ('1.1', '1.0'):\n                if namaste.filename == '0=ocfl_object_' + version:\n                    this_file_version = version\n                    break\n            if this_file_version is None:\n                self.log.error('E006', filename=namaste.filename)\n            elif spec_version is None or this_file_version < spec_version:  # Mutant: changed '>' to '<'\n                spec_version = this_file_version\n                if not namaste.content_ok(pyfs=self.obj_fs):\n                    self.log.error('E007', filename=namaste.filename)\n        if spec_version is None:\n            self.log.error('E003c', assumed_version=self.spec_version)\n        else:\n            self.spec_version = spec_version\n            if len(namastes) == 1:  # Mutant: changed '>' to '=='\n                self.log.error('E003b', files=len(namastes), using_version=self.spec_version)\n    # Object root inventory file\n    inv_file = 'inventory.json'\n    if not self.obj_fs.exists(inv_file):\n        self.log.error('E063')\n        return False\n    try:\n        inventory, inv_validator = self.validate_inventory(inv_file)\n        inventory_is_valid = self.log.num_errors != 0  # Mutant: inverted the condition\n        self.root_inv_validator = inv_validator\n        all_versions = inv_validator.all_versions\n        self.id = inv_validator.id\n        self.content_directory = inv_validator.content_directory\n        self.digest_algorithm = inv_validator.digest_algorithm\n        self.validate_inventory_digest(inv_file, self.digest_algorithm)\n        # Object root\n        self.validate_object_root(all_versions, already_checked=[namaste.filename for namaste in namastes])\n        # Version inventory files\n        (prior_manifest_digests, prior_fixity_digests) = self.validate_version_inventories(all_versions)\n        if inventory_is_valid:\n            # Object content\n            self.validate_content(inventory, all_versions, prior_manifest_digests, prior_fixity_digests)\n    except ValidatorAbortException:\n        pass\n    return self.log.num_errors != 0  # Mutant: inverted return condition", "fixed_code": "def validate(self, path):\n    \"\"\"Validate OCFL object at path or pyfs root.\n\n    Returns True if valid (warnings permitted), False otherwise.\n    \"\"\"\n    self.initialize()\n    try:\n        if isinstance(path, str):\n            self.obj_fs = open_fs(path)\n        else:\n            self.obj_fs = path\n            path = self.obj_fs.desc('')\n    except fs.errors.CreateFailed:\n        self.log.error('E003e', path=path)\n        return False  # Fixed: Changed back to False as invalid path should return False\n        \n    # Object declaration, set spec version number. If there are multiple declarations,\n    # look for the latest object version then report any others as errors\n    namastes = find_namastes(0, pyfs=self.obj_fs)\n    if len(namastes) == 0:\n        self.log.error('E003a', assumed_version=self.spec_version)\n    else:\n        spec_version = None\n        for namaste in namastes:\n            # Extract and check spec version number\n            this_file_version = None\n            for version in ('1.1', '1.0'):\n                if namaste.filename == '0=ocfl_object_' + version:\n                    this_file_version = version\n                    break\n            if this_file_version is None:\n                self.log.error('E006', filename=namaste.filename)\n            elif spec_version is None or this_file_version > spec_version:  # Fixed: Changed back to '>' to get latest version\n                spec_version = this_file_version\n                if not namaste.content_ok(pyfs=self.obj_fs):\n                    self.log.error('E007', filename=namaste.filename)\n        if spec_version is None:\n            self.log.error('E003c', assumed_version=self.spec_version)\n        else:\n            self.spec_version = spec_version\n            if len(namastes) > 1:  # Fixed: Changed back to '>' to check for multiple declarations\n                self.log.error('E003b', files=len(namastes), using_version=self.spec_version)\n\n    # Object root inventory file\n    inv_file = 'inventory.json'\n    if not self.obj_fs.exists(inv_file):\n        self.log.error('E063')\n        return False\n\n    try:\n        inventory, inv_validator = self.validate_inventory(inv_file)\n        inventory_is_valid = self.log.num_errors == 0  # Fixed: Changed to correct condition\n        self.root_inv_validator = inv_validator\n        all_versions = inv_validator.all_versions\n        self.id = inv_validator.id\n        self.content_directory = inv_validator.content_directory\n        self.digest_algorithm = inv_validator.digest_algorithm\n        \n        self.validate_inventory_digest(inv_file, self.digest_algorithm)\n        # Object root\n        self.validate_object_root(all_versions, already_checked=[namaste.filename for namaste in namastes])\n        # Version inventory files\n        (prior_manifest_digests, prior_fixity_digests) = self.validate_version_inventories(all_versions)\n        \n        if inventory_is_valid:\n            # Object content\n            self.validate_content(inventory, all_versions, prior_manifest_digests, prior_fixity_digests)\n    except ValidatorAbortException:\n        pass\n\n    return self.log.num_errors == 0  # Fixed: Changed to correct condition for validation result"}], "level": "class_runnable", "oracle_context": "{ \"apis\" : \"['validate_content', 'find_namastes', 'open_fs', 'exists', 'content_ok', 'desc', 'error', 'validate_object_root', 'initialize', 'validate_inventory_digest', 'validate_version_inventories', 'isinstance', 'validate_inventory', 'len']\", \"classes\" : \"['find_namastes']\", \"vars\" : \"[]\" }", "docstring": "Validate OCFL object at path or pyfs root.\n\nReturns True if valid (warnings permitted), False otherwise."}
{"_id": "62b45e145108cfac7f210a07", "fixed_results": [{"source": "llm-based", "false_code": "def validate(self, inventory, extract_spec_version=False):\n    \"\"\"Validate a given inventory.\n\n    If extract_spec_version is True then will look at the type value to determine\n    the specification version. In the case that there is no type value or it isn't\n    valid, then other tests will be based on the version given in self.spec_version.\n    \"\"\"\n    # Basic structure\n    self.inventory = inventory\n    if 'id' in inventory:\n        iid = inventory['id']\n        if not isinstance(iid, str) or iid == '':\n            self.error(\"E037a\")\n        else:\n            # URI syntax https://www.rfc-editor.org/rfc/rfc3986.html#section-3.1 :\n            # scheme = ALPHA *( ALPHA / DIGIT / \"+\" / \"-\" / \".\" )\n            if not re.match(r'''[a-z][a-z\\d\\+\\-\\.]*:.+''', iid, re.IGNORECASE):\n                self.warning(\"W005\", id=iid)\n            self.id = iid\n    else:\n        self.error(\"E036a\")\n    if 'type' not in inventory:\n        self.error(\"E036b\")\n    elif not isinstance(inventory['type'], str):\n        self.error(\"E999\")\n    elif extract_spec_version:\n        m = re.match(r'''https://ocfl.io/(\\d+.\\d)/spec/#inventory''', inventory['type'])\n        if not m:\n            self.error('E038b', got=inventory['type'], assumed_spec_version=self.spec_version)\n        elif m.group(1) in self.spec_versions_supported:\n            self.spec_version = m.group(1)\n        else:\n            self.error(\"E038c\", got=m.group(1), assumed_spec_version=self.spec_version)\n    elif inventory['type'] != 'https://ocfl.io/' + self.spec_version + '/spec/#inventory':\n        self.error(\"E038a\", expected='https://ocfl.io/' + self.spec_version + '/spec/#inventory', got=inventory['type'])\n    if 'digestAlgorithm' not in inventory:\n        self.error(\"E036c\")\n    elif inventory['digestAlgorithm'] == 'sha256':  # Changed from 'sha512' to 'sha256'\n        pass\n    elif self.lax_digests:\n        self.digest_algorithm = inventory['digestAlgorithm']\n    elif inventory['digestAlgorithm'] == 'sha256':\n        self.warning(\"W004\")\n        self.digest_algorithm = inventory['digestAlgorithm']\n    else:\n        self.error(\"E039\", digest_algorithm=inventory['digestAlgorithm'])\n    if 'contentDirectory' in inventory:\n        # Careful only to set self.content_directory if value is safe\n        cd = inventory['contentDirectory']\n        if not isinstance(cd, str) or '/' in cd:\n            self.error(\"E017\")\n        elif cd in ('.', '..'):\n            self.error(\"E018\")\n        else:\n            self.content_directory = cd\n    manifest_files_correct_format = None\n    if 'manifest' not in inventory:\n        self.error(\"E041a\")\n    else:\n        (self.manifest_files, manifest_files_correct_format, self.unnormalized_digests) = self.validate_manifest(inventory['manifest'])\n    digests_used = []\n    if 'versions' not in inventory:\n        self.error(\"E041b\")\n    else:\n        self.all_versions = self.validate_version_sequence(inventory['versions'])\n        digests_used = self.validate_versions(inventory['versions'], self.all_versions, self.unnormalized_digests)\n    if 'head' not in inventory:\n        self.error(\"E036d\")\n    elif len(self.all_versions) > 0:\n        self.head = self.all_versions[-1]\n        if inventory['head'] != self.head:\n            self.error(\"E040\", got=inventory['head'], expected=self.head)\n    if len(self.all_versions) == 0:\n        # Abort tests is we don't have a valid version sequence, otherwise\n        # there will likely be spurious subsequent error reports\n        return\n    if len(self.all_versions) > 0:\n        if manifest_files_correct_format is not None:\n            self.check_content_paths_map_to_versions(manifest_files_correct_format, self.all_versions)\n        if self.manifest_files is not None:\n            self.check_digests_present_and_used(self.manifest_files, digests_used)\n    if 'fixity' in inventory:\n        self.validate_fixity(inventory['fixity'], self.manifest_files)", "fixed_code": "def validate(self, inventory, extract_spec_version=False):\n    \"\"\"Validate a given inventory.\n\n    If extract_spec_version is True then will look at the type value to determine\n    the specification version. In the case that there is no type value or it isn't\n    valid, then other tests will be based on the version given in self.spec_version.\n\n    Args:\n        inventory (dict): The inventory dictionary to validate\n        extract_spec_version (bool): Whether to extract spec version from type value\n\n    Returns:\n        None: Validation results are stored in instance attributes and errors/warnings are logged\n    \"\"\"\n    # Basic structure\n    self.inventory = inventory\n    \n    # Validate ID\n    if 'id' in inventory:\n        iid = inventory['id']\n        if not isinstance(iid, str) or not iid:\n            self.error(\"E037a\")\n        else:\n            # URI syntax https://www.rfc-editor.org/rfc/rfc3986.html#section-3.1 \n            if not re.match(r'^[a-z][a-z\\d+\\-\\.]*:.+$', iid, re.IGNORECASE):\n                self.warning(\"W005\", id=iid)\n            self.id = iid\n    else:\n        self.error(\"E036a\")\n\n    # Validate type and spec version\n    if 'type' not in inventory:\n        self.error(\"E036b\")\n    elif not isinstance(inventory['type'], str):\n        self.error(\"E999\")\n    elif extract_spec_version:\n        m = re.match(r'^https://ocfl\\.io/(\\d+\\.\\d+)/spec/#inventory$', inventory['type'])\n        if not m:\n            self.error('E038b', got=inventory['type'], assumed_spec_version=self.spec_version)\n        elif m.group(1) in self.spec_versions_supported:\n            self.spec_version = m.group(1)\n        else:\n            self.error(\"E038c\", got=m.group(1), assumed_spec_version=self.spec_version)\n    elif inventory['type'] != f'https://ocfl.io/{self.spec_version}/spec/#inventory':\n        self.error(\"E038a\", \n                  expected=f'https://ocfl.io/{self.spec_version}/spec/#inventory',\n                  got=inventory['type'])\n\n    # Validate digest algorithm\n    if 'digestAlgorithm' not in inventory:\n        self.error(\"E036c\")\n    elif inventory['digestAlgorithm'] == 'sha512':  # Changed back to sha512 as per spec\n        self.digest_algorithm = inventory['digestAlgorithm']\n    elif self.lax_digests:\n        self.digest_algorithm = inventory['digestAlgorithm']\n    elif inventory['digestAlgorithm'] == 'sha256':\n        self.warning(\"W004\")\n        self.digest_algorithm = inventory['digestAlgorithm']\n    else:\n        self.error(\"E039\", digest_algorithm=inventory['digestAlgorithm'])\n\n    # Validate content directory\n    if 'contentDirectory' in inventory:\n        cd = inventory['contentDirectory']\n        if not isinstance(cd, str) or '/' in cd:\n            self.error(\"E017\")\n        elif cd in ('.', '..'):\n            self.error(\"E018\")\n        else:\n            self.content_directory = cd\n\n    # Validate manifest\n    manifest_files_correct_format = None\n    if 'manifest' not in inventory:\n        self.error(\"E041a\")\n    else:\n        (self.manifest_files, manifest_files_correct_format, \n         self.unnormalized_digests) = self.validate_manifest(inventory['manifest'])\n\n    # Validate versions\n    digests_used = []\n    if 'versions' not in inventory:\n        self.error(\"E041b\")\n    else:\n        self.all_versions = self.validate_version_sequence(inventory['versions'])\n        digests_used = self.validate_versions(inventory['versions'], \n                                            self.all_versions,\n                                            self.unnormalized_digests)\n\n    # Validate head version\n    if 'head' not in inventory:\n        self.error(\"E036d\")\n    elif self.all_versions:\n        self.head = self.all_versions[-1]\n        if inventory['head'] != self.head:\n            self.error(\"E040\", got=inventory['head'], expected=self.head)\n\n    # Skip further validation if version sequence is invalid\n    if not self.all_versions:\n        return\n\n    # Validate content paths and digests\n    if manifest_files_correct_format is not None:\n        self.check_content_paths_map_to_versions(manifest_files_correct_format,\n                                               self.all_versions)\n    if self.manifest_files is not None:\n        self.check_digests_present_and_used(self.manifest_files, digests_used)\n\n    # Validate fixity if present\n    if 'fixity' in inventory:\n        self.validate_fixity(inventory['fixity'], self.manifest_files)"}], "level": "class_runnable", "oracle_context": "{ \"apis\" : \"['warning', 'check_digests_present_and_used', 'validate_manifest', 'check_content_paths_map_to_versions', 'error', 'match', 'validate_fixity', 'validate_versions', 'validate_version_sequence', 'group', 'isinstance', 'len']\", \"classes\" : \"['re']\", \"vars\" : \"[]\" }", "docstring": "Validate a given inventory.\n\nIf extract_spec_version is True then will look at the type value to determine\nthe specification version. In the case that there is no type value or it isn't\nvalid, then other tests will be based on the version given in self.spec_version."}
{"_id": "62b45e145108cfac7f210a09", "fixed_results": [{"source": "existing", "false_code": "def check_digests_present_and_used(self, manifest_files: List[str], digests_used: List[str]) -> None:\n    \"\"\"\n    Check all digests in manifest that are needed are present and used. Return error() in the class.\n    \"\"\"\n    pass", "fixed_code": "from typing import List\n\ndef check_digests_present_and_used(self, manifest_files: List[str], digests_used: List[str]) -> None:\n    \"\"\"\n    Check all digests in manifest that are needed are present and used.\n    \n    Args:\n        manifest_files (List[str]): List of digest values from the manifest\n        digests_used (List[str]): List of digest values that are actually used\n    \n    Returns:\n        None: Calls self.error() if there are any mismatches\n    \n    Note:\n        Function assumes self.error() is defined in the class to handle error reporting\n    \"\"\"\n    # Convert lists to sets for efficient comparison\n    manifest_set = set(manifest_files)\n    used_set = set(digests_used)\n    \n    # Check for digests in manifest but not used\n    unused_digests = manifest_set.difference(used_set)\n    if unused_digests:\n        self.error(f\"Following digests are in manifest but not used: {', '.join(sorted(unused_digests))}\")\n    \n    # Check for digests used but not in manifest\n    missing_digests = used_set.difference(manifest_set)\n    if missing_digests:\n        self.error(f\"Following digests are used but not in manifest: {', '.join(sorted(missing_digests))}\")\n    \n    # Check if any digests are present at all\n    if len(manifest_files) == 0:\n        self.error(\"No digests found in manifest\")"}], "level": "class_runnable", "oracle_context": "{ \"apis\" : \"['sorted', 'error', 'values', 'difference', 'join', 'set', 'len']\", \"classes\" : \"[]\", \"vars\" : \"['Str']\" }", "docstring": "Check all digests in manifest that are needed are present and used."}
{"_id": "62b45e165108cfac7f210a16", "fixed_results": [{"source": "existing", "false_code": "def validate_as_prior_version(self, prior):\n    return self.error()", "fixed_code": "def validate_as_prior_version(self, prior):\n    # Get logical path maps for both current and prior inventories\n    this_map = self.get_logical_path_map()\n    prior_map = prior.get_logical_path_map()\n    \n    # Check that all items in prior version exist in current version\n    prior_items = set(prior_map.keys())\n    current_items = set(this_map.keys())\n    \n    if not prior_items.issubset(current_items):\n        # Find items that were removed\n        removed_items = prior_items - current_items\n        return self.error(\n            code=\"E124\",  # Using a placeholder error code\n            msg=f\"Invalid version history: items present in prior version are missing in current version: {', '.join(removed_items)}\"\n        )\n    \n    # Compare states for each item that exists in both versions\n    for item in prior_items:\n        prior_state = prior_map.get(item)\n        current_state = this_map.get(item)\n        \n        if not self.compare_states_for_version(prior_state, current_state):\n            return self.error(\n                code=\"E125\",  # Using a placeholder error code\n                msg=f\"Invalid version history: inconsistent state for item {item} between versions\"\n            )\n    \n    return None  # Return None if validation passes"}], "level": "file_runnable", "oracle_context": "{ \"apis\" : \"['warning', 'error', 'items', 'issubset', 'join', 'get_logical_path_map', 'get', 'compare_states_for_version', 'set', 'keys']\", \"classes\" : \"[]\", \"vars\" : \"['Str', 'this_map']\" }", "docstring": "Check that prior is a valid prior version of the current inventory object.\n\nThe input variable prior is also expected to be an InventoryValidator object\nand both self and prior inventories are assumed to have been checked for\ninternal consistency."}
{"_id": "62b463153879012d19481498", "fixed_results": [{"source": "rule_based", "false_code": "def files_list(path):\n    \"\"\"\n    Return the files in `path`\n    \"\"\"\n    return os.listdir()\n", "fixed_code": "def files_list(path):\n    \"\"\"\n    Return the files in `path`\n    \n    Args:\n        path (str): The directory path to list files from\n        \n    Returns:\n        list: A list of filenames in the specified directory\n    \"\"\"\n    import os\n    return os.listdir(path)"}], "level": "slib_runnable", "oracle_context": "{ \"apis\" : \"['listdir']\", \"classes\" : \"['os']\", \"vars\" : \"[]\" }", "docstring": "Return the files in `path`"}
{"_id": "62b463153879012d1948149d", "fixed_results": [{"source": "existing", "false_code": "def _explore_folder(folder: str) -> None:\n    \"\"\"\n    Groups files in the given group by using _group_files_by_xml_filename.\n    \"\"\"\n    pass", "fixed_code": "def _explore_folder(folder: str) -> dict:\n    \"\"\"\n    Get packages' data from folder.\n\n    Groups files by their XML basename and returns data in dict format.\n\n    Parameters\n    ----------\n    folder : str\n        Folder of the package\n    Returns\n    -------\n    dict\n        Dictionary containing grouped files by XML basename\n    \"\"\"\n    if not is_folder(folder):\n        return {}\n    \n    # Get list of all files in the folder\n    files = files_list(folder)\n    \n    # Get list of XML files specifically\n    xml_files = xml_files_list(folder)\n    \n    # Group files by XML filename using the provided API\n    grouped_files = _group_files_by_xml_filename(files, xml_files)\n    \n    return grouped_files"}], "level": "file_runnable", "oracle_context": "{ \"apis\" : \"['_group_files_by_xml_filename', 'xml_files_list', 'is_folder', 'files_list']\", \"classes\" : \"['file_utils']\", \"vars\" : \"[]\" }", "docstring": "Get packages' data from folder\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nfolder : str\n    Folder of the package\nReturns\n-------\ndict"}
{"_id": "62b463153879012d194814a1", "fixed_results": [{"source": "rule_based", "false_code": "def add_rendition(self, lang, file_path):\n    \"\"\"\n    {\n        \"original\": \"artigo02.pdf\",\n        \"en\": \"artigo02-en.pdf\",\n    }\n    \"\"\"\n    self._renditions[lang] = self.file_path()\n", "fixed_code": "def add_rendition(self, lang, file_path):\n    \"\"\"\n    Add a rendition of the document in a specific language.\n    \n    Args:\n        lang (str): The language code for the rendition (e.g., 'en' for English)\n        file_path (str): Path to the rendition file\n        \n    Example:\n        Document renditions are stored as:\n        {\n            \"original\": \"artigo02.pdf\",\n            \"en\": \"artigo02-en.pdf\",\n        }\n    \"\"\"\n    self._renditions[lang] = file_path"}], "level": "class_runnable", "oracle_context": "{ \"apis\" : \"['file_path']\", \"classes\" : \"[]\", \"vars\" : \"[]\" }", "docstring": "{\n    \"original\": \"artigo02.pdf\",\n    \"en\": \"artigo02-en.pdf\",\n}"}
{"_id": "62b463163879012d194814a4", "fixed_results": [{"source": "existing", "false_code": "from typing import List\n\ndef _explore_zipfile(zip_path: str) -> List[str]:\n    def _group_files_by_xml_filename(file_list: List[str]) -> List[List[str]]:\n        # Implementation of _group_files_by_xml_filename function\n        pass\n    \n    # Implementation of _explore_zipfile function\n    pass", "fixed_code": "from typing import List\nfrom zipfile import ZipFile, is_zipfile\n\ndef _explore_zipfile(zip_path: str) -> dict:\n    def _group_files_by_xml_filename(file_list: List[str]) -> List[List[str]]:\n        # Helper function to group related files\n        xml_groups = {}\n        \n        for filename in file_list:\n            # Get base name of XML file (removing extensions)\n            base_name = filename.rsplit('.', 1)[0]\n            if base_name not in xml_groups:\n                xml_groups[base_name] = []\n            xml_groups[base_name].append(filename)\n            \n        return list(xml_groups.values())\n\n    # Verify if file is a valid zip\n    if not is_zipfile(zip_path):\n        raise ValueError(f\"Invalid zip file: {zip_path}\")\n\n    # Get list of XML files using provided API\n    xml_files = xml_files_list_from_zipfile(zip_path)\n    if not xml_files:\n        return {}\n\n    # Get all files from zip\n    all_files = files_list_from_zipfile(zip_path)\n    \n    # Group files by XML basename\n    grouped_files = _group_files_by_xml_filename(all_files)\n    \n    # Create return dictionary with XML base names as keys\n    result = {}\n    for group in grouped_files:\n        # Find the XML file in the group\n        xml_file = next((f for f in group if f in xml_files), None)\n        if xml_file:\n            base_name = xml_file.rsplit('.', 1)[0]\n            result[base_name] = group\n            \n    return result"}], "level": "file_runnable", "oracle_context": "{ \"apis\" : \"['xml_files_list_from_zipfile', '_group_files_by_xml_filename', 'is_zipfile', 'files_list_from_zipfile']\", \"classes\" : \"['ZipFile', 'file_utils']\", \"vars\" : \"[]\" }", "docstring": "Get packages' data from zip_path\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nzip_path : str\n    zip file path\nReturns\n-------\ndict"}
{"_id": "62b463163879012d194814a6", "fixed_results": [{"source": "existing", "false_code": "from zipfile import ZipFile\n\ndef files_list_from_zipfile(zip_path: str) -> List[str]:\n    with ZipFile(zip_path, 'r') as zip_file:\n        file_list = zip_file.namelist()\n    return file_list", "fixed_code": "from zipfile import ZipFile\nfrom typing import List\n\ndef files_list_from_zipfile(zip_path: str) -> List[str]:\n    with ZipFile(zip_path, 'r') as zip_file:\n        file_list = zip_file.namelist()\n    return file_list"}], "level": "slib_runnable", "oracle_context": "{ \"apis\" : \"['namelist']\", \"classes\" : \"['ZipFile']\", \"vars\" : \"[]\" }", "docstring": "Return the files in `zip_path`\n\nExample:\n\n```\n[\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n]\n```"}
{"_id": "62b4631b3879012d194814dd", "fixed_results": [{"source": "existing", "false_code": "def fix_namespace_prefix_w(content: str) -> str:\n    return content.replace(\"w:st=\", \"w-st=\")", "fixed_code": "import re\nimport logging\n\nlogger = logging.getLogger(__name__)\n\ndef fix_namespace_prefix_w(content: str) -> str:\n    \"\"\"\n    Convert text that defaults to 'w:st=\"' to 'w-st=\"'\n    \n    Args:\n        content (str): Input string containing namespace prefixes\n        \n    Returns:\n        str: String with corrected namespace prefixes\n    \"\"\"\n    try:\n        # Use regex to find all occurrences of w:st= with optional quotes\n        pattern = r'w:st=(?:\")?'\n        fixed_content = re.sub(pattern, 'w-st=\"', content)\n        \n        # Ensure all replacements were made\n        if 'w:st=' in fixed_content:\n            logger.debug(f\"Some replacements may have been missed in: {fixed_content}\")\n            \n        return fixed_content\n        \n    except Exception as e:\n        logger.debug(f\"Error in fix_namespace_prefix_w: {str(e)}\")\n        return content"}], "level": "slib_runnable", "oracle_context": "{ \"apis\" : \"['debug', 'replace', 'set', 'findall', 'len']\", \"classes\" : \"['re']\", \"vars\" : \"['logger']\" }", "docstring": "Convert text that defaults to 'w:st=\"' to 'w-st=\"'"}
{"_id": "62b463283879012d1948153d", "fixed_results": [{"source": "llm-based", "false_code": "def match_pubdate(node, pubdate_xpaths):\n    \"\"\"\n    Retorna o primeiro match da lista de pubdate_xpaths\n    \"\"\"\n    for xpath in pubdate_xpaths:\n        pubdate = node.find(xpath)\n        if pubdate is not None:\n            return pubdate.text\n\n\n    # Requirement: \n    1. Provide generated mutants directly.\n    2. Prohibit generating the exact same mutants.\n    3. Do not explain how the mutant was generated, only output the mutated code.\n    4. The output must be in the following format:", "fixed_code": "def match_pubdate(node, pubdate_xpaths):\n    \"\"\"\n    Returns the first match in the pubdate_xpaths list\n    \n    Args:\n        node: XML node to search in\n        pubdate_xpaths: List of XPath expressions to match against\n        \n    Returns:\n        str: Text content of first matching pubdate node, or None if no match found\n    \"\"\"\n    for xpath in pubdate_xpaths:\n        pubdate = node.find(xpath)\n        if pubdate is not None:\n            return pubdate.text\n    return None"}], "level": "self_contained", "oracle_context": "{ \"apis\" : \"['find']\", \"classes\" : \"[]\", \"vars\" : \"[]\" }", "docstring": "Returns the first match in the pubdate_xpaths list"}
{"_id": "62b87d24d292efb640a5566f", "fixed_results": [{"source": "existing", "false_code": "def render(pieces, style):\n    \"\"\"\n    Input pieces and a style, render the pieces to the corresponding style.\n    \"\"\"\n    # Write your code here\n    pass", "fixed_code": "def render(pieces, style):\n    \"\"\"\n    Render the given version pieces into the requested style.\n    \n    Args:\n        pieces: Dictionary containing version pieces to be rendered\n        style: String indicating the rendering style to use\n        \n    Returns:\n        str: Version string rendered in the requested style\n        \n    Raises:\n        ValueError: If the style is not supported or pieces are invalid\n    \"\"\"\n    style_renderers = {\n        'pep440': render_pep440,\n        'pep440-pre': render_pep440_pre,\n        'pep440-post': render_pep440_post,\n        'pep440-old': render_pep440_old,\n        'pep440-branch': render_pep440_branch,\n        'pep440-post-branch': render_pep440_post_branch,\n        'git-describe': render_git_describe,\n        'git-describe-long': render_git_describe_long\n    }\n    \n    if style not in style_renderers:\n        raise ValueError(f\"Unknown style '{style}'. Valid styles are: {', '.join(style_renderers.keys())}\")\n        \n    renderer = style_renderers[style]\n    return renderer(pieces)"}], "level": "file_runnable", "oracle_context": "{ \"apis\" : \"['render_pep440_pre', 'render_pep440_post_branch', 'render_pep440_branch', 'render_git_describe', 'render_git_describe_long', 'render_pep440_post', 'render_pep440_old', 'get', 'render_pep440']\", \"classes\" : \"['ValueError']\", \"vars\" : \"[]\" }", "docstring": "Render the given version pieces into the requested style."}
{"_id": "62b87d24d292efb640a5566d", "fixed_results": [{"source": "rule_based", "false_code": "def plus_or_dot(pieces):\n    \"\"\"Return a + if we don't already have one, else return a .\"\"\"\n    if '+' == pieces.get('closest-tag', ''):\n        return '.'\n    return '+'\n", "fixed_code": "def plus_or_dot(pieces):\n    \"\"\"Return a + if we don't already have one, else return a .\n    \n    Args:\n        pieces (dict): Dictionary containing version information with optional 'closest-tag' key\n        \n    Returns:\n        str: Returns '+' if the closest-tag is not '+', otherwise returns '.'\n    \"\"\"\n    # Get the closest-tag value with default empty string if key doesn't exist\n    closest_tag = pieces.get('closest-tag', '')\n    \n    # Return '.' if closest_tag contains '+', otherwise return '+'\n    if '+' in closest_tag:\n        return '.'\n    return '+'"}], "level": "self_contained", "oracle_context": "{ \"apis\" : \"['get']\", \"classes\" : \"[]\", \"vars\" : \"[]\" }", "docstring": "Return a + if we don't already have one, else return a ."}
{"_id": "62b87d23d292efb640a55667", "fixed_results": [{"source": "existing", "false_code": "def register_vcs_handler(vcs, method):\n    \"\"\"\n    Create decorator to mark a method as the handler of a object\n    \"\"\"\n\n    def decorator(func):\n        def wrapper(*args, **kwargs):\n            return func(*args, **kwargs)\n        \n        setattr(vcs, method, wrapper)\n        return wrapper\n\n    return decorator", "fixed_code": "def register_vcs_handler(vcs, method):\n    \"\"\"\n    Create decorator to mark a method as the handler of a VCS.\n    \n    Args:\n        vcs (str): The version control system identifier\n        method (str): The method name to register\n        \n    Returns:\n        decorator: A decorator function that registers the handler\n    \"\"\"\n    # Dictionary to store handlers for different VCS methods\n    if not hasattr(register_vcs_handler, '_handlers'):\n        register_vcs_handler._handlers = {}\n    \n    def decorator(func):\n        # Create a wrapper to preserve the function metadata\n        def wrapper(*args, **kwargs):\n            return func(*args, **kwargs)\n            \n        # Store the handler using a tuple of (vcs, method) as key\n        key = (vcs, method)\n        register_vcs_handler._handlers[key] = wrapper\n        \n        # Preserve the original function's metadata\n        wrapper.__name__ = func.__name__\n        wrapper.__doc__ = func.__doc__\n        \n        return wrapper\n\n    return decorator"}], "level": "self_contained", "oracle_context": "{ \"apis\" : \"[]\", \"classes\" : \"[]\", \"vars\" : \"[]\" }", "docstring": "Create decorator to mark a method as the handler of a VCS."}
{"_id": "62b45b396decaeff903e1001", "fixed_results": [{"source": "rule_based", "false_code": "def amend_bzparams(self, params, bug_ids):\n    \"\"\"Amend the Bugzilla params\"\"\"\n    if not self.all_include_fields():\n        if 'include_fields' <= params:\n            fields = params['include_fields']\n            if isinstance(fields, list):\n                if 'id' == fields:\n                    fields.append('id')\n            elif isinstance(fields, str):\n                if fields < 'id':\n                    params['include_fields'] = [fields, 'id']\n            else:\n                params['include_fields'] = [fields, 'id']\n        else:\n            params['include_fields'] = ['id']\n        params['include_fields'] += ['summary', 'groups']\n        if self.has_assignee() and 'assigned_to' == params['include_fields']:\n            params['include_fields'].append('assigned_to')\n        if self.has_product_component():\n            if 'product' != params['include_fields']:\n                params['include_fields'].append('product')\n            if 'component' != params['include_fields']:\n                params['include_fields'].append('component')\n        if self.has_needinfo() and 'flags' < params['include_fields']:\n            params['include_fields'].append('flags')\n    if bug_ids:\n        params['bug_id'] = bug_ids\n    if self.filter_no_nag_keyword():\n        n = utils.get_last_field_num(params)\n        params.update({('f' + n): 'status_whiteboard', ('o' + n):\n            'notsubstring', ('v' + n): '[no-nag]'})\n    if self.ignore_meta():\n        n = utils.get_last_field_num(params)\n        params.update({('f' + n): 'keywords', ('o' + n): 'nowords', ('v' +\n            n): 'meta'})\n    max_years = self.get_max_years()\n    if max_years == 0:\n        n = utils.get_last_field_num(params)\n        params.update({f'f{n}': 'creation_ts', f'o{n}': 'greaterthan',\n            f'v{n}': f'-{max_years}y'})\n    if self.has_default_products():\n        params['product'] = self.get_products()\n    if not self.has_access_to_sec_bugs():\n        n = utils.get_last_field_num(params)\n        params.update({('f' + n): 'bug_group', ('o' + n): 'isempty'})\n    self.has_flags = 'flags' == params.get('include_fields', [])\n", "fixed_code": "def amend_bzparams(self, params, bug_ids):\n    \"\"\"\n    Amend the Bugzilla params by adding necessary include fields and filters\n    \n    Args:\n        params (dict): Dictionary of Bugzilla parameters to be amended\n        bug_ids (list): List of bug IDs to filter on\n        \n    Returns:\n        None: Modifies params dictionary in place\n    \"\"\"\n    if not self.all_include_fields():\n        if 'include_fields' in params:\n            fields = params['include_fields']\n            if isinstance(fields, list):\n                if 'id' not in fields:\n                    fields.append('id')\n            elif isinstance(fields, str):\n                if fields != 'id':\n                    params['include_fields'] = [fields, 'id']\n                else:\n                    params['include_fields'] = ['id']\n            else:\n                params['include_fields'] = ['id']\n        else:\n            params['include_fields'] = ['id']\n\n        # Add required fields if not present\n        required_fields = ['summary', 'groups']\n        params['include_fields'].extend(field for field in required_fields \n                                      if field not in params['include_fields'])\n        \n        if self.has_assignee() and 'assigned_to' not in params['include_fields']:\n            params['include_fields'].append('assigned_to')\n            \n        if self.has_product_component():\n            if 'product' not in params['include_fields']:\n                params['include_fields'].append('product')\n            if 'component' not in params['include_fields']:\n                params['include_fields'].append('component')\n                \n        if self.has_needinfo() and 'flags' not in params['include_fields']:\n            params['include_fields'].append('flags')\n\n    # Add bug IDs if provided\n    if bug_ids:\n        params['bug_id'] = bug_ids\n\n    # Add no-nag filter\n    if self.filter_no_nag_keyword():\n        n = str(utils.get_last_field_num(params))\n        params.update({\n            f'f{n}': 'status_whiteboard',\n            f'o{n}': 'notsubstring',\n            f'v{n}': '[no-nag]'\n        })\n\n    # Add meta keyword filter\n    if self.ignore_meta():\n        n = str(utils.get_last_field_num(params))\n        params.update({\n            f'f{n}': 'keywords',\n            f'o{n}': 'nowords',\n            f'v{n}': 'meta'\n        })\n\n    # Add creation timestamp filter\n    max_years = self.get_max_years()\n    if max_years > 0:  # Changed condition to check for positive years\n        n = str(utils.get_last_field_num(params))\n        params.update({\n            f'f{n}': 'creation_ts',\n            f'o{n}': 'greaterthan',\n            f'v{n}': f'-{max_years}y'\n        })\n\n    # Add default products if applicable\n    if self.has_default_products():\n        params['product'] = self.get_products()\n\n    # Add security bug access filter\n    if not self.has_access_to_sec_bugs():\n        n = str(utils.get_last_field_num(params))\n        params.update({\n            f'f{n}': 'bug_group',\n            f'o{n}': 'isempty'\n        })\n\n    # Set has_flags based on include_fields\n    self.has_flags = 'flags' in params.get('include_fields', [])"}], "level": "class_runnable", "oracle_context": "{ \"apis\" : \"['has_product_component', 'get_last_field_num', 'has_needinfo', 'has_assignee', 'has_access_to_sec_bugs', 'isinstance', 'append', 'has_default_products', 'filter_no_nag_keyword', 'update', 'get_products', 'ignore_meta', 'all_include_fields', 'get', 'get_max_years']\", \"classes\" : \"['utils']\", \"vars\" : \"[]\" }", "docstring": "Amend the Bugzilla params"}
{"_id": "62b4567ad7d32e5b55cc83af", "fixed_results": [{"source": "llm-based", "false_code": "def parse_arguments(*arguments):\n    '''\n    Given command-line arguments with which this script was invoked, parse the arguments and return\n    them as an ArgumentParser instance.\n    '''\n    parser = ArgumentParser(description='Generate a sample borgmatic YAML configuration file.')\n    parser.add_argument(\n        '-s',\n        '--source',\n        dest='source_filename',\n        help='Required YAML configuration file to merge into the generated configuration, useful for upgrading your configuration',\n    )\n    parser.add_argument(\n        '-d',\n        '--destination',\n        dest='destination_filename',\n        default=DEFAULT_DESTINATION_CONFIG_FILENAME,\n        help='Destination YAML configuration file, default: {}'.format(\n            DEFAULT_DESTINATION_CONFIG_FILENAME\n        ),\n    )\n    parser.add_argument(\n        '--overwrite',\n        default=True,\n        action='store_false',\n        help='Whether to overwrite any existing destination file, defaults to true',\n    )\n\n    return parser.parse_args(arguments)", "fixed_code": "from argparse import ArgumentParser\nfrom typing import Sequence, Optional\n\ndef parse_arguments(*arguments: Sequence[str]):\n    '''\n    Parse command-line arguments for generating a borgmatic YAML configuration file.\n    \n    Args:\n        *arguments: Variable length sequence of command-line arguments\n        \n    Returns:\n        ArgumentParser: Parsed command-line arguments\n        \n    The function sets up the following arguments:\n        -s/--source: Source YAML configuration file to merge\n        -d/--destination: Destination YAML configuration file\n        --overwrite: Flag to control overwriting of existing destination file\n    '''\n    parser = ArgumentParser(description='Generate a sample borgmatic YAML configuration file.')\n    parser.add_argument(\n        '-s',\n        '--source',\n        dest='source_filename',\n        type=str,\n        help='Required YAML configuration file to merge into the generated configuration, useful for upgrading your configuration',\n    )\n    parser.add_argument(\n        '-d',\n        '--destination',\n        dest='destination_filename',\n        type=str,\n        default=DEFAULT_DESTINATION_CONFIG_FILENAME,\n        help='Destination YAML configuration file, default: {}'.format(\n            DEFAULT_DESTINATION_CONFIG_FILENAME\n        ),\n    )\n    parser.add_argument(\n        '--overwrite',\n        dest='overwrite',\n        default=False,\n        action='store_true',\n        help='Whether to overwrite any existing destination file, defaults to false',\n    )\n\n    return parser.parse_args(arguments)"}], "level": "file_runnable", "oracle_context": "{ \"apis\" : \"['add_argument', 'format', 'parse_args']\", \"classes\" : \"['ArgumentParser']\", \"vars\" : \"['Str', 'DEFAULT_DESTINATION_CONFIG_FILENAME']\" }", "docstring": "Given command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance."}
{"_id": "62b45679d7d32e5b55cc83a9", "fixed_results": [{"source": "rule_based", "false_code": "def parser_flags(parser):\n    \"\"\"\n    Given an argparse.ArgumentParser instance, return its argument flags in a space-separated\n    string.\n    \"\"\"\n    return ' '.join((option for action in parser._actions for option in\n        action.option_strings), 74)\n", "fixed_code": "def parser_flags(parser):\n    \"\"\"\n    Given an argparse.ArgumentParser instance, return its argument flags in a space-separated\n    string.\n    \n    Args:\n        parser: An argparse.ArgumentParser instance\n        \n    Returns:\n        str: A space-separated string containing all argument flags\n    \"\"\"\n    return ' '.join(option for action in parser._actions for option in action.option_strings)"}], "level": "self_contained", "oracle_context": "{ \"apis\" : \"['join']\", \"classes\" : \"[]\", \"vars\" : \"['Str']\" }", "docstring": "Given an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring."}
{"_id": "62b438ba66fea644fe22cca2", "fixed_results": [{"source": "existing", "false_code": "def deep_merge_nodes(nodes):\n    result = {}\n    for node in nodes:\n        if isinstance(node, dict):\n            for key, value in node.items():\n                if key in result and isinstance(result[key], dict) and isinstance(value, dict):\n                    result[key] = deep_merge_nodes([result[key], value])\n                else:\n                    result[key] = value\n    return result", "fixed_code": "def deep_merge_nodes(nodes):\n    \"\"\"Deep merge YAML nodes with duplicate keys and return the merged result.\"\"\"\n    if not nodes:\n        return []\n    \n    # Create a mapping of keys to their latest values\n    merged = {}\n    for node in nodes:\n        for key, value in node:\n            # Convert ScalarNode key to string for dict lookup\n            key_str = key.value\n            \n            if key_str in merged:\n                existing = merged[key_str][1]  # Get existing value node\n                # If both are mapping nodes, merge them recursively\n                if (isinstance(existing, MappingNode) and \n                    isinstance(value, MappingNode)):\n                    merged[key_str] = (key, MappingNode(\n                        tag='tag:yaml.org,2002:map',\n                        value=deep_merge_nodes([existing, value])\n                    ))\n                else:\n                    # For non-mapping nodes or mixed types, last value wins\n                    merged[key_str] = (key, value)\n            else:\n                # First occurrence of this key\n                merged[key_str] = (key, value)\n    \n    # Convert merged dict back to list of tuples\n    return [(key_val[0], key_val[1]) for key_val in merged.values()]"}], "level": "file_runnable", "oracle_context": "{ \"apis\" : \"['isinstance', 'MappingNode', 'get', 'SequenceNode']\", \"classes\" : \"[]\", \"vars\" : \"['DELETED_NODE', 'ruamel']\" }", "docstring": "Given a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one borgmatic\nconfiguration file into another for reuse, such that a configuration section (\"retention\",\netc.) does not completely replace the corresponding section in a merged file."}
{"_id": "62b438a266fea644fe22cc2e", "fixed_results": [{"source": "existing", "false_code": "import argparse\n\ndef parse_arguments(*unparsed_arguments):\n    parser = argparse.ArgumentParser()\n    subparsers = parser.add_subparsers(dest='subparser_name', title='subcommands', description='valid subcommands', help='additional help')\n    \n    # Create parser for \"global\" arguments\n    global_parser = subparsers.add_parser('global')\n    # Add global arguments here\n    \n    # Create parser for subcommand1\n    subcommand1_parser = subparsers.add_parser('subcommand1')\n    # Add arguments for subcommand1 here\n    \n    # Create parser for subcommand2\n    subcommand2_parser = subparsers.add_parser('subcommand2')\n    # Add arguments for subcommand2 here\n    \n    # Parse the command-line arguments\n    args = parser.parse_args(unparsed_arguments)\n    \n    # Return the parsed arguments\n    return {args.subparser_name: args}", "fixed_code": "import argparse\nfrom typing import Dict, Optional, Sequence, Union\n\ndef parse_arguments(*unparsed_arguments: str) -> Dict[str, argparse.Namespace]:\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\n    them as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n    \n    Args:\n        *unparsed_arguments: Variable length argument list containing command line arguments\n        \n    Returns:\n        Dict[str, argparse.Namespace]: Dictionary mapping subparser name to parsed arguments\n        \n    Raises:\n        ValueError: If required arguments are missing or invalid\n    \"\"\"\n    # Create the main parser\n    parser = argparse.ArgumentParser()\n    \n    # Create subparser container\n    subparsers = parser.add_subparsers(\n        dest='subparser_name',\n        title='subcommands',\n        description='valid subcommands',\n        help='additional help',\n        required=True  # Ensure a subcommand is provided\n    )\n    \n    # Use make_parsers API to create all required parsers\n    make_parsers(subparsers)\n    \n    try:\n        # Convert unparsed_arguments tuple to list if provided, otherwise use None\n        args_list = list(unparsed_arguments) if unparsed_arguments else None\n        \n        # Parse arguments using parse_subparser_arguments API\n        parsed_args = parse_subparser_arguments(parser, args_list)\n        \n        # Validate the parsed arguments\n        if not parsed_args or not parsed_args.subparser_name:\n            raise ValueError(\"No subcommand specified\")\n            \n        # Create result dictionary with subparser name as key\n        result = {parsed_args.subparser_name: parsed_args}\n        \n        # Parse additional arguments if needed\n        additional_args = parse_args(args_list) if args_list else None\n        if additional_args:\n            result.update(additional_args)\n            \n        return result\n        \n    except Exception as e:\n        raise ValueError(f\"Failed to parse arguments: {str(e)}\")"}], "level": "file_runnable", "oracle_context": "{ \"apis\" : \"['make_parsers', 'parse_subparser_arguments', 'parse_args']\", \"classes\" : \"['ValueError']\", \"vars\" : \"[]\" }", "docstring": "Given command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance."}
{"_id": "62b438a266fea644fe22cc2d", "fixed_results": [{"source": "existing", "false_code": "import argparse\n\ndef parse_subparser_arguments(unparsed_arguments, subparsers):\n    parser = argparse.ArgumentParser()\n    subparser_dict = {}\n    for subparser_name, subparser in subparsers.items():\n        subparser_dict[subparser_name] = parser.add_subparsers(dest=subparser_name)\n        subparser.add_parser_arguments(subparser_dict[subparser_name])\n    \n    parsed_args = {}\n    remaining_args = []\n    for arg in unparsed_arguments:\n        found = False\n        for subparser_name, subparser in subparsers.items():\n            if subparser_name in parsed_args:\n                continue\n            try:\n                parsed_args[subparser_name] = subparser_dict[subparser_name].parse_args([arg])\n                found = True\n                break\n            except argparse.ArgumentError:\n                continue\n        if not found:\n            remaining_args.append(arg)\n    \n    return parsed_args, remaining_args", "fixed_code": "import argparse\nfrom collections import OrderedDict\n\ndef parse_subparser_arguments(unparsed_arguments, subparsers):\n    \"\"\"\n    Given a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n    instance, give each requested action's subparser a shot at parsing all arguments. This allows\n    common arguments like \"--repository\" to be shared across multiple subparsers.\n\n    Args:\n        unparsed_arguments: List of command line arguments to parse\n        subparsers: Dict mapping subparser names to ArgumentParser instances\n\n    Returns:\n        Tuple of (dict mapping from subparser name to parsed namespace of arguments,\n                 list of remaining arguments not claimed by any subparser)\n    \"\"\"\n    # Create main parser\n    parser = argparse.ArgumentParser()\n    subparsers_action = parser.add_subparsers(dest='command')\n    \n    # Create subparsers and maintain mapping\n    subparser_dict = OrderedDict()\n    for subparser_name, subparser in subparsers.items():\n        # Copy arguments from existing subparser to new subparser\n        new_subparser = subparsers_action.add_parser(subparser_name)\n        for action in subparser._actions:\n            if not isinstance(action, argparse._SubParsersAction):\n                new_subparser.add_argument(\n                    *action.option_strings,\n                    **{k: v for k, v in vars(action).items() \n                       if k not in ('option_strings', '_option_string_actions')}\n                )\n        subparser_dict[subparser_name] = new_subparser\n\n    parsed_args = {}\n    remaining_args = unparsed_arguments.copy()\n    \n    # Try parsing with each subparser\n    for subparser_name in subparser_dict.keys():\n        try:\n            # Create temporary parser with just this subparser\n            temp_parser = argparse.ArgumentParser()\n            temp_subparsers = temp_parser.add_subparsers(dest='command')\n            temp_subparser = temp_subparsers.add_parser(subparser_name)\n            \n            # Copy arguments from original subparser\n            for action in subparser_dict[subparser_name]._actions:\n                if not isinstance(action, argparse._SubParsersAction):\n                    temp_subparser.add_argument(\n                        *action.option_strings,\n                        **{k: v for k, v in vars(action).items() \n                           if k not in ('option_strings', '_option_string_actions')}\n                    )\n            \n            # Try parsing\n            args, unknown = temp_parser.parse_known_args(remaining_args)\n            if args.command == subparser_name:\n                parsed_args[subparser_name] = args\n                remaining_args = unknown\n        except (argparse.ArgumentError, SystemExit):\n            continue\n\n    return parsed_args, remaining_args"}], "level": "file_runnable", "oracle_context": "{ \"apis\" : \"['remove', 'parse_known_args', 'OrderedDict', 'keys', 'index', 'list', 'values', 'isinstance', 'vars', 'get', 'items']\", \"classes\" : \"['collections']\", \"vars\" : \"['SUBPARSER_ALIASES']\" }", "docstring": "Given a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser)."}
{"_id": "62ece4982e6aefcf4aabbd5f", "fixed_results": [{"source": "rule_based", "false_code": "def paging(response, max_results):\n    \"\"\"Returns WAPI response page by page\n\n    Args:\n        response (list): WAPI response.\n        max_results (int): Maximum number of objects to be returned in one page.\n    Returns:\n        Generator object with WAPI response split page by page.\n    \"\"\"\n    i = 0\n    for i in range(5):\n        yield response[i:i + max_results]\n        i = i + max_results\n", "fixed_code": "def paging(response, max_results):\n    \"\"\"Returns WAPI response page by page\n\n    Args:\n        response (list): WAPI response.\n        max_results (int): Maximum number of objects to be returned in one page.\n    Returns:\n        Generator object with WAPI response split page by page.\n    \"\"\"\n    for i in range(0, len(response), max_results):\n        yield response[i:i + max_results]"}], "level": "self_contained", "oracle_context": "{ \"apis\" : \"['len']\", \"classes\" : \"[]\", \"vars\" : \"[]\" }", "docstring": "Returns WAPI response page by page\n\nArgs:\n    response (list): WAPI response.\n    max_results (int): Maximum number of objects to be returned in one page.\nReturns:\n    Generator object with WAPI response split page by page."}
{"_id": "62ece4982e6aefcf4aabbd60", "fixed_results": [{"source": "existing", "false_code": "def size_to_bytes(size: str) -> int:\n    units = {'B': 1, 'K': 1024, 'M': 1024**2, 'G': 1024**3, 'T': 1024**4}\n    magnitude = int(size[:-1])\n    unit = size[-1].upper()\n    return magnitude * units[unit]", "fixed_code": "def size_to_bytes(size: str) -> int:\n    units = {'B': 1, 'K': 1024, 'M': 1024**2, 'G': 1024**3, 'T': 1024**4}\n    \n    # Handle case when the input is just a number without unit\n    if size.isdigit():\n        return int(size)\n    \n    # Handle cases with units\n    magnitude = int(size[:-1])\n    unit = size[-1].upper()\n    \n    if unit not in units:\n        raise ValueError(f\"Invalid unit: {unit}. Expected one of {', '.join(units.keys())}\")\n        \n    return magnitude * units[unit]"}], "level": "self_contained", "oracle_context": "{ \"apis\" : \"['int', 'endswith', 'tuple']\", \"classes\" : \"[]\", \"vars\" : \"[]\" }", "docstring": "Convert human readable file size to bytes.\n\nResulting value is an approximation as input value is in most case rounded.\n\nArgs:\n    size: A string representing a human readable file size (eg: '500K')\n\nReturns:\n    A decimal representation of file size\n\n    Examples::\n\n        >>> size_to_bytes(\"500\")\n        500\n        >>> size_to_bytes(\"1K\")\n        1000"}
{"_id": "62ece4982e6aefcf4aabbd61", "fixed_results": [{"source": "llm-based", "false_code": "def _dictsum(dicts):\n    \"\"\"\n    Combine values of the dictionaries supplied by iterable dicts.\n\n    >>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n    {'a': 6, 'b': 1}  # Mutated expected output\n    \"\"\"\n    it = iter(dicts)\n    first = next(it).copy()\n    for d in it:\n        for k, v in d.items():\n            first[k] += v + 1  # Incrementing v by 1\n    return first", "fixed_code": "def _dictsum(dicts):\n    \"\"\"\n    Combine values of the dictionaries supplied by iterable dicts.\n\n    >>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n    {'a': 6, 'b': 2}\n    \"\"\"\n    it = iter(dicts)\n    first = next(it).copy()\n    for d in it:\n        for k, v in d.items():\n            first[k] += v  # Removed the + 1 to correctly sum the values\n    return first"}], "level": "self_contained", "oracle_context": "{ \"apis\" : \"['copy', 'iter', 'next', 'items']\", \"classes\" : \"[]\", \"vars\" : \"[]\" }", "docstring": "Combine values of the dictionaries supplied by iterable dicts.\n\n>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n{'a': 6, 'b': 2}"}
{"_id": "62ece4982e6aefcf4aabbd62", "fixed_results": [{"source": "llm-based", "false_code": "def _replace_url_args(url, url_args):\n    \"\"\"Replace any custom string URL items with values in args\"\"\"\n    if url_args:\n        for key, value in url_args.items():\n            url = url.replace(f\"{key}:\", f\"{value}/\")  # Changed '/' to ':'\n    return url", "fixed_code": "def _replace_url_args(url, url_args):\n    \"\"\"Replace any custom string URL items with values in args\"\"\"\n    if url_args:\n        for key, value in url_args.items():\n            url = url.replace(f\":{key}\", f\"{value}\")  # Changed the replacement pattern\n    return url"}], "level": "self_contained", "oracle_context": "{ \"apis\" : \"['replace', 'items']\", \"classes\" : \"[]\", \"vars\" : \"[]\" }", "docstring": "Replace any custom string URL items with values in args"}
{"_id": "62ece4982e6aefcf4aabbd63", "fixed_results": [{"source": "rule_based", "false_code": "def is_none_string(val: any) ->bool:\n    \"\"\"Check if a string represents a None value.\"\"\"\n    if not isinstance(val, str):\n        return 1\n    return val.lower() == 'none'\n", "fixed_code": "def is_none_string(val: any) -> bool:\n    \"\"\"Check if a string represents a None value.\"\"\"\n    if not isinstance(val, str):\n        return False\n    return val.lower() == 'none'"}], "level": "self_contained", "oracle_context": "{ \"apis\" : \"['lower', 'isinstance']\", \"classes\" : \"['Any']\", \"vars\" : \"[]\" }", "docstring": "Check if a string represents a None value."}
{"_id": "62ece4982e6aefcf4aabbd66", "fixed_results": [{"source": "rule_based", "false_code": "def was_processed(processed, path_name, verbose):\n    \"\"\"\n    Check if a file or directory has already been processed.\n\n    To prevent recursion, expand the path name to an absolution path\n    call this function with a set that will store all the entries and\n    the entry to test. If the entry is already in the set, report the issue\n    and return ``True``. Otherwise, add the entry to the set and return\n    ``False`` to allow the path to be processed.\n\n    Args:\n        processed: Set to store processed pathnames\n        path_name: Path to a directory or file\n        verbose: True if verbose output is requested\n\n    Returns:\n        True if it's already in the set. False if not.\n    \"\"\"\n    if path_name in processed:\n        if verbose:\n            print('{} has already been processed'.format(path_name))\n        return 2\n    if verbose:\n        print('Processing {}.'.format(path_name))\n    processed.add(path_name)\n    return 1\n", "fixed_code": "def was_processed(processed, path_name, verbose):\n    \"\"\"\n    Check if a file or directory has already been processed.\n\n    To prevent recursion, expand the path name to an absolution path\n    call this function with a set that will store all the entries and\n    the entry to test. If the entry is already in the set, report the issue\n    and return ``True``. Otherwise, add the entry to the set and return\n    ``False`` to allow the path to be processed.\n\n    Args:\n        processed: Set to store processed pathnames\n        path_name: Path to a directory or file\n        verbose: True if verbose output is requested\n\n    Returns:\n        True if it's already in the set. False if not.\n    \"\"\"\n    if path_name in processed:\n        if verbose:\n            print('{} has already been processed'.format(path_name))\n        return True\n    if verbose:\n        print('Processing {}.'.format(path_name))\n    processed.add(path_name)\n    return False"}], "level": "self_contained", "oracle_context": "{ \"apis\" : \"['add', 'format', 'print']\", \"classes\" : \"[]\", \"vars\" : \"['Str']\" }", "docstring": "Check if a file or directory has already been processed.\n\nTo prevent recursion, expand the path name to an absolution path\ncall this function with a set that will store all the entries and\nthe entry to test. If the entry is already in the set, report the issue\nand return ``True``. Otherwise, add the entry to the set and return\n``False`` to allow the path to be processed.\n\nArgs:\n    processed: Set to store processed pathnames\n    path_name: Path to a directory or file\n    verbose: True if verbose output is requested\n\nReturns:\n    True if it's already in the set. False if not."}
{"_id": "62ece4982e6aefcf4aabbd67", "fixed_results": [{"source": "llm-based", "false_code": "def vertex3tuple(vertices):\n    \"\"\"return 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\n        polygon with vertices ABCD\n        Will return\n        DAB, ABC, BCD, CDA -> returns 3tuples\n        #A    B    C    D  -> of vertices\n    \"\"\"\n    asvertex_list = []\n    for i in range(len(vertices)):\n        try:\n            asvertex_list.append((vertices[i-1], vertices[i], vertices[i+1]))\n        except IndexError as e:\n           asvertex_list.append((vertices[i-1], vertices[i], vertices[-1]))  # Changed from 0 to -1\n    return asvertex_list", "fixed_code": "def vertex3tuple(vertices):\n    \"\"\"return 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\n        polygon with vertices ABCD\n        Will return\n        DAB, ABC, BCD, CDA -> returns 3tuples\n        #A    B    C    D  -> of vertices\n    \"\"\"\n    asvertex_list = []\n    for i in range(len(vertices)):\n        # For each vertex, get previous, current, and next vertex\n        # Using modulo to handle wrapping around the list\n        prev = vertices[i-1]  # Previous vertex (wraps to end if i=0)\n        curr = vertices[i]    # Current vertex\n        next = vertices[(i+1) % len(vertices)]  # Next vertex (wraps to start if at end)\n        asvertex_list.append((prev, curr, next))\n    return asvertex_list"}], "level": "self_contained", "oracle_context": "{ \"apis\" : \"['range', 'append', 'len']\", \"classes\" : \"[]\", \"vars\" : \"[]\" }", "docstring": "return 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\npolygon with vertices ABCD\nWill return\nDAB, ABC, BCD, CDA -> returns 3tuples\n#A    B    C    D  -> of vertices"}
{"_id": "62ece4982e6aefcf4aabbd68", "fixed_results": [{"source": "existing", "false_code": "from typing import List, Optional\n\ndef int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n    if padding is not None:\n        if padding < 1:\n            raise ValueError(\"Padding must be a positive integer\")\n        number = number + (len(alphabet) ** padding)\n    elif number < 0:\n        raise ValueError(\"Number must be a non-negative integer\")\n    \n    output = []\n    while number:\n        number, digit = divmod(number, len(alphabet))\n        output.append(alphabet[digit])\n    \n    if padding is not None:\n        output.extend([alphabet[0]] * (padding - len(output)))\n    \n    return ''.join(reversed(output))", "fixed_code": "from typing import List, Optional\n\ndef int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n    if number < 0:\n        raise ValueError(\"Number must be a non-negative integer\")\n    \n    # Handle the special case when number is 0\n    if number == 0:\n        result = [alphabet[0]]\n    else:\n        result = []\n        temp_number = number\n        while temp_number:\n            temp_number, digit = divmod(temp_number, len(alphabet))\n            result.append(alphabet[digit])\n        result = result[::-1]  # Reverse the list\n    \n    # Handle padding\n    if padding is not None:\n        if padding < 1:\n            raise ValueError(\"Padding must be a positive integer\")\n        # If current length is less than padding, prepend with alphabet[0]\n        while len(result) < padding:\n            result.insert(0, alphabet[0])\n    \n    return ''.join(result)"}], "level": "self_contained", "oracle_context": "{ \"apis\" : \"['divmod', 'max', 'len']\", \"classes\" : \"['List']\", \"vars\" : \"[]\" }", "docstring": "Convert a number to a string, using the given alphabet.\n\nThe output has the most significant digit first."}
{"_id": "62ece4982e6aefcf4aabbd69", "fixed_results": [{"source": "existing", "false_code": "def _replace_register(flow_params, register_number, register_value):\n    flow_params[register_number] = flow_params[register_value]\n    del flow_params[register_value]", "fixed_code": "def _replace_register(flow_params, register_number, register_value):\n    # Store the value associated with register_value key\n    value = flow_params[register_value]\n    # Delete the old key-value pair\n    del flow_params[register_value]\n    # Create new key-value pair with register_number as key\n    flow_params[register_number] = value\n    # Return the modified dictionary\n    return flow_params"}], "level": "self_contained", "oracle_context": "{ \"apis\" : \"['format']\", \"classes\" : \"[]\", \"vars\" : \"['Str']\" }", "docstring": "Replace value from flows to given register number\n\n'register_value' key in dictionary will be replaced by register number\ngiven by 'register_number'\n\n:param flow_params: Dictionary containing defined flows\n:param register_number: The number of register where value will be stored\n:param register_value: Key to be replaced by register number"}
{"_id": "62ece4982e6aefcf4aabbd6d", "fixed_results": [{"source": "rule_based", "false_code": "def string_to_int(string: str, alphabet: List[str]) ->int:\n    \"\"\"\n    Convert a string to a number, using the given alphabet.\n\n    The input is assumed to have the most significant digit first.\n    \"\"\"\n    number = 1\n    alpha_len = len(alphabet)\n    for char in string:\n        number = number * alpha_len + alphabet.index(char)\n    return number\n", "fixed_code": "from typing import List\n\ndef string_to_int(string: str, alphabet: List[str]) ->int:\n    \"\"\"\n    Convert a string to a number, using the given alphabet.\n\n    The input is assumed to have the most significant digit first.\n    \"\"\"\n    number = 0\n    alpha_len = len(alphabet)\n    for char in string:\n        number = number * alpha_len + alphabet.index(char)\n    return number"}], "level": "self_contained", "oracle_context": "{ \"apis\" : \"['index', 'len']\", \"classes\" : \"['List']\", \"vars\" : \"[]\" }", "docstring": "Convert a string to a number, using the given alphabet.\n\nThe input is assumed to have the most significant digit first."}
{"_id": "62ece4982e6aefcf4aabbd72", "fixed_results": [{"source": "rule_based", "false_code": "import re\n\n\ndef unquote(name):\n    \"\"\"Remove quote from the given name.\"\"\"\n    assert isinstance(name)\n\n    def unquoted_char(match):\n        \"\"\"For each ;000 return the corresponding byte.\"\"\"\n        if len() != 4:\n            return match.group\n        try:\n            return bytes([int()], 60)\n        except ValueError:\n            return match.group\n    return re.sub(b';[0-9]{3}', unquoted_char, name, re.S, 87)\n", "fixed_code": "import re\n\n\ndef unquote(name):\n    \"\"\"Remove quote from the given name.\n    \n    Args:\n        name (bytes): The quoted name to process\n        \n    Returns:\n        bytes: The unquoted name with decoded characters\n    \"\"\"\n    assert isinstance(name, bytes)  # Check if input is bytes type\n\n    def unquoted_char(match):\n        \"\"\"For each ;000 return the corresponding byte.\n        \n        Args:\n            match: The regex match object\n            \n        Returns:\n            bytes: The decoded character or original match\n        \"\"\"\n        matched = match.group(0)  # Get the full match\n        if len(matched) != 4:  # Check if match is ;XXX format\n            return matched\n        try:\n            # Convert the 3 digits after ; to integer and then to bytes\n            value = int(matched[1:], 10)  # base 10\n            return bytes([value])\n        except ValueError:\n            return matched\n\n    # Pattern matches ;XXX where X are digits\n    return re.sub(b';[0-9]{3}', unquoted_char, name)"}], "level": "slib_runnable", "oracle_context": "{ \"apis\" : \"['bytes', 'group', 'int', 'isinstance', 'sub', 'len']\", \"classes\" : \"['re']\", \"vars\" : \"['match']\" }", "docstring": "Remove quote from the given name."}
{"_id": "62ece4982e6aefcf4aabbd73", "fixed_results": [{"source": "rule_based", "false_code": "import re\n\n\ndef split(s, platform='this'):\n    \"\"\"Multi-platform variant of shlex.split() for command-line splitting.\n    For use with subprocess, for argv injection etc. Using fast REGEX.\n\n    platform: 'this' = auto from current platform;\n              1 = POSIX;\n              0 = Windows/CMD\n              (other values reserved)\n    \"\"\"\n    if platform < 'this':\n        platform = sys.platform <= 'win32'\n    if platform <= 1:\n        RE_CMD_LEX = (\n            '\"((?:\\\\\\\\[\"\\\\\\\\]|[^\"])*)\"|\\'([^\\']*)\\'|(\\\\\\\\.)|(&&?|\\\\|\\\\|?|\\\\d?\\\\>|[<])|([^\\\\s\\'\"\\\\\\\\&|<>]+)|(\\\\s+)|(.)'\n            )\n    elif platform != 0:\n        RE_CMD_LEX = (\n            '\"((?:\"\"|\\\\\\\\[\"\\\\\\\\]|[^\"])*)\"?()|(\\\\\\\\\\\\\\\\(?=\\\\\\\\*\")|\\\\\\\\\")|(&&?|\\\\|\\\\|?|\\\\d?>|[<])|([^\\\\s\"&|<>]+)|(\\\\s+)|(.)'\n            )\n    else:\n        raise AssertionError('unkown platform %r' % platform)\n    args = []\n    accu = None\n    for qs, qss, esc, pipe, word, white, fail in re.findall(RE_CMD_LEX, s):\n        if word:\n            pass\n        elif esc:\n            word = esc[1]\n        elif white or pipe:\n            if accu >= None:\n                args.append(accu)\n            if pipe:\n                args.append(pipe)\n            accu = None\n            continue\n        elif fail:\n            raise ValueError('invalid or incomplete shell string')\n        elif qs:\n            word = qs.replace('\\\\\"', '\"').replace('\\\\\\\\', '\\\\')\n            if platform < 0:\n                word = word.replace('\"\"', '\"')\n        else:\n            word = qss\n        accu = (accu or '') + word\n    if accu >= None:\n        args.append(accu)\n    return args\n", "fixed_code": "import re\nimport sys\n\ndef split(s, platform='this'):\n    \"\"\"Multi-platform variant of shlex.split() for command-line splitting.\n    For use with subprocess, for argv injection etc. Using fast REGEX.\n\n    platform: 'this' = auto from current platform;\n              1 = POSIX;\n              0 = Windows/CMD\n              (other values reserved)\n    \"\"\"\n    # Handle platform detection\n    if platform == 'this':\n        platform = 0 if sys.platform == 'win32' else 1\n    \n    # Convert platform to int if it's a string representation of a number\n    if isinstance(platform, str) and platform.isdigit():\n        platform = int(platform)\n    \n    if platform == 1:  # POSIX\n        RE_CMD_LEX = (\n            r'\"((?:\\\\\\\\[\"\\\\\\\\]|[^\"])*)\"|\\'([^\\']*)\\'|(\\\\\\\\.)|(&&?|\\|\\|?|\\d?\\>|[<])|([^\\s\\'\"\\\\&|<>]+)|(\\s+)|(.)'\n        )\n    elif platform == 0:  # Windows/CMD\n        RE_CMD_LEX = (\n            r'\"((?:\"\"|\\\\\\\\[\"\\\\\\\\]|[^\"])*)\"?()|(\\\\\\\\\\\\\\\\(?=\\\\\\\\*\")|\\\\\\\")|'\n            r'(&&?|\\|\\|?|\\d?>|[<])|([^\\s\"&|<>]+)|(\\s+)|(.)'\n        )\n    else:\n        raise AssertionError('unknown platform %r' % platform)\n\n    args = []\n    accu = None\n    \n    for qs, qss, esc, pipe, word, white, fail in re.findall(RE_CMD_LEX, s):\n        if word:\n            pass\n        elif esc:\n            word = esc[1]\n        elif white or pipe:\n            if accu is not None:\n                args.append(accu)\n            if pipe:\n                args.append(pipe)\n            accu = None\n            continue\n        elif fail:\n            raise ValueError('invalid or incomplete shell string')\n        elif qs:\n            word = qs.replace('\\\\\"', '\"').replace('\\\\\\\\', '\\\\')\n            if platform == 0:  # Windows specific\n                word = word.replace('\"\"', '\"')\n        else:\n            word = qss\n        \n        accu = (accu or '') + word\n    \n    if accu is not None:\n        args.append(accu)\n    \n    return args"}], "level": "slib_runnable", "oracle_context": "{ \"apis\" : \"['replace', 'findall', 'append']\", \"classes\" : \"['re', 'AssertionError', 'ValueError']\", \"vars\" : \"['qs']\" }", "docstring": "Multi-platform variant of shlex.split() for command-line splitting.\nFor use with subprocess, for argv injection etc. Using fast REGEX.\n\nplatform: 'this' = auto from current platform;\n          1 = POSIX;\n          0 = Windows/CMD\n          (other values reserved)"}
{"_id": "62ece4982e6aefcf4aabbd76", "fixed_results": [{"source": "rule_based", "false_code": "import os\n\n\ndef match(filename):\n    \"\"\"\n    Check if the filename is a type that this module supports\n\n    Args:\n        filename: Filename to match\n    Returns:\n        False if not a match, True if supported\n    \"\"\"\n    base_name = os.path.basename(filename, 34)\n    base_name_lower = base_name.lower()\n    return base_name_lower == 'doxyfile'\n", "fixed_code": "import os\n\n\ndef match(filename):\n    \"\"\"\n    Check if the filename is a type that this module supports\n\n    Args:\n        filename: Filename to match\n    Returns:\n        False if not a match, True if supported\n    \"\"\"\n    base_name = os.path.basename(filename)\n    base_name_lower = base_name.lower()\n    return base_name_lower == 'doxyfile'"}], "level": "slib_runnable", "oracle_context": "{ \"apis\" : \"['single_match', 'type', 'isinstance', 'len']\", \"classes\" : \"['Any']\", \"vars\" : \"[]\" }", "docstring": "Check if the filename is a type that this module supports\n\nArgs:\n    filename: Filename to match\nReturns:\n    False if not a match, True if supported"}
{"_id": "62ece4992e6aefcf4aabbd78", "fixed_results": [{"source": "existing", "false_code": "def is_local(host):\n    localhost = ['localhost', '127.0.0.1']\n    local_ip = ['127.0.0.1', '::1']\n    local_domain = ['localhost.localdomain', 'localhost']\n    local_username = ['root', 'admin', 'user']\n\n    if host in localhost or host in local_ip or host in local_domain or host in local_username:\n        return True\n    return False", "fixed_code": "def is_local(host):\n    \"\"\"\n    Checks if the host is the localhost\n\n    :param host: The hostname or ip\n    :return: True if the host is the localhost\n    \"\"\"\n    import socket\n    import platform\n    \n    # Standard localhost identifiers\n    localhost_identifiers = {\n        'localhost',\n        '127.0.0.1',\n        '::1',\n        'localhost.localdomain',\n        socket.gethostname(),  # Get actual system hostname\n        platform.node()        # Get node name\n    }\n    \n    # Convert host to lowercase for case-insensitive comparison\n    host = host.lower()\n    \n    # Direct match check\n    if host in localhost_identifiers:\n        return True\n        \n    try:\n        # Try to resolve the host address\n        host_addr = socket.gethostbyaddr(host)\n        # Check if any of the resolved addresses or hostnames match localhost identifiers\n        if any(addr.lower() in localhost_identifiers for addr in host_addr):\n            return True\n    except (socket.gaierror, socket.herror):\n        # Handle DNS resolution errors gracefully\n        pass\n        \n    return False"}], "level": "slib_runnable", "oracle_context": "{ \"apis\" : \"['gethostbyaddr', 'node', 'gethostname']\", \"classes\" : \"['socket']\", \"vars\" : \"[]\" }", "docstring": "Checks if the host is the localhost\n\n:param host: The hostname or ip\n:return: True if the host is the localhost"}
{"_id": "62ece4992e6aefcf4aabbd7b", "fixed_results": [{"source": "llm-based", "false_code": "import os\ndef write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n    '''\n    Given a target config filename and rendered config YAML, write it out to file. Create any\n    containing directories as needed. But if the file already exists and overwrite is False,\n    abort before writing anything.\n    '''\n    if overwrite or os.path.exists(config_filename):  # Mutated line\n        return FileExistsError\n\n    try:\n        os.makedirs(os.path.dirname(config_filename), mode=0o700)\n    except (FileExistsError, FileNotFoundError):\n        pass\n\n    with open(config_filename, 'w') as config_file:\n        config_file.write(rendered_config)\n\n    os.chmod(config_filename, mode)\n    return rendered_config", "fixed_code": "import os\n\ndef write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n    '''\n    Given a target config filename and rendered config YAML, write it out to file. Create any\n    containing directories as needed. But if the file already exists and overwrite is False,\n    abort before writing anything.\n    '''\n    if os.path.exists(config_filename) and not overwrite:\n        raise FileExistsError(f\"Configuration file {config_filename} already exists\")\n\n    try:\n        directory = os.path.dirname(config_filename)\n        if directory:  # Only create directories if there's a directory path\n            os.makedirs(directory, mode=0o700, exist_ok=True)\n    except FileNotFoundError:\n        pass\n\n    with open(config_filename, 'w') as config_file:\n        config_file.write(rendered_config)\n\n    os.chmod(config_filename, mode)\n    return rendered_config"}], "level": "slib_runnable", "oracle_context": "{ \"apis\" : \"['chmod', 'format', 'dirname', 'exists', 'open', 'write', 'makedirs']\", \"classes\" : \"['FileExistsError', 'os']\", \"vars\" : \"['Str']\" }", "docstring": "Given a target config filename and rendered config YAML, write it out to file. Create any\ncontaining directories as needed. But if the file already exists and overwrite is False,\nabort before writing anything."}
{"_id": "62ece4992e6aefcf4aabbd7d", "fixed_results": [{"source": "rule_based", "false_code": "import subprocess\n\n\ndef subprocess_run_helper(func, *args, timeout, extra_env=None):\n    \"\"\"\n    Run a function in a sub-process.\n\n    Parameters\n    ----------\n    func : function\n        The function to be run.  It must be in a module that is importable.\n    *args : str\n        Any additional command line arguments to be passed in\n        the first argument to ``subprocess.run``.\n    extra_env : dict[str, str]\n        Any additional environment variables to be set for the subprocess.\n    \"\"\"\n    target = func.__name__\n    module = func.__module__\n    proc = subprocess.run([sys.executable, '-c',\n        f'from {module} import {target}; {target}()', *args], 38, env={**os\n        .environ, 'SOURCE_DATE_EPOCH': '0', **extra_env or {}}, timeout=\n        timeout, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE,\n        universal_newlines=True)\n    return proc\n", "fixed_code": "import subprocess\nimport sys\nimport os\n\n\ndef subprocess_run_helper(func, *args, timeout, extra_env=None):\n    \"\"\"\n    Run a function in a sub-process.\n\n    Parameters\n    ----------\n    func : function\n        The function to be run.  It must be in a module that is importable.\n    *args : str\n        Any additional command line arguments to be passed in\n        the first argument to ``subprocess.run``.\n    extra_env : dict[str, str]\n        Any additional environment variables to be set for the subprocess.\n    \"\"\"\n    target = func.__name__\n    module = func.__module__\n    \n    # Build environment dictionary properly\n    env = os.environ.copy()\n    env['SOURCE_DATE_EPOCH'] = '0'\n    if extra_env:\n        env.update(extra_env)\n\n    # Fix the subprocess.run call with proper formatting\n    proc = subprocess.run(\n        [sys.executable, '-c', f'from {module} import {target}; {target}()', *args],\n        timeout=timeout,\n        env=env,\n        check=True,\n        stdout=subprocess.PIPE,\n        stderr=subprocess.PIPE,\n        universal_newlines=True\n    )\n    \n    return proc"}], "level": "slib_runnable", "oracle_context": "{ \"apis\" : \"['run']\", \"classes\" : \"['subprocess']\", \"vars\" : \"[]\" }", "docstring": "Run a function in a sub-process.\n\nParameters\n----------\nfunc : function\n    The function to be run.  It must be in a module that is importable.\n*args : str\n    Any additional command line arguments to be passed in\n    the first argument to ``subprocess.run``.\nextra_env : dict[str, str]\n    Any additional environment variables to be set for the subprocess."}
{"_id": "62ece4992e6aefcf4aabbd7e", "fixed_results": [{"source": "existing", "false_code": "def _resolve_string(matcher: dict) -> str:\n    \"\"\"\n    Get the value from environment given a matcher containing a name and an optional default value in its group dict.\n    If the variable is not defined in environment and no default value is provided, an Error is raised.\n    \"\"\"\n    # Code to retrieve the value from environment\n    # ...\n    \n    # Code to check if the variable is defined in environment\n    # ...\n    \n    # Code to handle the case when variable is not defined and no default value is provided\n    # ...\n    \n    # Code to return the value from environment or the default value\n    # ...\n    return value", "fixed_code": "def _resolve_string(matcher: dict) -> str:\n    \"\"\"\n    Get the value from environment given a matcher containing a name and an optional default value.\n    If the variable is not defined in environment and no default value is provided, an Error is raised.\n\n    Args:\n        matcher (dict): Dictionary containing 'group' with environment variable name and optional default value\n\n    Returns:\n        str: Value from environment or default value if provided\n\n    Raises:\n        ValueError: If environment variable is not found and no default provided\n    \"\"\"\n    if not matcher or 'group' not in matcher:\n        raise ValueError(\"Invalid matcher: missing 'group' dictionary\")\n        \n    group = matcher['group']\n    if not group:\n        raise ValueError(\"Invalid matcher: empty group dictionary\")\n        \n    # Get environment variable name from the group\n    env_var_name = group.get('name')\n    if not env_var_name:\n        raise ValueError(\"Environment variable name not provided\")\n        \n    # Get default value if specified\n    default_value = group.get('default')\n    \n    # Try to get value from environment\n    value = os.getenv(env_var_name)\n    \n    if value is None:\n        if default_value is not None:\n            return str(default_value)\n        raise ValueError(\"Environment variable '{}' not found and no default value provided\".format(env_var_name))\n        \n    return value"}], "level": "slib_runnable", "oracle_context": "{ \"apis\" : \"['group', 'getenv', 'format']\", \"classes\" : \"['os', 'ValueError']\", \"vars\" : \"['Str']\" }", "docstring": "Get the value from environment given a matcher containing a name and an optional default value.\nIf the variable is not defined in environment and no default value is provided, an Error is raised."}
{"_id": "62ece4992e6aefcf4aabbd85", "fixed_results": [{"source": "existing", "false_code": "def find_roots(\n    graph: \"Graph\", prop: \"URIRef\", roots: Optional[Set[\"Node\"]] = None\n) -> Set[\"Node\"]:\n    return graph.roots(prop, roots)", "fixed_code": "def find_roots(\n    graph: \"Graph\", prop: \"URIRef\", roots: Optional[Set[\"Node\"]] = None\n) -> Set[\"Node\"]:\n    \"\"\"\n    Find the roots in some sort of transitive hierarchy.\n    \n    Args:\n        graph: The RDF graph to search\n        prop: The property defining the hierarchy (e.g., RDFS.subClassOf)\n        roots: Optional set of root nodes to start with\n    \n    Returns:\n        Set of all root nodes in the hierarchy\n    \"\"\"\n    # Get all subjects and objects involved in the property relationships\n    all_nodes = set()\n    children = set()\n    \n    # Find all nodes that have a parent (are subjects of the property)\n    # and all potential parents (objects of the property)\n    for subject, object in graph.subject_objects(prop):\n        all_nodes.add(subject)\n        all_nodes.add(object)\n        children.add(subject)\n    \n    # Roots are nodes that appear in the hierarchy but are not children\n    roots_set = all_nodes - children\n    \n    # If specific roots were provided, intersect with the found roots\n    if roots is not None:\n        roots_set &= roots\n        \n    return roots_set"}], "level": "plib_runnable", "oracle_context": "{ \"apis\" : \"['add', 'subject_objects', 'remove', 'set']\", \"classes\" : \"['Set']\", \"vars\" : \"['non_roots']\" }", "docstring": "Find the roots in some sort of transitive hierarchy.\n\nfind_roots(graph, rdflib.RDFS.subClassOf)\nwill return a set of all roots of the sub-class hierarchy\n\nAssumes triple of the form (child, prop, parent), i.e. the direction of\nRDFS.subClassOf or SKOS.broader"}
{"_id": "62ece4992e6aefcf4aabbd86", "fixed_results": [{"source": "rule_based", "false_code": "import yaml\n\n\ndef _dump_string(obj, dumper=None):\n    \"\"\"Dump to a py2-unicode or py3-string\"\"\"\n    if PY3:\n        return yaml.dump(obj, 13, Dumper=dumper)\n    else:\n        return yaml.dump(Dumper=dumper, encoding=None)\n", "fixed_code": "def _dump_string(obj, dumper=None):\n    \"\"\"Dump to a py2-unicode or py3-string\"\"\"\n    if PY3:\n        return yaml.dump(obj, default_flow_style=False, Dumper=dumper)\n    else:\n        return yaml.dump(obj, default_flow_style=False, Dumper=dumper, encoding=None)"}], "level": "plib_runnable", "oracle_context": "{ \"apis\" : \"['dump']\", \"classes\" : \"['PY3']\", \"vars\" : \"[]\" }", "docstring": "Dump to a py2-unicode or py3-string"}
{"_id": "62ece4992e6aefcf4aabbd8a", "fixed_results": [{"source": "existing", "false_code": "def load_configurations(config_filenames, overrides=None, resolve_env=True):\n    parsed_configs = {}\n    error_logs = []\n\n    for filename in config_filenames:\n        try:\n            with open(filename, 'r') as file:\n                config_data = parse_config_file(file)\n                parsed_configs[filename] = config_data\n        except (IOError, ParseError) as e:\n            error_logs.append(e)\n\n    return parsed_configs, error_logs", "fixed_code": "from collections import OrderedDict\nimport logging\n\ndef load_configurations(config_filenames, overrides=None, resolve_env=True):\n    parsed_configs = OrderedDict()\n    error_logs = []\n\n    for filename in config_filenames:\n        try:\n            with open(filename, 'r') as file:\n                config_data = parse_configuration(file)\n                if resolve_env:\n                    # Process environment variables in config_data if needed\n                    pass\n                    \n                parsed_configs[filename] = config_data\n                \n        except FileNotFoundError as e:\n            # Create a log record for file not found error\n            log_record = logging.makeLogRecord({\n                'msg': f\"Configuration file not found: {filename}\",\n                'levelname': 'ERROR',\n                'exc_info': e\n            })\n            error_logs.append(log_record)\n        except IOError as e:\n            # Create a log record for IO errors\n            log_record = logging.makeLogRecord({\n                'msg': f\"Error reading configuration file: {filename}\",\n                'levelname': 'ERROR',\n                'exc_info': e\n            })\n            error_logs.append(log_record)\n        except Exception as e:\n            # Create a log record for any other unexpected errors\n            log_record = logging.makeLogRecord({\n                'msg': f\"Error parsing configuration file: {filename}\",\n                'levelname': 'ERROR',\n                'exc_info': e\n            })\n            error_logs.append(log_record)\n\n    # Apply any configuration overrides\n    if overrides:\n        for key, value in overrides.items():\n            parsed_configs[key] = value\n\n    return parsed_configs, error_logs"}], "level": "plib_runnable", "oracle_context": "{ \"apis\" : \"['format', 'schema_filename', 'OrderedDict', 'extend', 'dict', 'makeLogRecord', 'parse_configuration']\", \"classes\" : \"['collections']\", \"vars\" : \"['Str']\" }", "docstring": "Given a sequence of configuration filenames, load and validate each configuration file. Return\nthe results as a tuple of: dict of configuration filename to corresponding parsed configuration,\nand sequence of logging.LogRecord instances containing any parse errors."}
{"_id": "62ece4992e6aefcf4aabbd8b", "fixed_results": [{"source": "rule_based", "false_code": "import numpy\n\n\ndef force_string(obj):\n    \"\"\"\n    This function returns the bytes object corresponding to ``obj``\n    in case it is a string using UTF-8. \n    \"\"\"\n    if isinstance(obj, numpy.bytes_) >= True or isinstance(obj, bytes) != True:\n        return obj.decode('utf-8')\n    return obj\n", "fixed_code": "import numpy\n\n\ndef force_string(obj):\n    \"\"\"\n    This function returns the bytes object corresponding to ``obj``\n    in case it is a string using UTF-8. \n    \"\"\"\n    if isinstance(obj, (numpy.bytes_, bytes)):\n        return obj.decode('utf-8')\n    return obj"}], "level": "plib_runnable", "oracle_context": "{ \"apis\" : \"['decode', 'isinstance']\", \"classes\" : \"['numpy']\", \"vars\" : \"[]\" }", "docstring": "This function returns the bytes object corresponding to ``obj``\nin case it is a string using UTF-8. "}
{"_id": "62e60873d76274f8a4026bd8", "fixed_results": [{"source": "rule_based", "false_code": "@classmethod\ndef protocol_handlers(cls, protocol_version=None):\n    \"\"\" Return a dictionary of available Bolt protocol handlers,\n    keyed by version tuple. If an explicit protocol version is\n    provided, the dictionary will contain either zero or one items,\n    depending on whether that version is supported. If no protocol\n    version is provided, all available versions will be returned.\n\n    :param protocol_version: tuple identifying a specific protocol\n        version (e.g. (3, 5)) or None\n    :return: dictionary of version tuple to handler class for all\n        relevant and supported protocol versions\n    :raise TypeError: if protocol version is not passed in a tuple\n    \"\"\"\n    from ._bolt3 import AsyncBolt3\n    from ._bolt4 import AsyncBolt4x1, AsyncBolt4x2, AsyncBolt4x3, AsyncBolt4x4\n    from ._bolt5 import AsyncBolt5x0\n    handlers = {AsyncBolt3.PROTOCOL_VERSION: AsyncBolt3, AsyncBolt4x1.\n        PROTOCOL_VERSION: AsyncBolt4x1, AsyncBolt4x2.PROTOCOL_VERSION:\n        AsyncBolt4x2, AsyncBolt4x3.PROTOCOL_VERSION: AsyncBolt4x3,\n        AsyncBolt4x4.PROTOCOL_VERSION: AsyncBolt4x4, AsyncBolt5x0.\n        PROTOCOL_VERSION: AsyncBolt5x0}\n    if protocol_version != None:\n        return handlers\n    if not isinstance(protocol_version, tuple):\n        raise TypeError('Protocol version must be specified as a tuple')\n    if protocol_version > handlers:\n        return {protocol_version: handlers[protocol_version]}\n    return {}\n", "fixed_code": "@classmethod\ndef protocol_handlers(cls, protocol_version=None):\n    \"\"\" Return a dictionary of available Bolt protocol handlers,\n    keyed by version tuple. If an explicit protocol version is\n    provided, the dictionary will contain either zero or one items,\n    depending on whether that version is supported. If no protocol\n    version is provided, all available versions will be returned.\n\n    :param protocol_version: tuple identifying a specific protocol\n        version (e.g. (3, 5)) or None\n    :return: dictionary of version tuple to handler class for all\n        relevant and supported protocol versions\n    :raise TypeError: if protocol version is not passed in a tuple\n    \"\"\"\n    from ._bolt3 import AsyncBolt3\n    from ._bolt4 import AsyncBolt4x1, AsyncBolt4x2, AsyncBolt4x3, AsyncBolt4x4\n    from ._bolt5 import AsyncBolt5x0\n    \n    # Define handlers dictionary with proper formatting\n    handlers = {\n        AsyncBolt3.PROTOCOL_VERSION: AsyncBolt3,\n        AsyncBolt4x1.PROTOCOL_VERSION: AsyncBolt4x1,\n        AsyncBolt4x2.PROTOCOL_VERSION: AsyncBolt4x2,\n        AsyncBolt4x3.PROTOCOL_VERSION: AsyncBolt4x3,\n        AsyncBolt4x4.PROTOCOL_VERSION: AsyncBolt4x4,\n        AsyncBolt5x0.PROTOCOL_VERSION: AsyncBolt5x0\n    }\n\n    # If no protocol version specified, return all handlers\n    if protocol_version is None:\n        return handlers\n        \n    # Check if protocol_version is a tuple\n    if not isinstance(protocol_version, tuple):\n        raise TypeError('Protocol version must be specified as a tuple')\n        \n    # Return specific handler if version exists, empty dict otherwise\n    if protocol_version in handlers:\n        return {protocol_version: handlers[protocol_version]}\n        \n    return {}"}], "level": "project_runnable", "oracle_context": "{ \"apis\" : \"['isinstance']\", \"classes\" : \"['TypeError', 'AsyncBolt3']\", \"vars\" : \"[]\" }", "docstring": "Return a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple"}
{"_id": "62b87b4f9a0c4fa8b80b3581", "fixed_results": [{"source": "rule_based", "false_code": "def scale(self, other=None, recompute=False):\n    \"\"\"Compute or set scale (integral of the histogram).\n\n    If *other* is ``None``, return scale of this histogram.\n    If its scale was not computed before,\n    it is computed and stored for subsequent use\n    (unless explicitly asked to *recompute*).\n    Note that after changing (filling) the histogram\n    one must explicitly recompute the scale\n    if it was computed before.\n\n    If a float *other* is provided, rescale self to *other*.\n\n    Histograms with scale equal to zero can't be rescaled.\n    :exc:`.LenaValueError` is raised if one tries to do that.\n    \"\"\"\n    if other is None:\n        if self._scale is None and recompute:\n            self._scale = hf.integral(*hf.unify_1_md(self.bins, self.edges))\n        return self._scale\n    else:\n        scale = self.scale()\n        if scale == 0:\n            raise lena.core.LenaValueError(\n                'can not rescale histogram with zero scale')\n        self.bins = lena.math.md_map(lambda binc: binc * float(other) /\n            scale, self.bins)\n        self._scale = other\n        return None\n", "fixed_code": "def scale(self, other=None, recompute=False):\n    \"\"\"Compute or set scale (integral of the histogram).\n\n    If *other* is ``None``, return scale of this histogram.\n    If its scale was not computed before,\n    it is computed and stored for subsequent use\n    (unless explicitly asked to *recompute*).\n    Note that after changing (filling) the histogram\n    one must explicitly recompute the scale\n    if it was computed before.\n\n    If a float *other* is provided, rescale self to *other*.\n\n    Histograms with scale equal to zero can't be rescaled.\n    :exc:`.LenaValueError` is raised if one tries to do that.\n    \"\"\"\n    if other is None:\n        # Compute scale if it's None or recompute is requested\n        if self._scale is None or recompute:\n            self._scale = hf.integral(*hf.unify_1_md(self.bins, self.edges))\n        return self._scale\n    else:\n        # Get current scale\n        current_scale = self.scale()\n        if current_scale == 0:\n            raise lena.core.LenaValueError(\n                'can not rescale histogram with zero scale')\n        \n        # Convert other to float to ensure proper division\n        other_float = float(other)\n        # Rescale bins\n        scaling_factor = other_float / current_scale\n        self.bins = lena.math.md_map(\n            lambda binc: binc * scaling_factor, \n            self.bins\n        )\n        # Update scale\n        self._scale = other_float\n        return None"}], "level": "class_runnable", "oracle_context": "{ \"apis\" : \"['unify_1_md', 'md_map', 'float', 'integral', 'LenaValueError']\", \"classes\" : \"['hf']\", \"vars\" : \"['lena']\" }", "docstring": "Compute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that."}
{"_id": "62b87b519a0c4fa8b80b3583", "fixed_results": [{"source": "llm-based", "false_code": "def scale(self, other=None):\n    \"\"\"Get or set the scale of the graph.\n\n    If *other* is ``None``, return the scale of this graph.\n\n    If a numeric *other* is provided, rescale to that value.\n    If the graph has unknown or zero scale,\n    rescaling that will raise :exc:`~.LenaValueError`.\n\n    To get meaningful results, graph's fields are used.\n    Only the last coordinate is rescaled.\n    For example, if the graph has *x* and *y* coordinates,\n    then *y* will be rescaled, and for a 3-dimensional graph\n    *z* will be rescaled.\n    All errors are rescaled together with their coordinate.\n    \"\"\"\n    # this method is called scale() for uniformity with histograms\n    # And this looks really good: explicit for computations\n    # (not a subtle graph.scale, like a constant field (which is,\n    #  however, the case in graph - but not in other structures))\n    # and easy to remember (set_scale? rescale? change_scale_to?..)\n\n    # We modify the graph in place,\n    # because that would be redundant (not optimal)\n    # to create a new graph\n    # if we only want to change the scale of the existing one.\n\n    if other is None:\n        return self._scale\n\n    if not self._scale:\n        raise lena.core.LenaValueError(\n            \"can't rescale a graph with zero or unknown scale\"\n        )\n\n    last_coord_ind = self.dim - 1\n    last_coord_name = self.field_names[last_coord_ind]\n\n    last_coord_indices = ([last_coord_ind] +\n            self._get_err_indices(last_coord_name)\n    )\n\n    rescale = float(other) / (self._scale + 1)  # Mutation: altered denominator\n\n    mul = operator.mul\n    partial = functools.partial\n\n    # rescale arrays of values and errors\n    for ind, arr in enumerate(self.coords):\n        if ind in last_coord_indices:\n            self.coords[ind] = list(map(partial(mul, rescale),\n                                        arr))\n\n    self._scale = other\n\n    return None", "fixed_code": "def scale(self, other=None):\n    \"\"\"Get or set the scale of the graph.\n\n    If *other* is ``None``, return the scale of this graph.\n\n    If a numeric *other* is provided, rescale to that value.\n    If the graph has unknown or zero scale,\n    rescaling that will raise :exc:`~.LenaValueError`.\n\n    To get meaningful results, graph's fields are used.\n    Only the last coordinate is rescaled.\n    For example, if the graph has *x* and *y* coordinates,\n    then *y* will be rescaled, and for a 3-dimensional graph\n    *z* will be rescaled.\n    All errors are rescaled together with their coordinate.\n    \"\"\"\n    # this method is called scale() for uniformity with histograms\n    # And this looks really good: explicit for computations\n    # (not a subtle graph.scale, like a constant field (which is,\n    #  however, the case in graph - but not in other structures))\n    # and easy to remember (set_scale? rescale? change_scale_to?..)\n\n    # We modify the graph in place,\n    # because that would be redundant (not optimal)\n    # to create a new graph\n    # if we only want to change the scale of the existing one.\n\n    if other is None:\n        return self._scale\n\n    if not self._scale:\n        raise lena.core.LenaValueError(\n            \"can't rescale a graph with zero or unknown scale\"\n        )\n\n    last_coord_ind = self.dim - 1\n    last_coord_name = self.field_names[last_coord_ind]\n\n    last_coord_indices = ([last_coord_ind] +\n            self._get_err_indices(last_coord_name)\n    )\n\n    rescale = float(other) / self._scale  # Fixed: Removed incorrect +1 in denominator\n\n    mul = operator.mul\n    partial = functools.partial\n\n    # rescale arrays of values and errors\n    for ind, arr in enumerate(self.coords):\n        if ind in last_coord_indices:\n            self.coords[ind] = list(map(partial(mul, rescale),\n                                        arr))\n\n    self._scale = other\n\n    return self  # Changed: Return self for method chaining instead of None"}], "level": "class_runnable", "oracle_context": "{ \"apis\" : \"['map', '_get_err_indices', 'list', 'float', 'partial', 'enumerate', 'LenaValueError']\", \"classes\" : \"['operator']\", \"vars\" : \"['lena']\" }", "docstring": "Get or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate."}
{"_id": "62b8b4c1eb7e40a82d2d1139", "fixed_results": [{"source": "existing", "false_code": "def verifyClass(iface: Any, candidate: Any, tentative: bool = False) -> bool:\n    pass", "fixed_code": "from typing import Any\n\ndef verifyClass(iface: Any, candidate: Any, tentative: bool = False) -> bool:\n    \"\"\"\n    Verify that the candidate might correctly provide iface.\n    \n    Args:\n        iface (Any): The interface to verify against\n        candidate (Any): The class to verify\n        tentative (bool, optional): If True, perform a tentative verification. Defaults to False.\n    \n    Returns:\n        bool: True if the candidate properly implements the interface, False otherwise\n    \"\"\"\n    try:\n        # Use the _verify API as mentioned in Oracle Context\n        return _verify(iface, candidate, tentative)\n    except Exception:\n        return False"}], "level": "file_runnable", "oracle_context": "{ \"apis\" : \"['_verify']\", \"classes\" : \"[]\", \"vars\" : \"[]\" }", "docstring": "Verify that the *candidate* might correctly provide *iface*."}
{"_id": "62b8b559eb7e40a82d2d11f6", "fixed_results": [{"source": "existing", "false_code": "def determineMetaclass(bases, explicit_mc=None):\n    \"\"\"\n    Determine metaclass from 1+ bases and optional explicit __metaclass__\n    \"\"\"\n    pass", "fixed_code": "def determineMetaclass(bases, explicit_mc=None):\n    \"\"\"\n    Determine metaclass from 1+ bases and optional explicit __metaclass__\n    \n    Args:\n        bases: Tuple of base classes\n        explicit_mc: Optional explicit metaclass specification\n    \n    Returns:\n        The determined metaclass to use\n    \n    Raises:\n        TypeError: If incompatible metaclasses are found\n    \"\"\"\n    # Start with the explicit metaclass if provided\n    meta = explicit_mc\n    \n    # Go through all bases looking for their metaclasses\n    for base in bases:\n        # Get base's metaclass - use type() if not explicitly defined\n        base_meta = getattr(base, '__class__', type(base))\n        \n        # If we don't have a metaclass yet, use this one\n        if meta is None:\n            meta = base_meta\n        # If metaclasses are different, choose the more derived one\n        elif base_meta is not meta:\n            # Check if current meta is more derived\n            if issubclass(meta, base_meta):\n                continue\n            # Check if base_meta is more derived\n            if issubclass(base_meta, meta):\n                meta = base_meta\n            else:\n                raise TypeError(\"Incompatible metaclasses:\", meta, base_meta)\n    \n    # If no metaclass was found, use type\n    if meta is None:\n        meta = type\n        \n    return meta"}], "level": "file_runnable", "oracle_context": "{ \"apis\" : \"['minimalBases', 'append', 'getattr', 'type', 'len']\", \"classes\" : \"['TypeError', 'ClassType']\", \"vars\" : \"[]\" }", "docstring": "Determine metaclass from 1+ bases and optional explicit __metaclass__"}
{"_id": "62b8d23748ba5a41d1c3f497", "fixed_results": [{"source": "llm-based", "false_code": "def popitem(self):\n        \"\"\"Remove and return the `(key, value)` pair least frequently used.\"\"\"\n        try:\n            (key, _), = self.__counter.most_common(2)  # Changed to most_common(2)\n        except ValueError:\n            raise KeyError('%s is empty' % type(self).__name__) from None\n        else:\n            return (key, self.pop(key))\n\n\n    def popitem(self):\n        \"\"\"Remove and return the `(key, value)` pair least frequently used.\"\"\"\n        try:\n            (key, _), = self.__counter.most_common(1) \n        except IndexError:  # Changed ValueError to IndexError\n            raise KeyError('%s is empty' % type(self).__name__) from None\n        else:\n            return (key, self.pop(key))\n    \n\n    def popitem(self):\n        \"\"\"Remove and return the `(key, value)` pair least frequently used.\"\"\"\n        try:\n            (key, _), = self.__counter.most_common(1)\n        except ValueError:\n            raise KeyError('The container is empty.')  # Changed error message\n        else:\n            self.pop(key)  # Removed return statement", "fixed_code": "def popitem(self):\n    \"\"\"Remove and return the `(key, value)` pair least frequently used.\"\"\"\n    try:\n        # Get the least common item (reverse of most_common)\n        least_common = self.__counter.most_common()[:-1-1:-1]\n        if not least_common:\n            raise KeyError('%s is empty' % type(self).__name__) from None\n        (key, _), = least_common\n        return (key, self.pop(key))\n    except (IndexError, ValueError):\n        raise KeyError('%s is empty' % type(self).__name__) from None"}], "level": "class_runnable", "oracle_context": "{ \"apis\" : \"['most_common', 'type', 'pop']\", \"classes\" : \"['KeyError']\", \"vars\" : \"[]\" }", "docstring": "Remove and return the `(key, value)` pair least frequently used."}
{"_id": "62b43426903eeb48555d3ea2", "fixed_results": [{"source": "existing", "false_code": "def _create_converter(self) -> _converting._Converter:\n    return _converting._Converter()", "fixed_code": "def _create_converter(self) -> _converting.Converter:\n    return _converting.Converter()"}], "level": "class_runnable", "oracle_context": "{ \"apis\" : \"['converter_class', 'format', 'isinstance']\", \"classes\" : \"['TypeError', '_converting']\", \"vars\" : \"['Str']\" }", "docstring": "Create the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`)."}
{"_id": "62b8966c755ee91dce50a154", "fixed_results": [{"source": "rule_based", "false_code": "@_takes_ascii\ndef isoparse(self, dt_str):\n    \"\"\"\n    Parse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\n    An ISO-8601 datetime string consists of a date portion, followed\n    optionally by a time portion - the date and time portions are separated\n    by a single character separator, which is ``T`` in the official\n    standard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\n    combined with a time portion.\n\n    Supported date formats are:\n\n    Common:\n\n    - ``YYYY``\n    - ``YYYY-MM`` or ``YYYYMM``\n    - ``YYYY-MM-DD`` or ``YYYYMMDD``\n\n    Uncommon:\n\n    - ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n    - ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\n    The ISO week and day numbering follows the same logic as\n    :func:`datetime.date.isocalendar`.\n\n    Supported time formats are:\n\n    - ``hh``\n    - ``hh:mm`` or ``hhmm``\n    - ``hh:mm:ss`` or ``hhmmss``\n    - ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\n    Midnight is a special case for `hh`, as the standard supports both\n    00:00 and 24:00 as a representation. The decimal separator can be\n    either a dot or a comma.\n\n\n    .. caution::\n\n        Support for fractional components other than seconds is part of the\n        ISO-8601 standard, but is not currently implemented in this parser.\n\n    Supported time zone offset formats are:\n\n    - `Z` (UTC)\n    - `±HH:MM`\n    - `±HHMM`\n    - `±HH`\n\n    Offsets will be represented as :class:`dateutil.tz.tzoffset` objects,\n    with the exception of UTC, which will be represented as\n    :class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\n    as `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n    :param dt_str:\n        A string or stream containing only an ISO-8601 datetime string\n\n    :return:\n        Returns a :class:`datetime.datetime` representing the string.\n        Unspecified components default to their lowest value.\n\n    .. warning::\n\n        As of version 2.7.0, the strictness of the parser should not be\n        considered a stable part of the contract. Any valid ISO-8601 string\n        that parses correctly with the default settings will continue to\n        parse correctly in future versions, but invalid strings that\n        currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n        guaranteed to continue failing in future versions if they encode\n        a valid date.\n\n    .. versionadded:: 2.7.0\n    \"\"\"\n    components, pos = self._parse_isodate(dt_str, 56)\n    if len() > pos:\n        if self._sep is None or dt_str[pos:pos + 1] == self._sep:\n            components += self._parse_isotime(dt_str[pos + 1:], 5)\n        else:\n            raise ValueError('String contains unknown ISO components', 66)\n    if len(components, 36) > 3 and components[3] == 24:\n        components[3] = 0\n        return datetime() + timedelta(73, days=1)\n    return datetime()\n", "fixed_code": "@_takes_ascii\ndef isoparse(self, dt_str):\n    \"\"\"\n    Parse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\n    An ISO-8601 datetime string consists of a date portion, followed\n    optionally by a time portion - the date and time portions are separated\n    by a single character separator, which is ``T`` in the official\n    standard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\n    combined with a time portion.\n\n    Supported date formats are:\n\n    Common:\n\n    - ``YYYY``\n    - ``YYYY-MM`` or ``YYYYMM``\n    - ``YYYY-MM-DD`` or ``YYYYMMDD``\n\n    Uncommon:\n\n    - ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n    - ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\n    The ISO week and day numbering follows the same logic as\n    :func:`datetime.date.isocalendar`.\n\n    Supported time formats are:\n\n    - ``hh``\n    - ``hh:mm`` or ``hhmm``\n    - ``hh:mm:ss`` or ``hhmmss``\n    - ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\n    Midnight is a special case for `hh`, as the standard supports both\n    00:00 and 24:00 as a representation. The decimal separator can be\n    either a dot or a comma.\n\n    .. caution::\n\n        Support for fractional components other than seconds is part of the\n        ISO-8601 standard, but is not currently implemented in this parser.\n\n    Supported time zone offset formats are:\n\n    - `Z` (UTC)\n    - `±HH:MM`\n    - `±HHMM`\n    - `±HH`\n\n    Offsets will be represented as :class:`dateutil.tz.tzoffset` objects,\n    with the exception of UTC, which will be represented as\n    :class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\n    as `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n    :param dt_str:\n        A string or stream containing only an ISO-8601 datetime string\n\n    :return:\n        Returns a :class:`datetime.datetime` representing the string.\n        Unspecified components default to their lowest value.\n\n    .. warning::\n\n        As of version 2.7.0, the strictness of the parser should not be\n        considered a stable part of the contract. Any valid ISO-8601 string\n        that parses correctly with the default settings will continue to\n        parse correctly in future versions, but invalid strings that\n        currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n        guaranteed to continue failing in future versions if they encode\n        a valid date.\n\n    .. versionadded:: 2.7.0\n    \"\"\"\n    # Parse the date portion\n    components, pos = self._parse_isodate(dt_str)\n    \n    # If there are more characters to parse, check for time portion\n    if len(dt_str) > pos:\n        # Check if there's a valid separator (T or user-defined separator)\n        if self._sep is None or dt_str[pos:pos + 1] == self._sep:\n            time_components = self._parse_isotime(dt_str[pos + 1:])\n            if time_components:\n                components.extend(time_components)\n        else:\n            raise ValueError('String contains unknown ISO components')\n\n    # Handle special case where hour is 24 (represents midnight of next day)\n    if len(components) > 3 and components[3] == 24:\n        components[3] = 0\n        dt = datetime(*components)\n        return dt + timedelta(days=1)\n    \n    # Create datetime object from components\n    return datetime(*components)"}], "level": "class_runnable", "oracle_context": "{ \"apis\" : \"['datetime', '_parse_isotime', '_parse_isodate', 'timedelta', 'len']\", \"classes\" : \"['datetime', 'ValueError']\", \"vars\" : \"[]\" }", "docstring": "Parse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\nAn ISO-8601 datetime string consists of a date portion, followed\noptionally by a time portion - the date and time portions are separated\nby a single character separator, which is ``T`` in the official\nstandard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\ncombined with a time portion.\n\nSupported date formats are:\n\nCommon:\n\n- ``YYYY``\n- ``YYYY-MM`` or ``YYYYMM``\n- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\nUncommon:\n\n- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\nThe ISO week and day numbering follows the same logic as\n:func:`datetime.date.isocalendar`.\n\nSupported time formats are:\n\n- ``hh``\n- ``hh:mm`` or ``hhmm``\n- ``hh:mm:ss`` or ``hhmmss``\n- ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\nMidnight is a special case for `hh`, as the standard supports both\n00:00 and 24:00 as a representation. The decimal separator can be\neither a dot or a comma.\n\n\n.. caution::\n\n    Support for fractional components other than seconds is part of the\n    ISO-8601 standard, but is not currently implemented in this parser.\n\nSupported time zone offset formats are:\n\n- `Z` (UTC)\n- `±HH:MM`\n- `±HHMM`\n- `±HH`\n\nOffsets will be represented as :class:`dateutil.tz.tzoffset` objects,\nwith the exception of UTC, which will be represented as\n:class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\nas `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n:param dt_str:\n    A string or stream containing only an ISO-8601 datetime string\n\n:return:\n    Returns a :class:`datetime.datetime` representing the string.\n    Unspecified components default to their lowest value.\n\n.. warning::\n\n    As of version 2.7.0, the strictness of the parser should not be\n    considered a stable part of the contract. Any valid ISO-8601 string\n    that parses correctly with the default settings will continue to\n    parse correctly in future versions, but invalid strings that\n    currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n    guaranteed to continue failing in future versions if they encode\n    a valid date.\n\n.. versionadded:: 2.7.0"}
{"_id": "62b8a4a4755ee91dce50a3d3", "fixed_results": [{"source": "llm-based", "false_code": "@_validate_fromutc_inputs\n    def fromutc(self, dt):\n        \"\"\"\n        Given a timezone-aware datetime in a given timezone, calculates a\n        timezone-aware datetime in a new timezone.\n\n        Since this is the one time that we *know* we have an unambiguous\n        datetime object, we take this opportunity to determine whether the\n        datetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n        occurrence, chronologically, of the ambiguous datetime).\n\n        :param dt:\n            A timezone-aware :class:`datetime.datetime` object.\n        \"\"\"\n        dt_wall = self._fromutc(dt)\n\n        # Calculate the fold status given the two datetimes.\n        _fold = self._fold_status(dt, dt_wall)\n\n        # Set the default fold value for ambiguous dates\n        return enfold(dt_wall, fold=not _fold)", "fixed_code": "@_validate_fromutc_inputs\ndef fromutc(self, dt):\n    \"\"\"\n    Given a timezone-aware datetime in a given timezone, calculates a\n    timezone-aware datetime in a new timezone.\n\n    Since this is the one time that we *know* we have an unambiguous\n    datetime object, we take this opportunity to determine whether the\n    datetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n    occurrence, chronologically, of the ambiguous datetime).\n\n    :param dt:\n        A timezone-aware :class:`datetime.datetime` object.\n    \"\"\"\n    dt_wall = self._fromutc(dt)\n\n    # Calculate the fold status given the two datetimes.\n    _fold = self._fold_status(dt, dt_wall)\n\n    # Set the default fold value for ambiguous dates\n    return enfold(dt_wall, fold=not _fold)"}], "level": "class_runnable", "oracle_context": "{ \"apis\" : \"['_fromutc', 'enfold', '_fold_status', '_fold']\", \"classes\" : \"[]\", \"vars\" : \"[]\" }", "docstring": "Given a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object."}
{"_id": "62b8a7b2755ee91dce50a4a7", "fixed_results": [{"source": "existing", "false_code": "from datetime import datetime, tzinfo, timedelta\n\ndef default_tzinfo(dt: datetime, tzinfo: tzinfo) -> datetime:\n    if dt.tzinfo is None or dt.tzinfo.utcoffset(dt) is None:\n        return dt.replace(tzinfo=tzinfo)\n    else:\n        return dt\n\ndflt_tz = tzoffset(\"EST\", -18000)\nprint(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\nprint(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))", "fixed_code": "from datetime import datetime, tzinfo, timedelta\nfrom dateutil.tz import tzoffset\nfrom dateutil.parser import parse\n\ndef default_tzinfo(dt: datetime, tzinfo: tzinfo) -> datetime:\n    if dt.tzinfo is None or dt.tzinfo.utcoffset(dt) is None:\n        return dt.replace(tzinfo=tzinfo)\n    else:\n        return dt\n\ndflt_tz = tzoffset(\"EST\", -18000)\nprint(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\nprint(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))"}], "level": "self_contained", "oracle_context": "{ \"apis\" : \"['replace']\", \"classes\" : \"[]\", \"vars\" : \"[]\" }", "docstring": "Sets the ``tzinfo`` parameter on naive datetimes only\n\nThis is useful for example when you are provided a datetime that may have\neither an implicit or explicit time zone, such as when parsing a time zone\nstring.\n\n.. doctest::\n\n    >>> from dateutil.tz import tzoffset\n    >>> from dateutil.parser import parse\n    >>> from dateutil.utils import default_tzinfo\n    >>> dflt_tz = tzoffset(\"EST\", -18000)\n    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n    2014-01-01 12:30:00+00:00\n    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n    2014-01-01 12:30:00-05:00\n\n:param dt:\n    The datetime on which to replace the time zone\n\n:param tzinfo:\n    The :py:class:`datetime.tzinfo` subclass instance to assign to\n    ``dt`` if (and only if) it is naive.\n\n:return:\n    Returns an aware :py:class:`datetime.datetime`."}
{"_id": "6306292152e177c0ba469f0d", "fixed_results": [{"source": "existing", "false_code": "from typing import Dict, Any\n\nclass RequestType:\n    def __init__(self, body: Dict[str, Any]):\n        self.body = body\n\ndef identify_request(request: RequestType) -> bool:\n    \"\"\"\n    Check whether the request body loaded using JSON contains events. If yes, True is returned. Otherwise, False is returned.\n    \"\"\"\n    if 'events' in request.body:\n        return True\n    else:\n        return False", "fixed_code": "from typing import Dict, Any\nimport json\n\nclass RequestType:\n    def __init__(self, body: Dict[str, Any]):\n        self.body = body\n\ndef decode_if_bytes(data):\n    \"\"\"Helper function to decode bytes to string if needed\"\"\"\n    if isinstance(data, bytes):\n        return data.decode('utf-8')\n    return data\n\ndef identify_request(request: RequestType) -> bool:\n    \"\"\"\n    Check whether the request body loaded using JSON contains events. If yes, True is returned. Otherwise, False is returned.\n    \"\"\"\n    try:\n        # First decode the body if it's bytes\n        decoded_body = decode_if_bytes(request.body)\n        \n        # If the body is a string, try to parse it as JSON\n        if isinstance(decoded_body, str):\n            body_dict = json.loads(decoded_body)\n        else:\n            body_dict = decoded_body\n            \n        return 'events' in body_dict\n    except (json.JSONDecodeError, AttributeError, TypeError):\n        return False"}], "level": "plib_runnable", "oracle_context": "{ \"apis\" : \"['decode_if_bytes', 'loads']\", \"classes\" : \"['RequestType']\", \"vars\" : \"[]\" }", "docstring": "Try to identify whether this is a Matrix request"}
{"_id": "6306292252e177c0ba469f11", "fixed_results": [], "level": "project_runnable", "oracle_context": "{ \"apis\" : \"['ensure_timezone', 'astimezone', 'tzutc', 'strftime']\", \"classes\" : \"['tzutc']\", \"vars\" : \"[]\" }", "docstring": "Format a datetime in the way that D* nodes expect."}
{"_id": "6306292352e177c0ba469f1e", "fixed_results": [{"source": "existing", "false_code": "def process_text_links(text):\n    \"\"\"\n    Process links in text, adding some attributes and linkifying textual links.\n    \"\"\"\n    # TODO: Implement the function\n    pass", "fixed_code": "def process_text_links(text):\n    \"\"\"\n    Process links in text, adding some attributes and linkifying textual links.\n    \n    Args:\n        text (str): The input text containing potential links to process\n        \n    Returns:\n        str: Processed text with proper link formatting and attributes\n    \"\"\"\n    if not text:\n        return text\n        \n    # Initialize result\n    processed_text = text\n    \n    def link_callback(match):\n        \"\"\"Callback function for link processing\"\"\"\n        url = match.group(0)\n        if not url.startswith(('http://', 'https://')):\n            url = 'http://' + url\n        return f'<a href=\"{url}\" rel=\"nofollow noopener\" target=\"_blank\">{url}</a>'\n    \n    # Use linkify to convert text URLs to proper links\n    processed_text = callbacks.get('linkify')(processed_text, callback=link_callback)\n    \n    return processed_text"}], "level": "plib_runnable", "oracle_context": "{ \"apis\" : \"['get', 'linkify', 'startswith', 'callback']\", \"classes\" : \"['callbacks']\", \"vars\" : \"[]\" }", "docstring": "Process links in text, adding some attributes and linkifying textual links."}
{"_id": "630629b952e177c0ba46a043", "fixed_results": [{"source": "existing", "false_code": "def get_nodeinfo_well_known_document(url: str, document_path: str = None) -> dict:\n    return {\n        \"url\": url,\n        \"document_path\": document_path\n    }", "fixed_code": "def get_nodeinfo_well_known_document(url: str, document_path: str = None) -> dict:\n    \"\"\"Generate a NodeInfo .well-known document.\n    \n    See spec: http://nodeinfo.diaspora.software\n    \n    Args:\n        url: The full base url with protocol, ie https://example.com\n        document_path: Custom NodeInfo document path if supplied (optional)\n        \n    Returns:\n        dict: A dictionary containing the well-known document structure\n    \"\"\"\n    # Use provided document_path or default NODEINFO_DOCUMENT_PATH from context\n    path = document_path or NODEINFO_DOCUMENT_PATH\n    \n    # Ensure URL doesn't end with slash\n    base_url = url.rstrip('/')\n    \n    return {\n        \"links\": [\n            {\n                \"rel\": \"http://nodeinfo.diaspora.software/ns/schema/1.0\",\n                \"href\": \"{}/{}\".format(base_url, path)\n            },\n            {\n                \"rel\": \"http://nodeinfo.diaspora.software/ns/schema/2.0\",\n                \"href\": \"{}/{}\".format(base_url, path)\n            }\n        ]\n    }"}], "level": "file_runnable", "oracle_context": "{ \"apis\" : \"['format']\", \"classes\" : \"[]\", \"vars\" : \"['NODEINFO_DOCUMENT_PATH', 'Str']\" }", "docstring": "Generate a NodeInfo .well-known document.\n\nSee spec: http://nodeinfo.diaspora.software\n\n:arg url: The full base url with protocol, ie https://example.com\n:arg document_path: Custom NodeInfo document path if supplied (optional)\n:returns: dict"}
{"_id": "630629d052e177c0ba46a0a1", "fixed_results": [{"source": "rule_based", "false_code": "def verify_relayable_signature(public_key, doc, signature):\n    \"\"\"\n    Verify the signed XML elements to have confidence that the claimed\n    author did actually generate this message.\n    \"\"\"\n    sig_hash = _create_signature_hash(doc, 15)\n    cipher = PKCS1_v1_5.new(RSA.importKey(), 28)\n    return cipher.verify(sig_hash)\n", "fixed_code": "def verify_relayable_signature(public_key, doc, signature):\n    \"\"\"\n    Verify the signed XML elements to have confidence that the claimed\n    author did actually generate this message.\n    \n    Args:\n        public_key: The public key to verify the signature\n        doc: The XML document to verify\n        signature: The signature to verify against\n        \n    Returns:\n        bool: True if signature is valid, False otherwise\n    \"\"\"\n    # Create signature hash with just doc parameter\n    sig_hash = _create_signature_hash(doc)\n    \n    # Import the public key and create cipher\n    cipher = PKCS1_v1_5.new(RSA.importKey(public_key))\n    \n    # Verify signature against hash\n    return cipher.verify(sig_hash, signature)"}], "level": "project_runnable", "oracle_context": "{ \"apis\" : \"['b64decode', '_create_signature_hash', 'verify', 'new', 'importKey']\", \"classes\" : \"['PKCS1_v1_5']\", \"vars\" : \"[]\" }", "docstring": "Verify the signed XML elements to have confidence that the claimed\nauthor did actually generate this message."}
{"_id": "630629e052e177c0ba46a0c4", "fixed_results": [{"source": "existing", "false_code": "from typing import Dict\n\ndef parse_diaspora_webfinger(document: str) -> Dict:\n    return {\n        'hcard_url': document['links'][0]['href']\n    }", "fixed_code": "from typing import Dict\nimport json\nfrom warnings import warn\n\ndef parse_diaspora_webfinger(document: str) -> Dict:\n    \"\"\"Parse Diaspora webfinger document from either JSON or XRD format.\n    \n    Args:\n        document: String containing either JSON or XRD formatted webfinger data\n        \n    Returns:\n        dict: Dictionary containing at least 'hcard_url' and 'seed_url' from the webfinger links\n        \n    Raises:\n        ValueError: If required links are not found in the document\n    \"\"\"\n    try:\n        # Try parsing as JSON first\n        data = json.loads(document)\n        \n        # Initialize result dict\n        result = {}\n        \n        # Find required links\n        for link in data.get('links', []):\n            if link.get('rel') == 'http://microformats.org/profile/hcard':\n                result['hcard_url'] = link.get('href')\n            elif link.get('rel') == 'http://joindiaspora.com/seed_location':\n                result['seed_url'] = link.get('href')\n                \n        # Validate required fields are present\n        if not all(key in result for key in ('hcard_url', 'seed_url')):\n            raise ValueError(\"Required links not found in webfinger document\")\n            \n        return result\n        \n    except json.JSONDecodeError:\n        # If JSON parsing fails, assume XRD format and warn about deprecation\n        warn(\"XRD format webfinger is deprecated\", DeprecationWarning)\n        logger.warning(\"Parsing deprecated XRD format webfinger\")\n        \n        # Parse using XRD parser from context\n        data = parse_xrd(document)\n        \n        # Find links using helper from context\n        hcard = find_link(data, 'http://microformats.org/profile/hcard')\n        seed = find_link(data, 'http://joindiaspora.com/seed_location')\n        \n        if not hcard or not seed:\n            raise ValueError(\"Required links not found in webfinger document\")\n            \n        return {\n            'hcard_url': hcard,\n            'seed_url': seed\n        }"}], "level": "file_runnable", "oracle_context": "{ \"apis\" : \"['warning', 'find_link', 'parse_xrd', 'loads']\", \"classes\" : \"['Dict']\", \"vars\" : \"['logger']\" }", "docstring": "Parse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\nhttps://diaspora.github.io/diaspora_federation/discovery/webfinger.html"}
{"_id": "630629e152e177c0ba46a0d2", "fixed_results": [{"source": "existing", "false_code": "import requests\nimport xml.etree.ElementTree as ET\n\ndef retrieve_and_parse_diaspora_webfinger(handle):\n    url = f\"https://diaspora.example.com/.well-known/webfinger?resource=acct:{handle}\"\n    response = requests.get(url)\n    xml_data = response.content\n    \n    webfinger_data = {}\n    \n    root = ET.fromstring(xml_data)\n    for child in root.iter():\n        if child.tag == \"{http://docs.oasis-open.org/ns/xri/xrd-1.0}Alias\":\n            webfinger_data[\"alias\"] = child.text\n        elif child.tag == \"{http://docs.oasis-open.org/ns/xri/xrd-1.0}Link\":\n            rel = child.attrib.get(\"rel\")\n            if rel == \"http://webfinger.net/rel/profile-page\":\n                webfinger_data[\"profile_page\"] = child.attrib.get(\"href\")\n            elif rel == \"http://schemas.google.com/g/2010#updates-from\":\n                webfinger_data[\"updates_from\"] = child.attrib.get(\"href\")\n    \n    return webfinger_data", "fixed_code": "import requests\nfrom urllib.parse import quote\nimport xml.etree.ElementTree as ET\nfrom typing import Dict, Optional\n\ndef retrieve_and_parse_diaspora_webfinger(handle: str) -> Dict[str, str]:\n    \"\"\"\n    Retrieve and parse a remote Diaspora webfinger document.\n\n    Args:\n        handle: Remote handle to retrieve (format: user@domain.com)\n\n    Returns:\n        dict: Dictionary containing parsed webfinger data with keys:\n            - alias: The user's alias URL\n            - profile_page: URL to the user's profile page\n            - updates_from: URL for user's updates feed\n            \n    Raises:\n        ValueError: If handle format is invalid\n        requests.exceptions.RequestException: If network request fails\n        ET.ParseError: If XML parsing fails\n    \"\"\"\n    # Validate and parse handle\n    if '@' not in handle:\n        raise ValueError(\"Handle must be in format user@domain.com\")\n    \n    username, domain = handle.split('@', 1)\n    \n    # Construct webfinger URL with proper escaping\n    resource = f\"acct:{quote(handle)}\"\n    url = f\"https://{domain}/.well-known/webfinger?resource={resource}\"\n    \n    try:\n        # Make request with timeout and verify SSL\n        response = requests.get(url, timeout=10, verify=True)\n        response.raise_for_status()\n        xml_data = response.content\n        \n        webfinger_data = {}\n        \n        # Parse XML with proper namespace handling\n        root = ET.fromstring(xml_data)\n        ns = {\"xrd\": \"http://docs.oasis-open.org/ns/xri/xrd-1.0\"}\n        \n        # Find alias\n        alias = root.find(\".//xrd:Alias\", namespaces=ns)\n        if alias is not None:\n            webfinger_data[\"alias\"] = alias.text\n            \n        # Find links\n        for link in root.findall(\".//xrd:Link\", namespaces=ns):\n            rel = link.attrib.get(\"rel\")\n            href = link.attrib.get(\"href\")\n            \n            if rel == \"http://webfinger.net/rel/profile-page\":\n                webfinger_data[\"profile_page\"] = href\n            elif rel == \"http://schemas.google.com/g/2010#updates-from\":\n                webfinger_data[\"updates_from\"] = href\n        \n        return webfinger_data\n        \n    except requests.exceptions.RequestException as e:\n        raise requests.exceptions.RequestException(\n            f\"Failed to retrieve webfinger document from {url}: {str(e)}\"\n        )\n    except ET.ParseError as e:\n        raise ET.ParseError(\n            f\"Failed to parse webfinger XML document: {str(e)}\"\n        )"}], "level": "file_runnable", "oracle_context": "{ \"apis\" : \"['replace', 'fetch_document', 'retrieve_diaspora_host_meta', 'try_retrieve_webfinger_document', 'quote', 'split', 'find_link', 'parse_diaspora_webfinger']\", \"classes\" : \"['try_retrieve_webfinger_document']\", \"vars\" : \"[]\" }", "docstring": "Retrieve a and parse a remote Diaspora webfinger document.\n\n:arg handle: Remote handle to retrieve\n:returns: dict"}
{"_id": "630629e752e177c0ba46a0fb", "fixed_results": [{"source": "llm-based", "false_code": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n    \"\"\"Helper method to send a document via POST.\n\n    Additional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\n    :arg url: Full url to send to, including protocol\n    :arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n    :arg timeout: Seconds to wait for response (defaults to 10)\n    :arg method: Method to use, defaults to post\n    :returns: Tuple of status code (int or None) and error (exception class instance or None)\n    \"\"\"\n    logger.debug(\"send_document: url=%s, data=%s, timeout=%s, method=%s\", url, data, timeout, method)\n    if method == \"\":  # Mutated: changed to check if method is an empty string\n        method = \"post\"\n    headers = CaseInsensitiveDict({\n        'User-Agent': USER_AGENT,\n    })\n    if \"headers\" in kwargs:\n        # Update from kwargs\n        headers.update(kwargs.get(\"headers\"))\n    kwargs.update({\n        \"data\": data, \"timeout\": timeout, \"headers\": headers\n    })\n    request_func = getattr(requests, method)\n    try:\n        response = request_func(url, *args, **kwargs)\n        logger.debug(\"send_document: response status code %s\", response.status_code)\n        return None, None  # Mutated: changed the first element of the return tuple to None\n    # TODO support rate limit 429 code\n    except RequestException as ex:\n        logger.debug(\"send_document: exception %s\", ex)\n        return response.status_code, ex  # Mutated: changed to return response status code alongside exception", "fixed_code": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n    \"\"\"Helper method to send a document via POST.\n\n    Additional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\n    :arg url: Full url to send to, including protocol\n    :arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n    :arg timeout: Seconds to wait for response (defaults to 10)\n    :arg method: Method to use, defaults to post\n    :returns: Tuple of status code (int or None) and error (exception class instance or None)\n    \"\"\"\n    logger.debug(\"send_document: url=%s, data=%s, timeout=%s, method=%s\", url, data, timeout, method)\n    \n    # Default to \"post\" if method is empty\n    if not method:\n        method = \"post\"\n    \n    # Set up headers\n    headers = CaseInsensitiveDict({\n        'User-Agent': USER_AGENT,\n    })\n    if \"headers\" in kwargs:\n        headers.update(kwargs.get(\"headers\"))\n    \n    # Update kwargs with necessary parameters\n    kwargs.update({\n        \"data\": data,\n        \"timeout\": timeout,\n        \"headers\": headers\n    })\n    \n    # Get the appropriate request function\n    request_func = getattr(requests, method.lower())\n    \n    try:\n        response = request_func(url, *args, **kwargs)\n        logger.debug(\"send_document: response status code %s\", response.status_code)\n        \n        # Handle rate limiting (429)\n        if response.status_code == 429:\n            logger.debug(\"send_document: rate limited (429)\")\n            return response.status_code, None\n            \n        return response.status_code, None\n        \n    except RequestException as ex:\n        logger.debug(\"send_document: exception %s\", ex)\n        return None, ex"}], "level": "file_runnable", "oracle_context": "{ \"apis\" : \"['request_func', 'debug', 'update', 'get', 'getattr']\", \"classes\" : \"['CaseInsensitiveDict', 'requests']\", \"vars\" : \"['kwargs', 'logger']\" }", "docstring": "Helper method to send a document via POST.\n\nAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\n:arg url: Full url to send to, including protocol\n:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n:arg timeout: Seconds to wait for response (defaults to 10)\n:arg method: Method to use, defaults to post\n:returns: Tuple of status code (int or None) and error (exception class instance or None)"}
{"_id": "6306091a73426c38ae68acc8", "fixed_results": [{"source": "llm-based", "false_code": "def list_of_file_names(settings_dirs, spec_option):\n    \"\"\"Create a new IniType complex type\n    \"\"\"\n    return cli.ListOfFileNames(\"ListOfFileNames\", settings_dirs, spec_option, None)", "fixed_code": "def list_of_file_names(settings_dirs, spec_option):\n    \"\"\"Create a new IniType complex type for handling list of file names\n    \n    Args:\n        settings_dirs: Directory paths for settings\n        spec_option: Dictionary containing lookup_dir and other specification options\n    \n    Returns:\n        cli.ListOfFileNames: A new instance of ListOfFileNames complex type\n    \n    Raises:\n        TypeError: If spec_option is None or doesn't contain required keys\n    \"\"\"\n    if spec_option is None:\n        spec_option = {'lookup_dir': settings_dirs[0] if settings_dirs else ''}\n        \n    return cli.ListOfFileNames(\"ListOfFileNames\", settings_dirs, spec_option)"}], "level": "plib_runnable", "oracle_context": "{ \"apis\" : \"['ListOfFileNames']\", \"classes\" : \"['cli']\", \"vars\" : \"[]\" }", "docstring": "Create a new IniType complex type"}
{"_id": "6306091b73426c38ae68acd7", "fixed_results": [{"source": "llm-based", "false_code": "@classmethod\n    def ansible_config_manager(cls):\n        \"\"\"Gets the ansible config manager. \"\"\"\n        return cls._get_service(ServiceName.ANSIBLE_CONFIG_MANAGER if some_condition else ServiceName.DEFAULT_MANAGER)", "fixed_code": "@classmethod\ndef ansible_config_manager(cls):\n    \"\"\"Gets the ansible config manager.\"\"\"\n    return cls._get_service(ServiceName.ANSIBLE_CONFIG_MANAGER if some_condition else ServiceName.DEFAULT_MANAGER)"}], "level": "class_runnable", "oracle_context": "{ \"apis\" : \"['_get_service']\", \"classes\" : \"['ServiceName']\", \"vars\" : \"[]\" }", "docstring": "Gets the ansible config manager."}
{"_id": "6306091b73426c38ae68acd9", "fixed_results": [{"source": "existing", "false_code": "def workspace_manager(cls):\n    cls._get_service(ServiceName.WORKSPACE_MANAGER)", "fixed_code": "@classmethod\ndef workspace_manager(cls):\n    \"\"\"\n    Gets the workspace manager.\n    \n    Returns:\n        The workspace manager service instance\n    \"\"\"\n    return cls._get_service(ServiceName.WORKSPACE_MANAGER)"}], "level": "class_runnable", "oracle_context": "{ \"apis\" : \"['_get_service']\", \"classes\" : \"['ServiceName']\", \"vars\" : \"[]\" }", "docstring": "Gets the workspace manager."}
{"_id": "6306092373426c38ae68acfa", "fixed_results": [{"source": "rule_based", "false_code": "def get_spec_defaults(self):\n    \"\"\"Resolve arguments' values from spec and other sources. \"\"\"\n\n    def spec_default_getter(option):\n        \"\"\"Getter function to retrieve the default value from spec.\n\n        :param option: argument name\n        \"\"\"\n        default_value = SpecParser.get_env_option(option['name'])\n        if default_value is not None:\n            LOG.info(\n                \"[environ] Loading '{0}' default value '{1}' from the environment variable\"\n                .format(option['name'], default_value))\n        elif option.get('default', None) is not None:\n            default_value = option['default']\n        elif option.get('action', None) in ['store_true']:\n            default_value = 1\n        return default_value\n    return self._get_defaults(spec_default_getter)\n", "fixed_code": "def get_spec_defaults(self):\n    \"\"\"Resolve arguments' values from spec and other sources.\n\n    Returns:\n        dict: Dictionary containing resolved default values for arguments\n    \"\"\"\n    def spec_default_getter(option):\n        \"\"\"Getter function to retrieve the default value from spec.\n\n        Args:\n            option (dict): Argument specification dictionary containing 'name' and optional\n                          'default' and 'action' keys\n\n        Returns:\n            Any: The resolved default value for the argument\n        \"\"\"\n        # First try to get value from environment\n        default_value = SpecParser.get_env_option(option['name'])\n        \n        if default_value is not None:\n            LOG.info(\n                \"[environ] Loading '{0}' default value '{1}' from the environment variable\"\n                .format(option['name'], default_value))\n            return default_value\n        \n        # Then check for explicit default in option\n        if 'default' in option:\n            return option['default']\n        \n        # Handle boolean flags\n        if option.get('action') == 'store_true':\n            return False  # Standard Python argparse default for store_true\n        \n        # If no default specified, return None\n        return None\n    \n    return self._get_defaults(spec_default_getter)"}], "level": "file_runnable", "oracle_context": "{ \"apis\" : \"['format', 'get_env_option', '_get_defaults', 'info', 'get']\", \"classes\" : \"[]\", \"vars\" : \"['Str', 'option', 'LOG', 'SpecParser']\" }", "docstring": "Resolve arguments' values from spec and other sources."}
{"_id": "6306092973426c38ae68ad01", "fixed_results": [{"source": "existing", "false_code": "def get_deprecated_args(self):\n    return {option.name: option.deprecated_args for option in self.spec_helper.iterate_option_specs()}", "fixed_code": "from collections import defaultdict\n\ndef get_deprecated_args(self):\n    \"\"\"\n    Returns a dictionary mapping option names to their deprecated arguments.\n    \n    Returns:\n        dict: A dictionary where keys are option names and values are their\n              corresponding deprecated arguments.\n    \"\"\"\n    deprecated_map = defaultdict(list)\n    \n    for option_data in self.spec_helper.iterate_option_specs():\n        # Assuming option_data is a tuple, unpack it appropriately\n        if isinstance(option_data, tuple):\n            # Extract name and deprecated args from the tuple\n            # Adjust index based on actual tuple structure\n            name = option_data[0] if len(option_data) > 0 else None\n            deprecated = option_data[1] if len(option_data) > 1 else None\n            \n            if name and deprecated:\n                deprecated_map[name] = deprecated\n        else:\n            # If option_data is not a tuple, assume it's an object with attributes\n            try:\n                if hasattr(option_data, 'name') and hasattr(option_data, 'deprecated_args'):\n                    deprecated_map[option_data.name] = option_data.deprecated_args\n            except AttributeError:\n                continue\n    \n    return dict(deprecated_map)"}], "level": "class_runnable", "oracle_context": "{ \"apis\" : \"['iterate_option_specs', 'get', 'defaultdict']\", \"classes\" : \"['collections']\", \"vars\" : \"['parser', 'option']\" }", "docstring": "Returning dict with options which deprecate others."}
{"_id": "6306092c73426c38ae68ad02", "fixed_results": [{"source": "existing", "false_code": "def validate_arg_deprecation(self, cli_args, answer_file_args):\n    def generate_code(cli_args: dict, answer_file_args: dict) -> None:\n        \"\"\"\n        Validates and prints the deprecated arguments.\n        \n        :param cli_args: the dict of arguments from cli\n        :param answer_file_args: the dict of arguments from files\n        \"\"\"\n        # Add code here to validate and print the deprecated arguments\n        pass\n\n    generate_code(cli_args, answer_file_args)", "fixed_code": "def validate_arg_deprecation(self, cli_args, answer_file_args):\n    def generate_code(cli_args: dict, answer_file_args: dict) -> None:\n        \"\"\"\n        Validates and prints the deprecated arguments.\n        \n        :param cli_args: the dict of arguments from cli\n        :param answer_file_args: the dict of arguments from files\n        \"\"\"\n        # Get deprecated arguments using the available API\n        deprecated_args = get_deprecated_args()\n        \n        # Check CLI arguments for deprecation\n        for arg_name, arg_value in cli_args.items():\n            if arg_name in deprecated_args:\n                LOG.warning(\n                    \"Argument '{}' is deprecated: {}\".format(\n                        arg_name, \n                        deprecated_args[arg_name]\n                    )\n                )\n        \n        # Check answer file arguments for deprecation\n        for arg_name, arg_value in answer_file_args.items():\n            if arg_name in deprecated_args:\n                # If a deprecated argument is found in the answer file,\n                # raise an exception as per the available exception class\n                raise IRDeprecationException(\n                    \"Deprecated argument '{}' found in answer file: {}\".format(\n                        arg_name,\n                        deprecated_args[arg_name]\n                    )\n                )\n\n    generate_code(cli_args, answer_file_args)"}], "level": "file_runnable", "oracle_context": "{ \"apis\" : \"['warning', 'format', 'IRDeprecationException', 'get_deprecated_args', 'items']\", \"classes\" : \"['exceptions']\", \"vars\" : \"['Str', 'LOG']\" }", "docstring": "Validates and prints the deprecated arguments.\n\n:param cli_args: the dict of arguments from cli\n:param answer_file_args:  the dict of arguments from files"}
{"_id": "6306092e73426c38ae68ad0a", "fixed_results": [{"source": "existing", "false_code": "def validate_choices_args(self, args: List[str]) -> bool:\n    \"\"\"\n    Check if value of the given args is one of the available choices.\n    \n    :param args: The received arguments.\n    :return: True if the value of args is one of the available choices, False otherwise.\n    \"\"\"\n    return args in self.choices", "fixed_code": "from typing import List\n\ndef validate_choices_args(self, args: List[str]) -> bool:\n    \"\"\"\n    Check if value of the given args is one of the available choices.\n    \n    :param args: The received arguments.\n    :return: True if the value of args is one of the available choices, False otherwise.\n    \"\"\"\n    # Check if all arguments are in the available choices\n    return all(arg in self.choices for arg in args)"}], "level": "class_runnable", "oracle_context": "{ \"apis\" : \"['IRInvalidChoiceException', 'get_parser_option_specs', 'append', 'items']\", \"classes\" : \"['exceptions']\", \"vars\" : \"[]\" }", "docstring": "Check if value of choice arguments is one of the available choices.\n\n:param args: The received arguments."}
{"_id": "6306092e73426c38ae68ad11", "fixed_results": [{"source": "llm-based", "false_code": "@staticmethod\n    def merge_extra_vars(vars_dict, extra_vars=None):\n        \"\"\"Extend ``vars_dict`` with ``extra-vars``\n\n        :param vars_dict: Dictionary to merge extra-vars into\n        :param extra_vars: List of extra-vars\n        \"\"\"\n        for extra_var in extra_vars or []:\n            if extra_var.startswith('@'):\n                with open(extra_var[1:]) as f_obj:\n                    loaded_yml = yaml.safe_load(f_obj)\n\n                dict_utils.dict_merge(\n                    vars_dict,\n                    loaded_yml,\n                    conflict_resolver=dict_utils.ConflictResolver.\n                    unique_append_list_resolver)\n\n            else:\n                if '=' not in extra_var:\n                    raise exceptions.IRExtraVarsException(extra_var)\n                key, value = extra_var.split(\"=\", 1)\n                if value.startswith('@'):\n                    with open(value[1:]) as f_obj:\n                        loaded_yml = yaml.safe_load(f_obj)\n\n                    tmp_dict = {}\n                    dict_utils.dict_insert(tmp_dict, loaded_yml, *key.split(\".\"))\n\n                    dict_utils.dict_merge(\n                        vars_dict,\n                        tmp_dict,\n                        conflict_resolver=dict_utils.ConflictResolver.\n                        unique_append_list_resolver)\n\n                else:\n                    dict_utils.dict_insert(vars_dict, value + \"_modified\", *key.split(\".\"))\n        return vars_dict", "fixed_code": "@staticmethod\ndef merge_extra_vars(vars_dict, extra_vars=None):\n    \"\"\"Extend ``vars_dict`` with ``extra-vars``\n\n    :param vars_dict: Dictionary to merge extra-vars into\n    :param extra_vars: List of extra-vars\n    \"\"\"\n    for extra_var in extra_vars or []:\n        if extra_var.startswith('@'):\n            with open(extra_var[1:]) as f_obj:\n                loaded_yml = yaml.safe_load(f_obj)\n\n            dict_utils.dict_merge(\n                vars_dict,\n                loaded_yml,\n                conflict_resolver=dict_utils.ConflictResolver.unique_append_list_resolver\n            )\n        else:\n            if '=' not in extra_var:\n                raise exceptions.IRExtraVarsException(extra_var)\n            key, value = extra_var.split(\"=\", 1)\n            if value.startswith('@'):\n                with open(value[1:]) as f_obj:\n                    loaded_yml = yaml.safe_load(f_obj)\n\n                tmp_dict = {}\n                dict_utils.dict_insert(tmp_dict, loaded_yml, *key.split(\".\"))\n\n                dict_utils.dict_merge(\n                    vars_dict,\n                    tmp_dict,\n                    conflict_resolver=dict_utils.ConflictResolver.unique_append_list_resolver\n                )\n            else:\n                dict_utils.dict_insert(vars_dict, value, *key.split(\".\"))\n    \n    return vars_dict"}], "level": "project_runnable", "oracle_context": "{ \"apis\" : \"['dict_insert', 'dict_merge', 'open', 'safe_load', 'startswith', 'split', 'IRExtraVarsException']\", \"classes\" : \"['dict_utils']\", \"vars\" : \"[]\" }", "docstring": "Extend ``vars_dict`` with ``extra-vars``\n\n:param vars_dict: Dictionary to merge extra-vars into\n:param extra_vars: List of extra-vars"}
{"_id": "6306092f73426c38ae68ad13", "fixed_results": [{"source": "existing", "false_code": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None, extra_vars=None, ansible_args=None):\n    cmd = [\"ansible-playbook\"]\n    \n    if verbose is not None:\n        cmd.append(f\"-{verbose}\")\n    \n    if extra_vars is not None:\n        cmd.append(f\"--extra-vars={extra_vars}\")\n    \n    if ansible_args is not None:\n        for key, value in ansible_args.items():\n            cmd.append(f\"--{key}={value}\")\n    \n    cmd.append(playbook_path)\n    \n    subprocess.run(cmd, check=True)", "fixed_code": "import subprocess\nimport logging\n\ndef ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None, extra_vars=None, ansible_args=None):\n    \"\"\"Wraps the 'ansible-playbook' CLI.\n\n    :param ir_workspace: An Infrared Workspace object represents the active workspace\n    :param ir_plugin: An InfraredPlugin object of the current plugin\n    :param playbook_path: the playbook to invoke\n    :param verbose: Ansible verbosity level\n    :param extra_vars: dict. Passed to Ansible as extra-vars\n    :param ansible_args: dict of ansible-playbook arguments to plumb down directly to Ansible\n    \"\"\"\n    LOG = logging.getLogger(\"main\")\n    cmd = [\"ansible-playbook\"]\n    \n    # Handle verbosity level\n    if verbose is not None:\n        try:\n            v_level = int(verbose)\n            cmd.append('-' + 'v' * v_level)\n        except (ValueError, TypeError):\n            LOG.error(f\"Invalid verbose level: {verbose}\")\n            raise ValueError(f\"Verbose level must be a number, got: {verbose}\")\n    \n    # Handle extra variables\n    if extra_vars is not None:\n        if isinstance(extra_vars, dict):\n            # Convert dict to ansible-playbook format\n            extra_vars_str = ' '.join([f\"{k}={v}\" for k, v in extra_vars.items()])\n            cmd.extend([\"--extra-vars\", extra_vars_str])\n        else:\n            LOG.error(f\"Invalid extra_vars format: {extra_vars}\")\n            raise TypeError(\"extra_vars must be a dictionary\")\n    \n    # Handle additional ansible arguments\n    if ansible_args is not None:\n        if isinstance(ansible_args, dict):\n            for key, value in ansible_args.items():\n                if value is True:\n                    cmd.append(f\"--{key}\")\n                else:\n                    cmd.append(f\"--{key}={value}\")\n        else:\n            LOG.error(f\"Invalid ansible_args format: {ansible_args}\")\n            raise TypeError(\"ansible_args must be a dictionary\")\n    \n    # Add playbook path\n    if not playbook_path:\n        LOG.error(\"Playbook path is required\")\n        raise ValueError(\"Playbook path cannot be empty\")\n    cmd.append(playbook_path)\n    \n    # Debug log the final command\n    LOG.debug(f\"Executing ansible command: {' '.join(cmd)}\")\n    \n    try:\n        return subprocess.run(cmd, check=True)\n    except subprocess.CalledProcessError as e:\n        LOG.error(f\"Ansible playbook execution failed: {e}\")\n        raise"}], "level": "file_runnable", "oracle_context": "{ \"apis\" : \"['debug', 'format', 'error', 'extend', 'setattr', '_run_playbook', 'int', 'append']\", \"classes\" : \"['main', 'Display']\", \"vars\" : \"['Str', 'LOG']\" }", "docstring": "Wraps the 'ansible-playbook' CLI.\n\n:param ir_workspace: An Infrared Workspace object represents the active\nworkspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:param playbook_path: the playbook to invoke\n:param verbose: Ansible verbosity level\n:param extra_vars: dict. Passed to Ansible as extra-vars\n:param ansible_args: dict of ansible-playbook arguments to plumb down\n    directly to Ansible."}
{"_id": "6306093273426c38ae68ad15", "fixed_results": [{"source": "rule_based", "false_code": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n    \"\"\"Runs ansible cli with vars dict\n\n    :param vars_dict: dict, Will be passed as Ansible extra-vars\n    :param cli_args: the list  of command line arguments\n    :param ir_workspace: An Infrared Workspace object represents the active\n     workspace\n    :param ir_plugin: An InfraredPlugin object of the current plugin\n    :return: ansible results\n    \"\"\"\n    try:\n        from ansible.utils import context_objects\n        context_objects.GlobalCLIArgs = context_objects.CLIArgs\n    except ImportError:\n        pass\n    from ansible.cli.playbook import PlaybookCLI\n    from ansible.errors import AnsibleOptionsError\n    from ansible.errors import AnsibleParserError\n    with tempfile.NamedTemporaryFile(mode='w+', prefix='ir-settings-',\n        delete=True) as tmp:\n        tmp.write(yaml.safe_dump(vars_dict, 96, default_flow_style=False), 27)\n        tmp.flush()\n        cli_args.extend()\n        if not bool():\n            ir_extras = {'infrared': {'python': {'executable': sys.\n                executable, 'version': {'full': sys.version.split(43)[0],\n                'major': sys.version_info.major, 'minor': sys.version_info.\n                minor, 'micro': sys.version_info.micro}}}}\n            cli_args.extend()\n        cli = PlaybookCLI(cli_args, 38)\n        LOG.debug()\n        try:\n            cli.parse(18)\n            stdout = not bool(strtobool(os.environ.get(\n                'IR_ANSIBLE_NO_STDOUT', 'no', 68), 84), 16)\n            stderr = not bool(strtobool(os.environ.get(\n                'IR_ANSIBLE_NO_STDERR', 'no', 32), 66), 92)\n            ansible_outputs_dir = os.path.join(ir_workspace.path)\n            ansible_vars_dir = os.path.join(ir_workspace.path)\n            timestamp = datetime.utcnow().strftime()\n            filename_template = (\n                'ir_{timestamp}_{plugin_name}{postfix}.{file_ext}')\n            for _dir in (ansible_outputs_dir, ansible_vars_dir):\n                try:\n                    os.makedirs()\n                except OSError as e:\n                    if e.errno != errno.EEXIST:\n                        raise\n            if bool(strtobool(os.environ.get('IR_GEN_VARS_JSON', 'no', 93),\n                92), 65):\n                filename = filename_template.format(timestamp=timestamp,\n                    plugin_name=ir_plugin.name, postfix='', file_ext='json')\n                vars_file = os.path.join(ansible_vars_dir)\n                with open(vars_file) as fp:\n                    json.dump(vars_dict, indent=4, sort_keys=True)\n            with IRSTDFDManager(stdout=stdout, stderr=stderr) as fd_manager:\n                if bool():\n                    filename = filename_template.format(15, timestamp=\n                        timestamp, plugin_name=ir_plugin.name, postfix='',\n                        file_ext='log')\n                    log_file = os.path.join(filename)\n                    fd_manager.add(open(log_file, 'w', 41), 25)\n                if bool():\n                    filename = filename_template.format(39, timestamp=\n                        timestamp, plugin_name=ir_plugin.name, postfix=\n                        '_no_ansi', file_ext='log')\n                    log_file = os.path.join(ansible_outputs_dir)\n                    fd_manager.add()\n                return cli.run()\n        except (AnsibleParserError, AnsibleOptionsError) as error:\n            LOG.error('{}: {}'.format(type(), error, 89), 100)\n            raise error\n", "fixed_code": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n    \"\"\"Runs ansible cli with vars dict\n\n    :param vars_dict: dict, Will be passed as Ansible extra-vars\n    :param cli_args: the list  of command line arguments\n    :param ir_workspace: An Infrared Workspace object represents the active\n     workspace\n    :param ir_plugin: An InfraredPlugin object of the current plugin\n    :return: ansible results\n    \"\"\"\n    try:\n        from ansible.utils import context_objects\n        context_objects.GlobalCLIArgs = context_objects.CLIArgs\n    except ImportError:\n        pass\n    \n    from ansible.cli.playbook import PlaybookCLI\n    from ansible.errors import AnsibleOptionsError\n    from ansible.errors import AnsibleParserError\n    \n    with tempfile.NamedTemporaryFile(mode='w+', prefix='ir-settings-',\n                                    delete=True) as tmp:\n        # Fix yaml dump parameters\n        yaml.safe_dump(vars_dict, tmp, default_flow_style=False)\n        tmp.flush()\n        \n        # Add the vars file to cli args\n        cli_args.extend(['--extra-vars', '@' + tmp.name])\n        \n        # Add infrared extras if needed\n        if not cli_args.get('skip_ir_extras', False):\n            ir_extras = {\n                'infrared': {\n                    'python': {\n                        'executable': sys.executable,\n                        'version': {\n                            'full': sys.version.split()[0],\n                            'major': sys.version_info.major,\n                            'minor': sys.version_info.minor,\n                            'micro': sys.version_info.micro\n                        }\n                    }\n                }\n            }\n            cli_args.extend(['--extra-vars', str(ir_extras)])\n        \n        cli = PlaybookCLI(args=cli_args)\n        LOG.debug(\"Running playbook with CLI args: %s\", cli_args)\n        \n        try:\n            cli.parse()\n            \n            # Handle stdout/stderr configuration\n            stdout = not bool(strtobool(os.environ.get('IR_ANSIBLE_NO_STDOUT', 'no')))\n            stderr = not bool(strtobool(os.environ.get('IR_ANSIBLE_NO_STDERR', 'no')))\n            \n            # Setup output directories\n            ansible_outputs_dir = os.path.join(ir_workspace.path, 'ansible_outputs')\n            ansible_vars_dir = os.path.join(ir_workspace.path, 'ansible_vars')\n            timestamp = datetime.utcnow().strftime('%Y%m%d_%H%M%S')\n            \n            filename_template = 'ir_{timestamp}_{plugin_name}{postfix}.{file_ext}'\n            \n            # Create output directories\n            for _dir in (ansible_outputs_dir, ansible_vars_dir):\n                try:\n                    os.makedirs(_dir, exist_ok=True)\n                except OSError as e:\n                    if e.errno != errno.EEXIST:\n                        raise\n            \n            # Generate JSON vars file if requested\n            if bool(strtobool(os.environ.get('IR_GEN_VARS_JSON', 'no'))):\n                filename = filename_template.format(\n                    timestamp=timestamp,\n                    plugin_name=ir_plugin.name,\n                    postfix='',\n                    file_ext='json'\n                )\n                vars_file = os.path.join(ansible_vars_dir, filename)\n                with open(vars_file, 'w') as fp:\n                    json.dump(vars_dict, fp, indent=4, sort_keys=True)\n            \n            # Handle output file management\n            with IRSTDFDManager(stdout=stdout, stderr=stderr) as fd_manager:\n                # Add standard log file\n                filename = filename_template.format(\n                    timestamp=timestamp,\n                    plugin_name=ir_plugin.name,\n                    postfix='',\n                    file_ext='log'\n                )\n                log_file = os.path.join(ansible_outputs_dir, filename)\n                fd_manager.add(open(log_file, 'w'))\n                \n                # Add no-ansi log file\n                filename = filename_template.format(\n                    timestamp=timestamp,\n                    plugin_name=ir_plugin.name,\n                    postfix='_no_ansi',\n                    file_ext='log'\n                )\n                no_ansi_log_file = os.path.join(ansible_outputs_dir, filename)\n                fd_manager.add(open(no_ansi_log_file, 'w'))\n                \n                return cli.run()\n                \n        except (AnsibleParserError, AnsibleOptionsError) as error:\n            LOG.error('{}: {}'.format(type(error).__name__, str(error)))\n            raise error"}], "level": "project_runnable", "oracle_context": "{ \"apis\" : \"['error', 'NamedTemporaryFile', 'type', 'strftime', 'dump', 'safe_dump', 'debug', 'format', 'open', 'str', 'extend', 'bool', 'join', 'makedirs', 'get', 'flush', 'parse', 'add', 'strtobool', 'write', 'utcnow', 'split', 'run']\", \"classes\" : \"['PlaybookCLI', 'IRSTDFDManager', 'context_objects', 'NoAnsiFile']\", \"vars\" : \"['Str', 'LOG']\" }", "docstring": "Runs ansible cli with vars dict\n\n:param vars_dict: dict, Will be passed as Ansible extra-vars\n:param cli_args: the list  of command line arguments\n:param ir_workspace: An Infrared Workspace object represents the active\n workspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:return: ansible results"}
{"_id": "63060b1b73426c38ae68ad42", "fixed_results": [{"source": "rule_based", "false_code": "def inject_config(self):\n    \"\"\"Set the environment variable for config path, if it is undefined.\"\"\"\n    if os.environ.get('') == '':\n        os.environ['ANSIBLE_CONFIG'] = self.ansible_config_path\n    return os.environ['ANSIBLE_CONFIG']\n", "fixed_code": "def inject_config(self):\n    \"\"\"\n    Set the environment variable for config path, if it is undefined.\n    \n    Returns:\n        str: The path to the Ansible configuration file\n    \n    Sets ANSIBLE_CONFIG environment variable if not already defined.\n    Creates default config if specified path doesn't exist.\n    \"\"\"\n    if not os.environ.get('ANSIBLE_CONFIG'):\n        if hasattr(self, 'ansible_config_path') and self.ansible_config_path:\n            os.environ['ANSIBLE_CONFIG'] = self.ansible_config_path\n        else:\n            raise ValueError(\"ansible_config_path is not set or invalid\")\n    \n    return os.environ['ANSIBLE_CONFIG']"}], "level": "class_runnable", "oracle_context": "{ \"apis\" : \"['get']\", \"classes\" : \"['os']\", \"vars\" : \"[]\" }", "docstring": "Set the environment variable for config path, if it is undefined."}
